# Краткое содержание "Apache Kafka. Потоковая обработка и анализ данных", 2-е издание, Гвен Шапира, Тодд Палино, Раджини Сиварам, Крит Петти

## Описание

Это мой конспект
книги ["Краткое содержание "Apache Kafka. Потоковая обработка и анализ данных", 2-е издание, Гвен Шапира, Тодд Палино, Раджини Сиварам, Крит Петти](https://www.piter.com/collection/bestsellery-oreilly/product/apache-kafka-potokovaya-obrabotka-i-analiz-dannyh-2-e-izdanie) (
сайт российского издательства).

Если Вы автор и считаете, что данный конспект нарушает авторские права - прошу сообщить, я сделаю этот репозиторий
приватным.

Нашли опечатку/неточность? Пишите - разберемся.

[Связаться со мной](https://t.me/szachesov)

# 1 Знакомьтесь: Kafka

## 1.1 Обмен сообщениями по типу "публикация/подписка"

Обмен сообщениям по типу "публикация/подписка"(publish/subscribe(pub/sub) messaging) - паттерн проектирования, в котором
отправитель(издатель) элемента данных(сообщений) не направляет его конкретному потребителю. Вместо этого он
классифицирует сообщения, а потребитель(подписчик) подписывается на определенные их классы.

## 1.2 Открываем для себя систему Kafka

**Apache Kafka** - система обмена сообщения по принципу "публикация/подписка". Её также называют **распределенной
платформой
потоковой обработки**.

Данные в Kafka хранятся долго, упорядочено. Их можно читать когда угодно, они могут распределяться по системе либо ради
защиты от сбоев, либо ради производительности.

### 1.2.1 Сообщения и пакеты

**Сообщения (message)** - единица данных в Kafka. Аналогия в БД - строки(row) или запись(record). Для Kafka это просто
массив байтов.

**Ключ (key)** - фрагмент метаданных, который может быть в сообщении. Может использоваться для лучшего управления
сообщениями в разделе. Простейшая схема - подобна распределению `value` при записи в `java.util.HasMap`.

**Пакет (batch)** - набор сообщений, относящиеся к одному топику или разделу. Используется для производительности. При
этом следует соблюдать баланс времени задержки-пропускной способности.

### 1.2.2 Схемы

**Схема** - дополнительная структура на содержимое сообщения. Варианты задания схемы: JSON, XML, Apache Avro и др.

Для Kafka важен единообразный формат данных.

### 1.2.3 Топики и разделы

**Топик (topic)** - категории, служащие для распределения сообщений. Аналогия БД - таблица.

**Разделы, партиции (partitions)** - единицы топиков. Сообщения записываются в него добавлением в конец, а читаются по
порядку от
начала.

Топик, состоящий из нескольких разделов не гарантирует упорядоченность сообщений в пределах всего топика - только в
разделе.

Разделы можно разместить на разные сервера и/или реплицировать.

**Поток данных (stream)** - данные, перемещающиеся от производителя к потребителю в рамках всего топика.

### 1.2.4 Производители и потребители

Типы пользователей Kafka: производитель, потребитель. Для клиентских API(например, Kafka Streams) производители и
потребители выступают в качестве строительных блоков и предоставляют более высокий уровень взаимодействия.

**Производитель (producer)** - генерирует новые сообщения для конкретного топика. По умолчанию будет поставлять
сообщения по всем партициям, а может в конкретный

**Потребитель (consumers)** - читают сообщения. Подписывается на один топик или более и читает в порядке их возрастания.

**Смещение, офсет (offset)** - непрерывно возрастающее целочисленное значение, ещё один элемент метаданных. Обычно
значение сохраняется в хранилище Kafka, что бы потребитель приостанавливал и возобновлял чтение с нужного места.

**Группы потребителей (consumer groups)** - один или несколько потребителей, объединившихся для обработки топика. В
группе гарантируется, что несколько потребителей не могут читать одну партицию.

**Владение (ownership)** - соответствие потребителя партиции.

### 1.2.5 Брокеры и кластеры

**Брокер (broker)** - отдельный сервер Kafka. Они получают сообщения от производителей, присваивают смещение и
записывают в дисковое хранилище. И обслуживает потребителей.

**Кластер (cluster)** - группа брокеров.

**Контроллер (cluster controller)** - один из брокеров в рамках кластера. Выбирает автоматически. Отвечает за
администрирование операций, распределение разделов по брокерам и мониторинг отказов последних.

**Ведущий (leader)** - брокер, которому принадлежит один раздел.

**Последователи (followers)** - помещается реплицированный раздел.

Производители соединяются с ведущим, потребители получают сообщения либо от ведущего, либо из последователя.

**Сохранение информации (retention)** - ключевая возможность Kafka, в течение длительного времени.

**Сжатые журналы (log compacted)** - механизм хранения данных в топиках, например хранения последнего сообщения с
конкретным ключом.

### 1.2.6 Несколько кластеров

Преимущества больших систем с несколькими кластерами:

* Разделение типов данных
* Изоляция по требованиям безопасности
* Несколько центров обработки данных

Репликация осуществляется в рамках одного кластера, но не нескольких кластеров.

Для возможности репликации данных между кластерами, можно использовать утилиту **MirrorMarker**.

# 2 Установка Kafka

## 2.1 Настройка среды

### 2.1.3 Установка ZooKeeper

ZooKeeper - это централизованный сервис для хранения информации о конфигурации, присвоения имен, обеспечения
распределения синхронизации и предоставления группового обслуживания.

#### 2.1.3.1 Ансамбль ZooKeeper

ZooKeeper предназначен для работы в качестве кластера - **ансамбль(ensemble)**.

Рекомендуется делать нечетное число серверов, из-за особенности алгоритма балансировки, и он сможет отвечать на запросы,
когда функционируют большинство членов ансамбля(кворум).

Чтобы внести изменения в настройки ансамбля, необходимо перезагрузить узлы по одному разу за раз.

Не рекомендуется запускать одновременно более семи узлов - будет снижаться производительность, причина - природа
протокола консенсуса. Лучше рассмотреть возможность добавление дополнительных узлов-наблюдателей.

## 2.3 Настройка брокера

### 2.3.1 Основные параметры брокера

#### 2.3.1.1 broker.id

Целочисленный идентификатор

* Обязательный
* Значение по-умолчанию 0
* Должно быть уникальным для брокера в пределах кластера
* Рекомендуется, чтобы число было как-то связано с хостом, чтобы более прозрачным было соответствие и удобно было при
  сопровождении брокеров. Например, имена хостов: `host1.example.com`, `host2.example.com`, а `broker.id` 1, 2
  соответственно.

#### 2.3.1.2 Listeners

Настройка слушателей: список URL, которые мы прослушиваем, с именем слушателя. Если имя слушателя не является общим
протоколом безопасности, то необходимо настроить ещё одну конфигурацию **listener.security.protocol.map**. Формат:
`protocol://hostname:port`

#### 2.3.1.3 zookeeper.connect

Путь, который использует ZooKeeper для хранения метаданных брокеров. Формат: `hostname:port/path`. `/path` -
опциональный, путь используемый в качестве нового корневого пути кластера. Считается хорошей практикой
использовать `/path`, чтобы использовать ZooKeeper с другими приложениями.

#### 2.3.1.4 log.dirs

Список путей в локальной системе, куда будут сохраняться файлы, которые содержится в партициях. Если задано несколько,
брокер будет сохранять в них файлы равномерно, с учетом количества внутренних разделов, а не дискового пространства.

#### 2.3.1.5 num.recovery.threads.per.data.dir

Число потоков пула, которые обрабатывают сегменты журналов(директорий с файлами). Пул применяется:

* при обычном запуске
* запуске после сбоя
* останове

При восстановлении после сбоя брокера с большим количеством разделов, выгода от применения может достигать нескольких
часов.

Определяется из расчета на один каталог журналов и числа `log.dirs`, то
есть `num.recovery.threads.per.data.dir * log.dirs = число потоков`.

#### 2.3.1.6 auto.create.topics.enable

Автоматически топик создается в следующих случаях:

* производитель начинает писать в топик сообщение
* потребитель начинает читать из топика сообщение
* любой клиент запрашивает метаданные топика

Если управление топика явно, вручную или системой инициализации, то можно установить значение `false`.

#### 2.3.1.7 auto.leader.rebalance.enable

Активирует балансировку ведущий реплик брокеров. Если не активно, ведущие реплики всех разделов сосредоточены на одном
брокере и кластер будет несбалансированным. Активирует фоновый поток, который проверяет распределение ведущих реплик.

#### 2.3.1.8 delete.topic.enable

Запретить удаление топиков. Полезно для безопасности.

### 2.3.2 Настройки топиков по умолчанию

#### 2.3.2.1 num.partitions

С каким количеством партиций создается топик автоматически. Количество может увеличиваться, но не уменьшаться. То есть
требуется его создать вручную.

```text
КАК ВЫБРАТЬ КОЛИЧЕСТВО РАЗДЕЛОВ
Вот несколько факторов, которые следует учитывать при выборе количества разделов.
• Какой пропускной способности планируется достичь для топика? Например, планируете вы записывать 100 Кбайт/с или 1 Гбайт/с?
• Какая максимальная пропускная способность ожидается при потреблении сообщений из 
отдельного раздела? Раздел всегда будет полностью потребляться одним потребителем (даже если 
не используются группы потребителей, потребитель должен прочитать все сообщения в разделе). 
Если знать, что потребитель записывает данные в базу, не способную обрабатывать более 50 Мбайт/с 
по каждому записывающему в нее потоку, становится очевидным ограничение в 50 Мбайт/с при 
потреблении данных из раздела.
• Аналогичным образом можно оценить максимальную пропускную способность из расчета на 
производитель для одного раздела, но, поскольку быстродействие производителей обычно выше, 
чем быстродействие потребителей, этот шаг чаще всего можно пропустить.
• При отправке сообщений разделам по ключам добавление новых разделов может оказаться очень 
непростой задачей, так что желательно рассчитывать пропускную способность, исходя не из текущего 
объема использования, а из планируемого в будущем.
• Обдумайте число разделов, размещаемых на каждом из брокеров, а также доступные каждому 
брокеру объем дискового пространства и полосу пропускания сети.
• Старайтесь избегать завышенных оценок, ведь любой раздел расходует оперативную память и другие 
ресурсы на брокере и увеличивает время обновления метаданных и передачи руководства.
• Будете ли вы выполнять зеркальное копирование данных? Возможно, вам также потребуется учесть 
пропускную способность своей конфигурации зеркального копирования. Большие разделы могут 
стать серьезным недостатком во многих конфигурациях зеркального копирования.
• Если вы используете облачные сервисы, есть ли ограничение количества IOPS (операции ввода/вывода 
в секунду) на ваших виртуальных машинах или дисках? В зависимости от облачного сервиса и конфигурации 
виртуальных машин могут существовать жесткие ограничения на количество разрешенных IOPS, которые 
приведут к нарушению квот. Наличие слишком большого количества разделов может иметь побочный 
эффект увеличения количества IOPS из-за задействованного параллелизма.
```

Если есть оценка целевой пропускной способности топика и ожидаемой пропускной способности,
то `целевая пропускная способность / ожидаемую пропускную способность потребителей = число партиций`.

Если подробной информации нет, то по опыту ограничение разделов на диске до 6 Гбайт сохраняемой информации в день часто
дает удовлетворительные результаты.

#### 2.3.2.2 default.replication.factor

При активном `auto.create.topics.enable`, задает коэффициент репликации для новых топиков.

Рекомендации, позволяющие избежать перебоев, от внутренних факторов(оборудования):

* Рекомендуется устанавливать минимум на 1 больше значения `min.insync.replicas`
* Для больших кластеров и большого оборудования на 2 больше значения `min.insync.replicas`. В идеале дать разрешить одно
  запланированное и одно не запланированное отключение, те у каждого минимум 3 точных копии каждой партиции.

#### 2.3.2.3 log.retention.ms

Наряду с `log.retention.hours`(по-умолчанию 168) и `log.retention.minutes`, задается время хранения сообщения. Если
заданы несколько параметров, то этот в приоритете.

Параметр анализирует времени последнего изменения (mtime) на диске.

#### 2.3.2.4 log.retention.bytes

Ограничение хранения на основе размера партиции. То
есть `количество партиций * log.retention.bytes = максимальный размер топика`.

Если использовать `log.retention.ms` и `log.retention.bytes`, то будут учитываться оба параметра. Для простоты, лучше
установить один, но для продвинутых можно оба.

#### 2.3.2.5 log.segment.bytes

Настройки выше относятся к сегментам журналов, а не отдельным сообщениям. Разер сегмента журнала на диске, когда он
закрывается и открывается новый. Удаление сообщений по таймеру `log.retention.ms` и `log.retention.bytes`, возможно
только для закрытых сегментов.

#### 2.3.2.6 log.roll.ms

Закрытие сегмента журнала по времени. Надо учитывать, если много партиций, то будет происходить закрытие многих журналов
на диске и возможно скажется на производительность диска.

#### 2.3.2.7 min.insync.replicas

Устанавливает количество реплик, которые будут гарантировано подхвачены и синхронизированы с производителем. Она хорошо
сочетается с настройкой производителя на проверку всех запросов. Снижает производительность и если кластер с высокой
пропускной способностью, которые допускают иногда потерю сообщения, то этот параметр лучше не использовать.

#### 2.3.2.8 message.max.bytes

Максимальный размер сообщений, если размер превышен при отправке сообщения - производителю возвращается ошибка. Данный
размер указан для уже сжатых сообщений.

Данный параметр нужно согласовывать с параметром `fetch.message.max.bytes` у клиента и `replica.fetch.max.bytes` на
брокерах при конфигурации кластера.

## 2.4 Выбор аппаратного обеспечения

На производительность влияют следующие факторы:

* Емкость дисков
* Пропускная способность дисков
* Оперативная память
* Сеть
* CPU

### 2.4.1 Пропускная способность дисков

SSD являются лучшим вариантом при наличии очень большого количества клиентских подключений.

### 2.4.2 Емкость диска

При подборе емкости, нужно понимать, сколько планируется получать информацию и учитывать 10% для других файлов.

### 2.4.3 Память

Сообщения сохраняются в страничном кэше системы, поэтому операция чтения достаточно быстрое.

Для Kafka не требуется выделение для JVM большого объема оперативной памяти в куче. Например, брокер обрабатывает 150к
сообщений с секунду при скорости передачи данных 200 Мбит/с, может работать с кучей 5 Гбайт. Остальная часть оперативной
памяти будет использована для страничного кэша. Поэтому не следует располагать Kafka в системе с другими важными
приложениями.

### 2.4.4 Передача данных по сети

Максимальный объем трафика у Kafka, определяется пропускной способностью сети. Рекомендуется применять сетевые адаптеры
NICs емкостью не менее 10 Гбайт.

### 2.4.5 CPU

Вычислительная мощность важна при очень большом масштабировании. Также мощность влияет на разархивирование и
архивирование сообщений.

## 2.6 Настройка кластеров Kafka

Преимущества кластера:

* Возможность масштабировать
* Возможность использовать репликации для надежности

### 2.6.1 Сколько должно быть брокеров

Размер кластера зависит от:

* емкости диска
* емкости реплик на одного брокера
* мощности процессора
* пропускной способности

В настоящее время рекомендуется иметь не более 14к реплик на брокер и 1м реплик на кластер.

### 2.6.2 Конфигурация брокеров

Требования к конфигурации брокеров:

* У всех брокеров должно быть одинаковое значение `zookeeper.connect`
* У каждого брокера в кластере должен быть уникальный `broker.id`

### 2.6.3 Тонкая настройка операционной системы

На Linux эти параметры обычно настраиваются в файле `/etc/sysctl.conf`

#### 2.6.3.1 Виртуальная память

Для высокой пропускной способности, следует избегать подкачки страниц памяти на диск, установив `vm.swappines=1`.

При использовании быстрых дисков(SSD) имеет смысл уменьшить количество "грязных" станиц, установив
`vm.dirty_background_ratio=5`.

`vm.dirty_ratio` - процент от всего объема памяти при достижении которой блокируется запись "грязных" станиц и
инициализируется сброс их на диск, разумно устанавливать 60-80, но быть осторожным с этим параметром.

Рекомендованное кол-во файловые дескрипторов для сегментов журнала и открытых
соединений `= (количество_разделов) Х (размер_раздела/размер_сегмента)`. На основе этого устанавливается
параметр `vm.max_map_count`, значение 400к-600к в зависимости от среды дает положительны результат. Также рекомендуется
установить `vm.overcommit_memory=0` иначе ОП будет захватывать слишком много памяти и не давая работать Kafka
оптимально.

#### 2.6.3.2 Диск

В качестве локальной файловой системы чаще задействуется:

* `Ext4` - работает хорошо, но требует потенциально небезопасный параметров
* `XFS` - использует алгоритм отложенного выделения, но более безопасный, чем в `Ext4`

`atime` - время последнего обращения к файлу. Данный параметр неиспользуемая Kafka, поэтому его можно деактивировать,
чтобы лишний раз не нагружать процессор, указав `noatime`.

При больших объемах рекомендуется использовать `largeio`.

#### 2.6.3.3 Передача данных по сети

Вначале нужно изменить объемы памяти, выделяемой для буферов отправки и получения для каждого сокета,
`net.core.wmem_default` и `net.core.rmem_default`, рекомендуется 2097152(2Мб).

Рекомендуется задать буферы отправки и получения для сокетов TCP с помощью `net.ipv4.tcp_wmem` и `net.ipv4.tcp_rmem`.

`net.ipv4.tcp_window_scaling=1` - оконное масштабирования TCP, чтобы клиенты эффективно передавали данные и
буферизировать их на стороне брокера.

`net.ipv4.tcp_max_syn_backlog` - больше чем по-умолчанию 1024, чтобы увеличить одновременное подключение.

`net.ipv4.netdev_max_backlog` - больше чем по-умолчанию 1000, чтобы помочь при всплесках сетевого трафика.

## 2.7 Промышленная эксплуатация

### 2.7.1 Параметры сборки мусора

Рекомендуется использовать сборщик мусора `G1`. Для корректировки производительности используются
параметры: `MaxGCPauseMillis`, `InitiatingHeapOccupancyPercent`. Для Kafka можно задавать более маленькие значения этих
параметров.

Например, для сервера 64 Гбайт оперативной памяти и Kafka работала с кучей 5 Гбайт. Можно
установить `MaxGCPauseMillis=20`, `InitiatingHeapOccupancyPercent=35`.

### 2.7.2 Планировка ЦОД

Важно применять репликации и размещать физически брокеров на стойках в ЦОД в среде со зоной отказа.

Рекомендуется размещать реплики партиций на разных полках и в целом разные брокеры на разных полках.

### 2.7.3 Размещение приложений на ZooKeeper

Со временем зависимость от ZooKeeper уменьшается и сойдет на нет.

Использования целого ансамбля ZooKeeper для одного кластера необоснованно, достаточно одного ZooKeeper.

Рекомендуется, чтобы потребители использовали для смещения Kafka, а не ZooKeeper.

Не рекомендуется использование одного ансамбля ZooKeeper, для других приложений отличный, не считая разные кластеры
брокеров Kafka.

### 3 Производители Kafka: запись сообщений в Kafka

