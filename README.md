# Краткое содержание "Apache Kafka. Потоковая обработка и анализ данных", 2-е издание, Гвен Шапира, Тодд Палино, Раджини Сиварам, Крит Петти

## Описание

Это мой конспект
книги ["Краткое содержание "Apache Kafka. Потоковая обработка и анализ данных", 2-е издание, Гвен Шапира, Тодд Палино, Раджини Сиварам, Крит Петти](https://www.piter.com/collection/bestsellery-oreilly/product/apache-kafka-potokovaya-obrabotka-i-analiz-dannyh-2-e-izdanie) (
сайт российского издательства).

Если Вы автор и считаете, что данный конспект нарушает авторские права - прошу сообщить, я сделаю этот репозиторий
приватным.

Нашли опечатку/неточность? Пишите - разберемся.

[Связаться со мной](https://t.me/szachesov)

# Глава 1. Знакомьтесь: Kafka

## Обмен сообщениями по типу "публикация/подписка"

Обмен сообщениям по типу "публикация/подписка"(publish/subscribe(pub/sub) messaging) - паттерн проектирования, в котором
отправитель(издатель) элемента данных(сообщений) не направляет его конкретному потребителю. Вместо этого он
классифицирует сообщения, а потребитель(подписчик) подписывается на определенные их классы.

## Открываем для себя систему Kafka

**Apache Kafka** - система обмена сообщения по принципу "публикация/подписка". Её также называют **распределенной
платформой
потоковой обработки**.

Данные в Kafka хранятся долго, упорядочено. Их можно читать когда угодно, они могут распределяться по системе либо ради
защиты от сбоев, либо ради производительности.

### Сообщения и пакеты

**Сообщения (message)** - единица данных в Kafka. Аналогия в БД - строки(row) или запись(record). Для Kafka это просто
массив байтов.

**Ключ (key)** - фрагмент метаданных, который может быть в сообщении. Может использоваться для лучшего управления
сообщениями в разделе. Простейшая схема - подобна распределению `value` при записи в `java.util.HasMap`.

**Пакет (batch)** - набор сообщений, относящиеся к одному топику или разделу. Используется для производительности. При
этом следует соблюдать баланс времени задержки-пропускной способности.

### Схемы

**Схема** - дополнительная структура на содержимое сообщения. Варианты задания схемы: JSON, XML, Apache Avro и др.

Для Kafka важен единообразный формат данных.

### Топики и разделы

**Топик (topic)** - категории, служащие для распределения сообщений. Аналогия БД - таблица.

**Разделы, партиции (partitions)** - единицы топиков. Сообщения записываются в него добавлением в конец, а читаются по
порядку от
начала.

Топик, состоящий из нескольких разделов не гарантирует упорядоченность сообщений в пределах всего топика - только в
разделе.

Разделы можно разместить на разные сервера и/или реплицировать.

**Поток данных (stream)** - данные, перемещающиеся от производителя к потребителю в рамках всего топика.

### Производители и потребители

Типы пользователей Kafka: производитель, потребитель. Для клиентских API(например, Kafka Streams) производители и
потребители выступают в качестве строительных блоков и предоставляют более высокий уровень взаимодействия.

**Производитель (producer)** - генерирует новые сообщения для конкретного топика. По умолчанию будет поставлять
сообщения по всем партициям, а может в конкретный

**Потребитель (consumers)** - читают сообщения. Подписывается на один топик или более и читает в порядке их возрастания.

**Смещение, офсет (offset)** - непрерывно возрастающее целочисленное значение, ещё один элемент метаданных. Обычно
значение сохраняется в хранилище Kafka, что бы потребитель приостанавливал и возобновлял чтение с нужного места.

**Группы потребителей (consumer groups)** - один или несколько потребителей, объединившихся для обработки топика. В
группе гарантируется, что несколько потребителей не могут читать одну партицию.

**Владение (ownership)** - соответствие потребителя партиции.

### Брокеры и кластеры

**Брокер (broker)** - отдельный сервер Kafka. Они получают сообщения от производителей, присваивают смещение и
записывают в дисковое хранилище. И обслуживает потребителей.

**Кластер (cluster)** - группа брокеров.

**Контроллер (cluster controller)** - один из брокеров в рамках кластера. Выбирает автоматически. Отвечает за
администрирование операций, распределение разделов по брокерам и мониторинг отказов последних.

**Ведущий (leader)** - брокер, которому принадлежит один раздел.

**Последователи (followers)** - помещается реплицированный раздел.

Производители соединяются с ведущим, потребители получают сообщения либо от ведущего, либо из последователя.

**Сохранение информации (retention)** - ключевая возможность Kafka, в течение длительного времени.

**Сжатые журналы (log compacted)** - механизм хранения данных в топиках, например хранения последнего сообщения с
конкретным ключом.

### Несколько кластеров

Преимущества больших систем с несколькими кластерами:

* Разделение типов данных
* Изоляция по требованиям безопасности
* Несколько центров обработки данных

Репликация осуществляется в рамках одного кластера, но не нескольких кластеров.

Для возможности репликации данных между кластерами, можно использовать утилиту **MirrorMarker**.

# Глава 2. Установка Kafka

## Настройка среды

### Установка ZooKeeper

ZooKeeper - это централизованный сервис для хранения информации о конфигурации, присвоения имен, обеспечения
распределения синхронизации и предоставления группового обслуживания.

#### Ансамбль ZooKeeper

ZooKeeper предназначен для работы в качестве кластера - **ансамбль(ensemble)**.

Рекомендуется делать нечетное число серверов, из-за особенности алгоритма балансировки, и он сможет отвечать на запросы,
когда функционируют большинство членов ансамбля(кворум).

Чтобы внести изменения в настройки ансамбля, необходимо перезагрузить узлы по одному разу за раз.

Не рекомендуется запускать одновременно более семи узлов - будет снижаться производительность, причина - природа
протокола консенсуса. Лучше рассмотреть возможность добавление дополнительных узлов-наблюдателей.

## Настройка брокера

### Основные параметры брокера

#### broker.id

Целочисленный идентификатор

* Обязательный
* Значение по-умолчанию 0
* Должно быть уникальным для брокера в пределах кластера
* Рекомендуется, чтобы число было как-то связано с хостом, чтобы более прозрачным было соответствие и удобно было при
  сопровождении брокеров. Например, имена хостов: `host1.example.com`, `host2.example.com`, а `broker.id` 1, 2
  соответственно.

#### Listeners

Настройка слушателей: список URL, которые мы прослушиваем, с именем слушателя. Если имя слушателя не является общим
протоколом безопасности, то необходимо настроить ещё одну конфигурацию **listener.security.protocol.map**. Формат:
`protocol://hostname:port`

#### zookeeper.connect

Путь, который использует ZooKeeper для хранения метаданных брокеров. Формат: `hostname:port/path`. `/path` -
опциональный, путь используемый в качестве нового корневого пути кластера. Считается хорошей практикой
использовать `/path`, чтобы использовать ZooKeeper с другими приложениями.

#### log.dirs

Список путей в локальной системе, куда будут сохраняться файлы, которые содержится в партициях. Если задано несколько,
брокер будет сохранять в них файлы равномерно, с учетом количества внутренних разделов, а не дискового пространства.

#### num.recovery.threads.per.data.dir

Число потоков пула, которые обрабатывают сегменты журналов(директорий с файлами). Пул применяется:

* при обычном запуске
* запуске после сбоя
* останове

При восстановлении после сбоя брокера с большим количеством разделов, выгода от применения может достигать нескольких
часов.

Определяется из расчета на один каталог журналов и числа `log.dirs`, то
есть `num.recovery.threads.per.data.dir * log.dirs = число потоков`.

#### auto.create.topics.enable

Автоматически топик создается в следующих случаях:

* производитель начинает писать в топик сообщение
* потребитель начинает читать из топика сообщение
* любой клиент запрашивает метаданные топика

Если управление топика явно, вручную или системой инициализации, то можно установить значение `false`.

#### auto.leader.rebalance.enable

Активирует балансировку ведущий реплик брокеров. Если не активно, ведущие реплики всех разделов сосредоточены на одном
брокере и кластер будет несбалансированным. Активирует фоновый поток, который проверяет распределение ведущих реплик.

#### delete.topic.enable

Запретить удаление топиков. Полезно для безопасности.

### Настройки топиков по умолчанию

#### num.partitions

С каким количеством партиций создается топик автоматически. Количество может увеличиваться, но не уменьшаться. То есть
требуется его создать вручную.

```text
КАК ВЫБРАТЬ КОЛИЧЕСТВО РАЗДЕЛОВ
Вот несколько факторов, которые следует учитывать при выборе количества разделов.
• Какой пропускной способности планируется достичь для топика? Например, планируете вы записывать 100 Кбайт/с или 1 Гбайт/с?
• Какая максимальная пропускная способность ожидается при потреблении сообщений из 
отдельного раздела? Раздел всегда будет полностью потребляться одним потребителем (даже если 
не используются группы потребителей, потребитель должен прочитать все сообщения в разделе). 
Если знать, что потребитель записывает данные в базу, не способную обрабатывать более 50 Мбайт/с 
по каждому записывающему в нее потоку, становится очевидным ограничение в 50 Мбайт/с при 
потреблении данных из раздела.
• Аналогичным образом можно оценить максимальную пропускную способность из расчета на 
производитель для одного раздела, но, поскольку быстродействие производителей обычно выше, 
чем быстродействие потребителей, этот шаг чаще всего можно пропустить.
• При отправке сообщений разделам по ключам добавление новых разделов может оказаться очень 
непростой задачей, так что желательно рассчитывать пропускную способность, исходя не из текущего 
объема использования, а из планируемого в будущем.
• Обдумайте число разделов, размещаемых на каждом из брокеров, а также доступные каждому 
брокеру объем дискового пространства и полосу пропускания сети.
• Старайтесь избегать завышенных оценок, ведь любой раздел расходует оперативную память и другие 
ресурсы на брокере и увеличивает время обновления метаданных и передачи руководства.
• Будете ли вы выполнять зеркальное копирование данных? Возможно, вам также потребуется учесть 
пропускную способность своей конфигурации зеркального копирования. Большие разделы могут 
стать серьезным недостатком во многих конфигурациях зеркального копирования.
• Если вы используете облачные сервисы, есть ли ограничение количества IOPS (операции ввода/вывода 
в секунду) на ваших виртуальных машинах или дисках? В зависимости от облачного сервиса и конфигурации 
виртуальных машин могут существовать жесткие ограничения на количество разрешенных IOPS, которые 
приведут к нарушению квот. Наличие слишком большого количества разделов может иметь побочный 
эффект увеличения количества IOPS из-за задействованного параллелизма.
```

Если есть оценка целевой пропускной способности топика и ожидаемой пропускной способности,
то `целевая пропускная способность / ожидаемую пропускную способность потребителей = число партиций`.

Если подробной информации нет, то по опыту ограничение разделов на диске до 6 Гбайт сохраняемой информации в день часто
дает удовлетворительные результаты.

#### default.replication.factor

При активном `auto.create.topics.enable`, задает коэффициент репликации для новых топиков.

Рекомендации, позволяющие избежать перебоев, от внутренних факторов(оборудования):

* Рекомендуется устанавливать минимум на 1 больше значения `min.insync.replicas`
* Для больших кластеров и большого оборудования на 2 больше значения `min.insync.replicas`. В идеале дать разрешить одно
  запланированное и одно не запланированное отключение, те у каждого минимум 3 точных копии каждой партиции.

#### log.retention.ms

Наряду с `log.retention.hours`(по-умолчанию 168) и `log.retention.minutes`, задается время хранения сообщения. Если
заданы несколько параметров, то этот в приоритете.

Параметр анализирует времени последнего изменения (mtime) на диске.

#### log.retention.bytes

Ограничение хранения на основе размера партиции. То
есть `количество партиций * log.retention.bytes = максимальный размер топика`.

Если использовать `log.retention.ms` и `log.retention.bytes`, то будут учитываться оба параметра. Для простоты, лучше
установить один, но для продвинутых можно оба.

#### log.segment.bytes

Настройки выше относятся к сегментам журналов, а не отдельным сообщениям. Разер сегмента журнала на диске, когда он
закрывается и открывается новый. Удаление сообщений по таймеру `log.retention.ms` и `log.retention.bytes`, возможно
только для закрытых сегментов.

#### log.roll.ms

Закрытие сегмента журнала по времени. Надо учитывать, если много партиций, то будет происходить закрытие многих журналов
на диске и возможно скажется на производительность диска.

#### min.insync.replicas

Устанавливает количество реплик, которые будут гарантировано подхвачены и синхронизированы с производителем. Она хорошо
сочетается с настройкой производителя на проверку всех запросов. Снижает производительность и если кластер с высокой
пропускной способностью, которые допускают иногда потерю сообщения, то этот параметр лучше не использовать.

#### message.max.bytes

Максимальный размер сообщений, если размер превышен при отправке сообщения - производителю возвращается ошибка. Данный
размер указан для уже сжатых сообщений.

Данный параметр нужно согласовывать с параметром `fetch.message.max.bytes` у клиента и `replica.fetch.max.bytes` на
брокерах при конфигурации кластера.

## Выбор аппаратного обеспечения

На производительность влияют следующие факторы:

* Емкость дисков
* Пропускная способность дисков
* Оперативная память
* Сеть
* CPU

### Пропускная способность дисков

SSD являются лучшим вариантом при наличии очень большого количества клиентских подключений.

### Емкость диска

При подборе емкости, нужно понимать, сколько планируется получать информацию и учитывать 10% для других файлов.

### Память

Сообщения сохраняются в страничном кэше системы, поэтому операция чтения достаточно быстрое.

Для Kafka не требуется выделение для JVM большого объема оперативной памяти в куче. Например, брокер обрабатывает 150к
сообщений с секунду при скорости передачи данных 200 Мбит/с, может работать с кучей 5 Гбайт. Остальная часть оперативной
памяти будет использована для страничного кэша. Поэтому не следует располагать Kafka в системе с другими важными
приложениями.

### Передача данных по сети

Максимальный объем трафика у Kafka, определяется пропускной способностью сети. Рекомендуется применять сетевые адаптеры
NICs емкостью не менее 10 Гбайт.

### CPU

Вычислительная мощность важна при очень большом масштабировании. Также мощность влияет на разархивирование и
архивирование сообщений.

## Настройка кластеров Kafka

Преимущества кластера:

* Возможность масштабировать
* Возможность использовать репликации для надежности

### Сколько должно быть брокеров

Размер кластера зависит от:

* емкости диска
* емкости реплик на одного брокера
* мощности процессора
* пропускной способности

В настоящее время рекомендуется иметь не более 14000 реплик партиций на брокер и 1 млн. реплик на кластер.

### Конфигурация брокеров

Требования к конфигурации брокеров:

* У всех брокеров должно быть одинаковое значение `zookeeper.connect`
* У каждого брокера в кластере должен быть уникальный `broker.id`

### Тонкая настройка операционной системы

На Linux эти параметры обычно настраиваются в файле `/etc/sysctl.conf`

#### Виртуальная память

Для высокой пропускной способности, следует избегать подкачки страниц памяти на диск, установив `vm.swappines=1`.

При использовании быстрых дисков(SSD) имеет смысл уменьшить количество "грязных" станиц, установив
`vm.dirty_background_ratio=5`.

`vm.dirty_ratio` - процент от всего объема памяти при достижении которой блокируется запись "грязных" станиц и
инициализируется сброс их на диск, разумно устанавливать 60-80, но быть осторожным с этим параметром.

Рекомендованное кол-во файловые дескрипторов для сегментов журнала и открытых
соединений `= (количество_разделов) Х (размер_раздела/размер_сегмента)`. На основе этого устанавливается
параметр `vm.max_map_count`, значение 400к-600к в зависимости от среды дает положительны результат. Также рекомендуется
установить `vm.overcommit_memory=0` иначе ОП будет захватывать слишком много памяти и не давая работать Kafka
оптимально.

#### Диск

В качестве локальной файловой системы чаще задействуется:

* `Ext4` - работает хорошо, но требует потенциально небезопасный параметров
* `XFS` - использует алгоритм отложенного выделения, но более безопасный, чем в `Ext4`

`atime` - время последнего обращения к файлу. Данный параметр неиспользуемая Kafka, поэтому его можно деактивировать,
чтобы лишний раз не нагружать процессор, указав `noatime`.

При больших объемах рекомендуется использовать `largeio`.

#### Передача данных по сети

Вначале нужно изменить объемы памяти, выделяемой для буферов отправки и получения для каждого сокета,
`net.core.wmem_default` и `net.core.rmem_default`, рекомендуется 2097152(2Мб).

Рекомендуется задать буферы отправки и получения для сокетов TCP с помощью `net.ipv4.tcp_wmem` и `net.ipv4.tcp_rmem`.

`net.ipv4.tcp_window_scaling=1` - оконное масштабирования TCP, чтобы клиенты эффективно передавали данные и
буферизировать их на стороне брокера.

`net.ipv4.tcp_max_syn_backlog` - больше чем по-умолчанию 1024, чтобы увеличить одновременное подключение.

`net.ipv4.netdev_max_backlog` - больше чем по-умолчанию 1000, чтобы помочь при всплесках сетевого трафика.

## Промышленная эксплуатация

### Параметры сборки мусора

Рекомендуется использовать сборщик мусора `G1`. Для корректировки производительности используются
параметры: `MaxGCPauseMillis`, `InitiatingHeapOccupancyPercent`. Для Kafka можно задавать более маленькие значения этих
параметров.

Например, для сервера 64 Гбайт оперативной памяти и Kafka работала с кучей 5 Гбайт. Можно
установить `MaxGCPauseMillis=20`, `InitiatingHeapOccupancyPercent=35`.

### Планировка ЦОД

Важно применять репликации и размещать физически брокеров на стойках в ЦОД в среде со зоной отказа.

Рекомендуется размещать реплики партиций на разных полках и в целом разные брокеры на разных полках.

### Размещение приложений на ZooKeeper

Со временем зависимость от ZooKeeper уменьшается и сойдет на нет.

Использования целого ансамбля ZooKeeper для одного кластера необоснованно, достаточно одного ZooKeeper.

Рекомендуется, чтобы потребители использовали для смещения Kafka, а не ZooKeeper.

Не рекомендуется использование одного ансамбля ZooKeeper, для других приложений отличный, не считая разные кластеры
брокеров Kafka.

# Глава 3. Производители Kafka: запись сообщений в Kafka

## Обзор производителя

Общий алгоритм производителя:

1. Для генерации сообщений для Kafka нужно создать объект `ProducerRecord`, включающий топик и значение. Опционально
   можно задать ключ, партицию, временную метку и/или набор заголовков.
2. После отправки `ProducerRecord` он сериализует объекты ключа и значения в байтовый массив для передачи по сети.
3. Если не указали партицию явно, данные попадают в `Partitioner`. Он выбирает партицию, обычно в соответствии с ключом.
   Если партиция указана, то выбор будет очевиден.
4. Далее запись помещается в пакет записей, предназначенных для отправки в соответствующие топики и разделы.
5. После получения брокером, он отправляет ответ. В случае успеха - возвращает объект `RecordMetadata`, содержащий инфу
   о топике, партиции и офсете.
6. Если брокеру не удалось записать запись, вернется сообщение об ошибке.
7. При получении об ошибке, производитель может попробовать отправить несколько раз сообщение.

## Создание производителя Kafka

Обязательные свойства:

* `bootstrap.servers` - список пар `host:port` брокеров для ПЕРВОНОЧАЛЬНОГО соединения с кластером. Не обязательно
  включать все брокеры, но рекомендуется указывать как минимум два, на случай сбоев.
* `key.serializer` - имя класса для сериализации ключей записей. Производителю нужно знать, как преобразовать объекты в
  байтовые массивы. Должен указан класс, реализующего от `org.apache.kafka.serialization.Serializer`. Необходимо задать,
  даже если отправляется только значение, можно применять `VoidSerializer`.
* `value.serializer` - имя класса для сериализации значения записей.

Пример создания производителя:

```java
class Producer {
    void method() {
        Properties kafkaProps = new Properties();
        kafkaProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092, broker2:9092");
        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProps);
    }
}
```

Методы отправки сообщения:

* _Сделать и забыть_ - отправляем на сервер сообщение и не волнуемся.
* _Синхронная отправка_ - Технически производитель всегда асинхронный и при вызове метода `send()` возвращается объект
  `Future`. Однако, можно воспользоваться методом `get()` и ожидать ответа.
* _Асинхронная отправка_ - вызывается метод `send()` и передаем функцию обратного вызова.

## Отправка сообщения в Kafka

Просто вариант:

```java
class Producer {
    void method() {
        ProducerRecord<String, String> record =
                new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        try {
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

```

### Синхронная отправка сообщения

```java
class Producer {
    void method() {
        ProducerRecord<String, String> record =
                new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        try {
            producer.send(record).get();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

```

В классе `KafkaProducer` два типа ошибок.

* Ошибки которые можно исправить, отправив сообщение повторно(retriable). Можно настроить `KafkaProducer`, чтобы при
  таких ошибках отправка была автоматическая.
* Ошибки невозможно исправить повторной отправкой.

### Асинхронная отправка сообщения

```java
class Producer {
    void method() {
        producer.send(record, new DemoProducerCallback());
    }
}

private static class DemoProducerCallback implements Callback {

    @Override
    public void onCompletion(RecordMetadata recordMetadata, Exception e) {
        if (e != null) {
            e.printStackTrace();
        }
    }
}

```

Обратные вызовы выполняются в главном потоке производителя, последовательно. Обратный вызов должен при этом выполняться
достаточно быстро, чтобы не мешать отправке. Не рекомендуются выполнять блокирующие операции внутри обратного вызова, а
следует использовать другой поток.

## Настройка производителей

### client.id

Идентификатор клиента, может быть только строкой.

### acks

Определяет, сколько реплик партиций должны получить запись, прежде чем производитель сможет считать запись успешной.

* `acks=0` - производитель не будет ждать ответа от брокера, чтобы счесть отправку сообщения успешной.
* `acks=1` - производитель получает от брокера ответ об успешном получении сразу же, как только ведущая реплика получит
  сообщение.
* `acks=all` - ответ от брокера об успешном получении сообщения приходит производителю после того, как оно дойдет до
  всех синхронизируемых реплик

При менее надежной конфигурации `acks` будет выше только производительность производителя. Но не сквозной задержкой
между моментами создания сообщения и момента, когда сообщение будет доступно потребителями. В данном случае время
одинаково.

### Время доставки сообщения

Время, за которое Kafka не ответит успешно или пока мы не будем готовы сдаться при повторных отправках.

Временные интервалы затраченные на отправку:

* Время до возвращения асинхронного вызова `send()`. В этот момент вызывающий поток заблокирован.
* Время с момента успешного возвращаемого `send()` до запуска обратного вызова(успешного или с ошибкой).

#### max.block.ms

Время ожидания `send()` и явного запроса метаданных `partitionsFor()`. Иначе ошибка.

#### delivery.timeout.ms

Время с момента, когда готова запись к отправке(функция `send()` возвращается успешно) до момента, пока брокер не
ответил или клиент не откажется. `delivery.timeout.ms >= linger.ms + retry.backo.ms + request.timeout.ms`.

#### request.timeout.ms

Время ожидания производителя ответа от сервера при отправке.

#### Повторные попытки и retry.backoff.ms

`retries` - количество повторных попыток

Ожидание между повторными попытками отправить. Не рекомендуется в текущий версии, лучше подсчитать сколько времени
потребуется для восстановления после сбоя брокера и установить параметр `delivery.timeout.ms` общее время, потраченное
на повторные попытки было больше, времени восстановления кластера.

### linger.ms

Время ожидания дополнительных сообщений перед отправкой текущего пакета. Рекомендуется выставлять значение больше 0.

### buffer.memory

Объем памяти, используемой для буферизации сообщений, ожидающих отправки.

### compression.type

Тип сжатия сообщения. По умолчанию сообщения не сжимаются.

* `snappy` - хорошая степень сжатия при высокой производительности
* `gzip` - лучшая степень сжатия

### batch.size

Объем памяти в байтах для каждого пакета, который собирается перед отправкой в одну партицию.

### max.in.flight.requests.per.connection

Количество пакетов, которые производитель может отправить серверу, не получая ответов.

При активных параметрах `retries > 0` и `max.in.flight.requests.per.connection > 1` может нарушаться последовательность
отправки сообщений. Для сохранения упорядоченности??? и отсутствия дубликатов, необходимо использовать
`enable.idempotence=true`.

### max.request.size

Максимальный размер запроса производителя. Ограничивает максимальный размер сообщения, так и число сообщений, отсылаемых
в одном запросе. Рекомендуется устанавливать равное значению параметру брокера `message.max.bytes`.

### receive.buffer.bytes и send.buffer.bytes

Размеры TCP-буферов отправки и получения, используемых сокетами при записи и чтении данных. `-1` - значения ОП.

### enable.idempotence

Когда активно, производитель будет прикреплять порядковый номер к каждой отправляемой записи, чтобы у брокера не было
дубликатов.

При активации, важно установить значения для следующих параметров:

* `max.in.flight.requests.per.connection <= 5`
* `retries > 0`
* `acks = all`

## Сериализаторы

### Пользовательские сериализаторы

Класс значения:

```java
class Customer {
    private int customerID;
    private String customerName;

    public Customer(int ID, String name) {
        this.customerID = ID;
        this.customerName = name;
    }

    public int getID() {
        return customerID;
    }

    public String getName() {
        return customerName;
    }
}

```

Пользовательский сериализатор:

```java
import java.nio.ByteBuffer;
import java.util.Map;

import org.apache.kafka.common.errors.SerializationException;

public class CustomerSerializer implements Serializer<Customer> {

    @Override
    public void configure(Map configs, boolean isKey) {
        // нечего настраивать
    }

    @Override
    /**
     * Мы сериализуем объект Customer как: 4-байтное целое число, соответствующее
     * customerId 4-байтное целое число, соответствующее длине customerName в байтах в кодировке UTF-8
     * (0, если имя не заполнено) N байт, соответствующих customerName в кодировке UTF-8
     */
    public byte[] serialize(String topic, Customer data) {
        try {
            byte[] serializedName;
            int stringSize;
            if (data == null) return null;
            else {
                if (data.getName() != null) {
                    serializedName = data.getName().getBytes("UTF-8");
                    stringSize = serializedName.length;
                } else {
                    serializedName = new byte[0];
                    stringSize = 0;
                }
            }
            ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + stringSize);
            buffer.putInt(data.getID());
            buffer.putInt(stringSize);
            buffer.put(serializedName);
            return buffer.array();
        } catch (Exception e) {
            throw new SerializationException("Error when serializing Customer to byte[] " + e);
        }
    }

    @Override
    public void close() {
        // нечего закрывать
    }
}

```

Данный подход не рекомендуется, так как код при этом не надежный при изменении класса сообщения.

### Сериализация с помощью Apache Avro

Apache Avro - формат сериализации данных.

Пример схемы Avro:

```json

{
  "namespace": "customerManagement.avro",
  "type": "record",
  "name": "Customer",
  "fields": [
    {
      "name": "id",
      "type": "int"
    },
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "email",
      "type": [
        "null",
        "string"
      ],
      "default": "null"
    }
  ]
}

```

Важные моменты использования данных схем:

* Схема для записи и схема для чтения должны быть совместимы
* У десериализатора должен быть доступ к схеме, задействованной при записи данных, даже если она отличается от схемы,
  которую ожидает обращающееся к данным приложение.

### Использование записей Apache Avro

Можно хранить схемы в записи, но это имеет накладные расходы. Лучше применить паттерн Реестр схем(Schema Registry).
Данную схему будут использовать как производитель, так и потребитель. При в отправляемой в Kafka записи, храним только
идентификатор схемы в реестре.

```java
class Producer {
    void method() {

        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
        props.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
        props.put("schema.registry.url", schemaUrl);

        String topic = "customerContacts";
        Producer<String, Customer> producer = new KafkaProducer<>(props);
        // Генерация новых событий продолжается вплоть до нажатия Ctrl+C
        while (true) {
            Customer customer = CustomerGenerator.getNext();
            System.out.println("Generated customer " + customer.toString());
            ProducerRecord<String, Customer> record =
                    new ProducerRecord<>(topic, customer.getName(), customer);
            producer.send(record);
        }
    }
}

```

## Партиции

Ключи сообщения служат двум целям:

1. Дополнительная информация, сохраняемая вместе с сообщением
2. Для определения, в какую партицию записывать. Все сообщения с одинаковым ключом попадут в одну партицию.

При создании объекта `ProducerRecord` с неопределенным ключом, используется метод секционирования по умолчанию.
`ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "France");`

Разделитель по умолчанию и ключ `null` - запись отправиться в один из доступных партиций случайным образом с
использованием циклического алгоритма.

Если ключ есть и разделитель по умолчанию - Kafka вычисляет хеш-значение ключа собственным алгоритмом и отправляет
сообщение в конкретный раздел. Для вычисления нужны все партиции раздела, поэтому если один не доступен - будет ошибка.

Существуют также разделители:

* `RoundRobinPartitioner` - случайное назначение разделов
* `UniformStickyPartitioner` - "липкое" случайное назначение разделов(для балансировки нагрузки)

Когда применяется разделитель по умолчанию, соответствие ключей партициям остается согласованным до тех пор, пока число
партиций не меняется. Если ключи важны для распределения, простейшим решением будет создавать топики с достаточным
числом партиций(https://oreil.ly/ortRk).

### Реализация пользовательской стратегии секционирования

Например, есть B2B-поставщик(producer) и покупатель(consumer). На одного покупателя, приходится большой процент всех
покупок. Поэтому рекомендуется выделить в отдельную партицию инфу о его покупках:

```java
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.record.InvalidRecordException;
import org.apache.kafka.common.utils.Utils;

public class BananaPartitioner implements Partitioner {
    public void configure(Map<String, ?> configs) {
    }

    public int partition(
            String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        if ((keyBytes == null) || (!(key instanceof String))) {
            throw new InvalidRecordException("We expect all messages " + "to have customer name as key");
        }
        if (key.equals("Banana")) return numPartitions - 1; // Banana всегда попадает в последний раздел
        // Другие записи распределяются по разделам путем хеширования
        return Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1);
    }

    public void close() {
    }
}

```

## Заголовки

Заголовки позволяют добавлять метаданные о записи, не добавляя никакой информации к самой записи.

Варианты использования для указания: источника данных, маршрутизации или отслеживания сообщения.

```java
class Producer {
    void method() {

        ProducerRecord<String, String> record =
                new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        record.headers().add("privacy-level", "YOLO".getBytes(StandardCharsets.UTF_8));
    }
}

```

## Перехватчики

```java
public class CountingProducerInterceptor implements ProducerInterceptor<Object, Object> {

    ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();
    static AtomicLong numSent = new AtomicLong(0);
    static AtomicLong numAcked = new AtomicLong(0);

    public void configure(Map<String, ?> map) {
        long windowSize = Long.parseLong((String) map.get("counting.interceptor.window.size.ms"));
        executorService.scheduleAtFixedRate(
                CountingProducerInterceptor::run, windowSize, windowSize, TimeUnit.MILLISECONDS);
    }

    public ProducerRecord<Object, Object> onSend(ProducerRecord<Object, Object> producerRecord) {
        numSent.incrementAndGet();
        return producerRecord;
    }

    public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {
        numAcked.incrementAndGet();
    }

    public void close() {
        executorService.shutdownNow();
    }

    public static void run() {
        System.out.println(numSent.getAndSet(0));
        System.out.println(numAcked.getAndSet(0));
    }
}

```

## Квоты и регулирование запросов

Механизм квотирования - ограничивать скорость создания и потребления сообщений.

Типы:

1. На производство
2. На потребление
3. На запрос

п.1 и п.2 - ограничивают скорость, с которой клиенты могут отправлять и получать данные, измеряются байт/сек. п.3 -
ограничивают процент времени, обработки клиентских запросов.

Можно применять для всех клиентов, для конкретных идентификаторам клиентов, конкретным пользователям.

Квоты можно установить в конфигурационном файле, но это настройка статично и не рекомендуется.

Для динамичной установки квот рекомендуется `kafka-config.sh` или `API AdminClient`.

Примеры:

1. Ограничение клиента C на выдачу только 1024 байт/с:

`bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024' --entity-name clientC --entity-type clients`

2. Ограничение пользователя 1 на выдачу только 1024 байт/с и потребление только 2048 байт/с:

`bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048' --entity-name user1 --entitytype users`

3. Ограничение на потребление только 2048 байт/с всех пользователей, за исключением имеющих более конкретные настройки:

`bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config 'consumer_byte_rate=2048' --entity-type users`

# Глава 4. Потребители Kafka: чтение данных из Kafka

## Принципы работы потребителей Kafka

### Потребители и группы потребителй

Если несколько потребителей подписаны на один топик и относятся к одной группе, они будут получать сообщения из разных
подмножеств партиций.

* Если есть 1 топик, 4 партиции и 1 группа потребителей и 1 потребитель. Потребитель будет читать сообщения из всех
  разделов.
* Если есть 1 топик, 4 партиции и 1 группа потребителей и 2 потребителя. Каждый потребитель будет читать из 2х партиций.
* Если есть 1 топик, 4 партиции и 1 группа потребителей и 4 потребителей. Каждый потребитель будет читать из 1ой
  партиции.
* Если есть 1 топик, 4 партиции и 1 группа потребителей и 5 потребителей. 4 потребителя будут читать из 1ой партиции, а
  1 потребитель будет простаивать.

Основной способ масштабирования получения данных - добавление потребителей в группу. Потребители выполняют операции с
большим значением задержки, чем производители.

Можно, но не рекомендуется масштабировать чтение, за счет создания новых групп и чтения в рамках одного приложения, т.е.
разных сообщений/партиций.

### Группы потребителей и переблалансировка разделов

Передача партиции от одного потребителя другому называется перебалансировка(rebalance).

Типы перебалансировки:

* _Безотлагательная перебалансировка_. Потребители прекращают потребление, отказываются от своих прав владения
  разделами, снова присоединится к группе потребителей и получают совершенное новое назначение разделов.
* _Совместная перебалансировка_. Инкрементальная перебалансировка, включает в себя перебаланисровку небольшого
  подмножество разделов от одного потребителя к другому. Более долгая, но не прерывает работу.

Потребители поддерживают членство в группе(активность) и принадлежность разделов за счет отправки координатору группы
брокеров периодических контрольных сигналов.

Если потребитель длительное время прекращает отправку контрольных сигналов, время его сеанса истекает, координатор
группы признает его неработающим и инициирует перебалансировку.

### Статические участники группы

Идентификация потребителя в группе - временная, если потребитель покинет и зайдет в группу идентификация поменяется.

Можно установить потребителя с уникальным идентификатором `group.instance.id`. Если такие потребители выключаться, он не
покидает группу, а останется пока его сессия не завершиться. После переподключения к группе ему назначаться теже
партиции.

Статическое членство полезно, когда приложение поддерживает локальное состояние или кэш, который заполняется разделами,
назначенными каждому потребителю. Однако партиции не будут переназначаться другим потребителям.

Статические участники не покидают группу проактивно при выключении, это зависит от `session.timeout.ms`. Данный параметр
должен быть достаточно высоким, чтобы не вызывать перебалансировку при простом перезапуске приложения, но достаточно
низким, обеспечивая автоматическое переназначение разделов потребителей при более длительном времени простоя.

## Создание потребителя Kafka

Потребитель представляет собой экземпляр класса `KafkaConsumer`. Для его создания требуются экземпляр `Properties`.
Обязательные свойства:

* `bootstrap.servers` - строка подключения к кластеру
* `key.deserializer` - десериализатор ключа
* `value.deserializer` - десериализатор значения

`group.id` - задает группу потребителей. Не является обязательным, но очень широко используется.

```java
public class ConsumerMain {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092,broker2:9092");
        props.put(CommonClientConfigs.GROUP_ID_CONFIG, "CountryCounter");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
    }
}

```

## Подписание на топик

Чтобы подписаться на топик, нужно передать список топиков методу `consumer.subscribe(List.of("customerCountries"))`.

Также можно передать название топиков через регулярное выражение `consumer.subscribe(Pattern.compile("test.*"))`.
Фильтрация топиков для подписки осуществляется на стороне клиента. То есть при использовании регулярного выражения,
наличии множества топиков и партиций, и множества клиентов. Будет большой трафик на предоставление метаданных, может
даже больше, чем для отправки данных.

## Цикл опроса

В самой основе API потребителя лежит простой цикл.

```java
class ConsumerMain {
    void method() {
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf(
                        "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());

                custCountryMap.merge(record.value(), 1, (oldValue, newValue) -> oldValue + 1);

                JSONObject json = new JSONObject(custCountryMap);
                System.out.println(json);
            }
        }
    }
}

```

Получение данных производится методом `poll()`. Передаваемый параметр - длительность ожидания ответа. При первом вызове
метода `poll()` для нового потребителя он отвечает за поиск координатора группы, присоединение потребителя к группе и
назначение ему разделов.

Если `poll()` не вызывается дольше, чем значение `max.poll.interval.ms`, потребитель станет считаться мертвым и будет
вычеркнут из группы потребителей, поэтому стоит избегать длительных дополнительных действий при обработке сообщения.

### Потокобезопасность

Железное правило: один потребитель на один поток. Полезно обернуть логику потребителя в его собственный объект, а затем
использовать`ExecutorService` для запуска нескольких потоков, каждый с собственным потребителем(https://oreil.ly/8YOVe).

Другой подход: один потребитель заполнял очередь событий, а несколько потоков выполняли работу из
нее(https://oreil.ly/uMzj1).

## Настройка потребителей

### fetch.min.bytes

Минимальный объем данных, получаемых от брокера при извлечении записей. По умолчанию 1 байт. Если размер сообщений
меньше, то брокер будет ждать ещё сообщения для отправки потребителю. Рекомендуется повышать значение, но с учетом
повышения задержки с низкой пропускной способностью.

### fetch.max.wait.ms

Максимальное время ожидания, пока не наберется размер сообщение больше `fetch.min.bytes`. То есть либо передача
сообщений пройдет по `fetch.min.bytes`, или `fetch.max.wait.ms`.

### fetch.max.bytes

Максимальное количество байтов, которые Kafka будет возвращать каждый раз, когда потребитель опрашивает брокер. Если
первый пакет записи будет превышать этого размера при отправке, то в данной отправке параметр игнорируется.

### max.poll.records

Максимальное количество записей, возвращаемое по вызову `poll()`.

### max.partition.fetch.bytes

Максимальное число байтов, возвращаемых сервером из расчета на один раздел. Конфигурация сложная, рекомендуется
использовать `max.poll.records`.

### session.timeout.ms и heartbeat.interval.ms

`session.timeout.ms` - время ожидания ответа контрольного сигнала от потребителя, после которого он будет считаться
отключенным. `heartbeat.interval.ms` - частота отправки потребителем контрольных сигналов. Данные параметры желательно
менять одновременно. При низком значении, возможно ранее выявить не работающий потребитель, но может привести к частой
перебалансировке.

### max.poll.interval.ms

Время, в течение которого потребитель может обходиться без опроса(`poll()`), прежде чем будет признан мертвым.

### default.api.timeout.ms

Тайм-аут, который будет применяться ко всем вызовам API, не указанный явно.

### request.timeout.ms

Максимальное время, в течение которого потребитель будет ожидать ответа от брокера. При превышении, закроет соединение и
попытается подключиться снова. Не рекомендуется уменьшать дефолтное значение.

### auto.offset.reset

Поведение потребителя при начале чтения партиции, для которого у него зафиксированное смещение отсутствует или стало
некорректным.

* `latest` - в отсутствие корректного смещения потребитель начинает читать самые свежие
* `earliest` - начинает читать все данные из раздела с начала
* `none` - возникает исключение при попытках пользования с недопустимым смещением

### enable.auto.commit

Будет ли потребитель фиксировать смещение автоматически.

### partition.assignment.strategy

Определяет, какие разделы к какому потребителю будут относиться

* **Range (Диапазонная)**. Каждому потребителю присваиваются последовательные подмножества разделов из топиков, на
  которые он подписан.
* **RoundRobin (Циклическая)**. Все разделы от всех партиций подписанных топиков распределяются по потребителям
  последовательно, один за другим.
* **Sticky (Липкая)**. Первая цель: получить максимально сбалансированное назначение. Вторая: оставить как можно больше
  назначений на месте.
* **Cooperative Sticky (Совместная липкая)**. Такая же, как `Липкая`, но поддерживает совместные перебалансировки, при
  которых потребители могут продолжать потреблять из разделов, которые не были переназначены.

### client.id

Используют брокеры для идентификации отправленных клиентом запросов. Применяются для логирования и для показателей, при
задании квот.

### client.rack

По-умолчанию потребители получают сообщения от ведущей реплики каждой партиции. Реплики могут находится физически в
разных местах. Данная настройка позволяет включить выборку из физически ближайшей реплики относительно потребителя.

### group.instance.id

Предоставление потребителю статического членства в группе.

### receive.buffer.bytes и send.buffer.bytes

Размер TCP-буферов отправки и получения, применяемых сокетами при записи и чтения. Рекомендуется повышать, когда клиенты
взаимодействуют с брокерами из другого ЦОД.

### offsets.retention.minutes

Настройка брокера, влияет на потребителей. Последнее смещение храниться в Kafka, до тех пор пока есть активные члены
группы. Если группа неактивна, то через время указанное в настройке смещение сбрасывается.

## Фиксация и смещение

Метод `poll()` возвращает ещё не прочитанные сообщения. Потребитель фиксирует последнее сообщение, которое успешно
обработано из партиции. Это называется фиксацией смещения(offset commit).

Потребитель отправляют сообщение в брокер, которое обновляет специальный топик `__consumer_offsets`, содержащий смещение
для каждого раздела. После перебалансировки, новый потребитель будет знать с какого смещения было последнее чтение.

### Автоматическая фиксация

При активном `enable.auto.commit` потребитель каждые 5с (настраивается `auto.commit.interval.ms`) проверяет
необходимость фиксации. Если есть такая необходимость, то при следующем вызове poll() будет фиксировать последнее
смещение, возвращенное предыдущим вызовом.

Автоматическая фиксация удобна, но она не позволяет управлять так, чтобы избежать дублирования в случае сбоев.

### Фиксация текущего смещения

При отключенном `enable.auto.commit` фиксация будет происходить при явном указании фиксации в коде.

```java
public class ConsumerMain {

    public static void main(String[] args) {
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>();

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf(
                        "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
            }

            // Фиксация смещения после обработки всего пакета сообщений
            try {
                consumer.commitSync();
            } catch (CommitFailedException e) {
                log.error("commit failed", e);
            }
        }
    }
}

```

Важно не вызывать метод `commitSync()` до окончания обработки последней записи в пакете сообщения.

### Асинхронная фиксация

Минусы синхронной фиксации - приложение заблокировано, пока не придет ответ фиксации. При асинхронной такого минуса нет.

Метод асинхронной фиксации: `commitAsync()`.

Минус данного подхода, что `commitAsync()` не повторяет попытку фиксации. Причина: на момент получения `commitAsync()`
ответа от сервера уже может быть успешно выполнена более поздняя фиксация.

Также можно передать функцию обратного вызова применяемую при ответе брокера. Их часто используют для каналирования
ошибок фиксации или их подсчета в виде показателей:

```java
public class ConsumerMain {

    public static void main(String[] args) {

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf(
                        "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
            }

            // Асинхронная фиксация с обратным вызовом
            consumer.commitAsync(
                    (offsets, e) -> {
                        if (e != null) log.error("Commit failed for offsets {}", offsets, e);
                    });
        }
    }
}
```

Простой способ обеспечить правильный порядок при асинхронных повторах:

1. Увеличивайте порядковый номер при каждой фиксации и вставьте его во время фиксации в обратный вызов `commitAsync()`.
2. Когда будете готовы отправить повторный запрос, проверьте, равен ли порядковый номер фиксации в обратном вызове
   переменной экземпляра.
3. Если да, то более поздняя фиксация не выполнялась и можно спокойно пробовать отправить запрос
   еще раз.
4. Если же значение переменной экземпляра больше, не нужно пробовать повторно отправлять запрос, потому что уже был
   сделан более поздний запрос на фиксацию.

Пример из https://stackoverflow.com/a/53244658/17965846

```java
class ConsumerCommitAsync {

    public static void main(String[] args) {
        Properties props = new Properties();
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        try {
            AtomicInteger atomicInteger = new AtomicInteger(0);
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(5);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }

                consumer.commitAsync(
                        new OffsetCommitCallback() {
                            private final int marker = atomicInteger.incrementAndGet();

                            @Override
                            public void onComplete(
                                    Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
                                if (exception != null) {
                                    if (marker == atomicInteger.get()) consumer.commitAsync(this);
                                } else {
                                    // Can't try anymore
                                }
                            }
                        });
            }
        } catch (WakeupException e) {
            // ignore for shutdown
        } finally {
            consumer.commitSync(); // Block
            consumer.close();
            System.out.println("Closed consumer and we are done");
        }
    }
}

```

### Сочетание асинхронной и синхронной фиксации

Случайные сбои при фиксации - незначительная помеха, но если речь идет о последней фиксации перед закрытием потребителя
или перебалансировкой, то лучше позаботится, чтобы он точно оказалась успешной. Для этого сочетают асинхронную и
синхронную фиксацию:

```java
public class ConsumerCommitAsyncAndSync {
    public static volatile boolean closing = false;

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092,broker2:9092");
        props.put(CommonClientConfigs.GROUP_ID_CONFIG, "CountryCounter");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            Duration timeout = Duration.ofMillis(100);
            while (!closing) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %s, offset = %d, customer = %s, country = %s",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }
                consumer.commitAsync();
            }
            consumer.commitSync();
        } catch (Exception e) {
            log.error("Unexpected error", e);
        }
    }
}

```

### Фиксация заданного смещения

Если требуется выполнять фиксацию смещения раньше, чем после полного обработки пакетов. Возможно вызвать `commitAsync()`
или `commitSync()` с конкретным оффсетом:

```java
public class ConsumerCommitCurrent {

    public static void main(String[] args) {

        Properties props = new Properties();
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {

            Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
            int count = 0;

            Duration timeout = Duration.ofMillis(100);
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    currentOffsets.put(
                            new TopicPartition(record.topic(), record.partition()),
                            new OffsetAndMetadata(record.offset() + 1, "no metadata"));
                    if (count % 1000 == 0) consumer.commitAsync(currentOffsets, null);
                    count++;
                }
            }
        }
    }
}

```

## Прослушивание на предмет перебалансировки

Во время смены принадлежности патриций потребителю, возможно задать необходимое поведение с помощью объекта
`ConsumerRebalanceListener` передав его в метода `subscribe()`.

У `ConsumerRebalanceListener` методы для реализации:

1. `void onPartitionsRevoked(Collection<TopicPartition> partitions)` - вызывается после переназначения разделов
   потребителю, но до того как он начнет получать сообщения.
2. `void onPartitionsAssigned(Collection<TopicPartition> partitions)` - При безотлагательной перебалансировки вызывается
   до начала преебалансировки и после того, как потребитель стал получать сообщения. При совместной перебалансировки -
   вызывается в конце перебалансировке, только с тем подмножеством партиций, от которых потребитель должен отказаться.
3. `void onPartitionsLost(Collection<TopicPartition> partitions)` - дефолтно делегирует методу `onPartitionsAssigned()`.
   Вызывается только при использовании совместного алгоритма перебалансировки и только в исключительных случаях, когда
   разделы были назначены другим потребителям без предварительного отзыва алгоритма перебалансировки.

Если используется алгоритм совместной перебалансировки, есть следующие момент:

* `onPartitionsAssigned()` будет вызываться при каждой перебалансироке, как способ уведомления. Если нет новых разделов,
  вызывиться с пустой коллекцией.
* `onPartitionsRevoked()` будет вызываться только в том случае, если потребитель отказался от прав владения разделом.
* `onPartitionsLost()` будет вызываться в исключительных условиях перебалансироки, к моменту вызова в коллекции у
  партиций будут новые владельцы.

```java
public class ConsumerMainRebalanceListener {

    private static final KafkaConsumer<String, String> consumer = getConsumer();
    private static final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
    private static final Duration timeout = Duration.ofMillis(100);

    public static void main(String[] args) {

        try {
            consumer.subscribe(List.of("topic"), new HandleRebalance());
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    currentOffsets.put(
                            new TopicPartition(record.topic(), record.partition()),
                            new OffsetAndMetadata(record.offset() + 1, null));
                }
                consumer.commitAsync(currentOffsets, null);
            }
        } catch (WakeupException e) {
            // Игнорируем, поскольку закрываемся
        } catch (Exception e) {
            log.error("Unexpected error", e);
        } finally {
            try {
                consumer.commitSync(currentOffsets);
            } finally {
                consumer.close();
                System.out.println("Closed consumer and we are done");
            }
        }
    }

    private static class HandleRebalance implements ConsumerRebalanceListener {
        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
        }

        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
            System.out.println(
                    "Lost partitions in rebalance. " + "Committing current offsets:" + currentOffsets);
            consumer.commitSync(currentOffsets);
        }
    }
}
```

## Получение записей с заданными смещениями

Если необходимо получать конкретные смещения:

* `seekToBeginning()` - начать с начала для топиков
* `seekToEnd()` - начать с конца для топиков

Установить текущее смещение для всех разделов:

```java
        long oneHourEarlier =
                Instant.now().atZone(ZoneId.systemDefault()).minusHours(1).toEpochSecond();
        Map<TopicPartition, Long> partitionTimestampMap =
                consumer.assignment().stream().collect(Collectors.toMap(tp -> tp, tp -> oneHourEarlier));
        Map<TopicPartition, OffsetAndTimestamp> offsetMap =
                consumer.offsetsForTimes(partitionTimestampMap);
        for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetMap.entrySet()) {
            consumer.seek(entry.getKey(), entry.getValue().offset());
        }

```

## Выход из цикла

Если требуется выключить потребителя и выйти немедленно из цикла опроса, может быть проблема длительного ожидания
`poll()`. Для этого можно воспользоваться ещё одним потоком для вызова метода `wakeup()`, если цикл опроса в главном
потоке можно воспользоваться `ShutdownHook`. Если `wakeup()` был вызван в момент, когда поток не ожидает опроса, то
исключение `WakeupException` будет вызвано в следующем вызове `poll()`.

```java
    // Registering a shutdown hook so we can exit cleanly
    Runtime.getRuntime()
        .addShutdownHook(
            new Thread(
                () -> {
                  System.out.println("Starting exit...");
                  // Note that shutdownhook runs in a separate thread, so the only thing we can
                  // safely do to a consumer is wake it up
                  movingAvg.consumer.wakeup();
                  try {
                    mainThread.join();
                  } catch (InterruptedException e) {
                    e.printStackTrace();
                  }
                }));

```

Подробнее http://bit.ly/2u47e9A.

## Десериализаторы

Сериализатор применяемый при отправке сообщения, должен соответствовать десериализатору при получении. В этом плане
реестр схем `AvroSerializer` гарантирует, все записываемые данные совместимы со схемой топика, а значит, их можно
десериализовать с помощью соответствующего десериализатора и схемы.

### Пользовательские сериализаторы

Реализовывать пользовательские десериализаторы и сериализаторы не рекомендуется, оно приводит к сильному сцеплению
производителей и потребителей. Лучше использовать формат `JSON`, `Trift`, `Protobuf` или `Avro`.

### Использование десериализаци Avro в потребителе Kafka

```java
    Duration timeout = Duration.ofMillis(100);
    Properties props = new Properties();
    props.put("bootstrap.servers", "broker1:9092,broker2:9092");
    props.put("group.id", "CountryCounter");
    props.put("key.serializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroDeserializer");
    props.put("specific.avro.reader", "true");
    props.put("schema.registry.url", schemaUrl);
    String topic = "customerContacts";
    KafkaConsumer<String, Customer> consumer = new KafkaConsumer<>(props);
    consumer.subscribe(Collections.singletonList(topic));
    System.out.println("Reading topic:" + topic);
    while (true) {
      ConsumerRecords<String, Customer> records = consumer.poll(timeout);
      for (ConsumerRecord<String, Customer> record : records) {
        System.out.println("Current customer name is: " + record.value().getName());
      }
      consumer.commitSync();
    }

```

## Автономный потребитель: зачем и как использовать потребитель без группы

Можно назначить конкретные разделы для чтения. Пользователям может или подписываться на топик и состоять в группе, или
назначать себе разделы, но не то и другое одновременно.

```java
    Duration timeout = Duration.ofMillis(100);
    List<PartitionInfo> partitionInfos = consumer.partitionsFor("topic");
    List<TopicPartition> partitions = new ArrayList<>();
    if (partitionInfos != null) {
      for (PartitionInfo partition : partitionInfos)
        partitions.add(new TopicPartition(partition.topic(), partition.partition()));
      consumer.assign(partitions);
      while (true) {
        ConsumerRecords<String, String> records = consumer.poll(timeout);
        for (ConsumerRecord<String, String> record : records) {
          System.out.printf(
              "topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
              record.topic(), record.partition(), record.offset(), record.key(), record.value());
        }
        consumer.commitSync();
      }

```

В данном случае, если добавляются новые разделы в топик, то необходимо об этом позаботится самостоятельно: либо
периодически обращаться к `consumer.partitionsFor()` или просто перезапуская приложение при добавлении разделов.

# Глава 5. Программное управление Apache Kafka

Возникают ситуации, когда требуется реализовать пользовательское управление кластера Kafka, например, клиентское
создание топика. Для этого предоставлен API `AdminClient`.

## Обзор AdminClient

### Асинхронный и в конечном итоге согласованный API

`AdminClient` - асинхронный, каждый метод возвращает один или несколько `Future`. При этом они обернуты в `Result`,
предоставляющий методы ожидания завершения операции и вспомогательные методы.

Распространение метаданных к брокерам является асинхронным, возвращаемый `Future` считается завершенным, когда состояние
контроллера полностью обновлено. На этом этапе не все брокеры могут быть осведомлены о новом состоянии.

### Опции

Каждый метод `AdminClient` принимает аргумент объекта `Options`, специфичный для метода. Единственная общая настройка
`timeoutMs` - как долго клиент будет ждать ответ от кластера.

### Плоская иерархия

Все операции администратора реализуются в `KafkaAdminClient`, без иерархии. Это сделано, чтобы производит поиск нужного
метода, только в одном классе.

### Дополнительные примечания

Все операции изменения состояния кластера обрабатываются контроллером. Операции, считывающие состояние кластера и
описание, могут обрабатываться любым брокером и направляются наименее загруженному. Это может быть полезно в случае
неожиданного поведения, например, операция занимает слишком много времени.

## Жизненный цикл AdminClient: создание, настройка и закрытие

Создание:

```java
    Properties props = new Properties();
    props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    AdminClient admin = AdminClient.create(props);

    admin.close(Duration.ofSeconds(30));

```

Единственный обязательный параметр - список URL брокеров кластера. Требуется на проде указать не менее трех.

При вызове метода `close` нужно учитывать, что некоторые операции `AdminClient` в процессе выполнения. Если указать
тайм-аут, то будет ожидание выполнения заданное время, если нет - то будет ожидать пока все операции не завершаться.

### client.dns.lookup

По умолчанию Kafka создает соединение на основе имени хоста. Но не хватает два сценария:

#### Использование псевдонима DNS

Если есть несколько брокеров с указанием адреса как псевдонима. Чтобы не указывать их всех в конфигурации сервера, можно
создать единый псевдоним `DNS`, который будет с ними сопоставляться. Это не удобно при использовании `SASL`. В этом
сценарии лучше использовать `client.dns.lookup=resolve_canonical_bootstrap_servers_only`. При такой конфигурации клиент
`израсходует` псевдоним `DNS`.

#### DNS-имя с несколькими IP-адресами

В современных сетевых архитектурах принято размещать все брокеры за прокси-сервером или балансировщиком нагрузки,
особенно в `Kubernetes`. Очень часто указывается список IP-адресов, все из которых разрешаются для балансировщиков
нагрузки, и все они направляют трафик на один и тот же брокер. `client.dns.lookup=use_all_dns_ips` рекомендуется
использовать, чтобы клиент не упустил преимущества высокодоступного уровня балансировки нагрузки.

### request.timeout.ms

Время, которое приложение может потратить на ожидание ответа `AdminClient`. Сюда входит время на повторную попытку. По
умолчанию 120с, можно уменьшить, но при этом использовать параметр тайм-аута в параметре метода `AdminClient`.

## Управление основными топиками

Распространенные сценарии - управление топиками(получение списка, описание, создание, удаление).

Все топики в кластере:

`ListTopicsResult listTopics = admin.listTopics()`

`ListTopicsResult` является оберткой над коллекцией `Futures`.

Получить список топиков, и если нет одного, то создать его:

```java
    DescribeTopicsResult demoTopic = admin.describeTopics(TOPIC_LIST);
    try {
      topicDescription = demoTopic.topicNameValues().get(TOPIC_NAME).get();
      System.out.println("Description of demo topic:" + topicDescription);
      if (topicDescription.partitions().size() != NUM_PARTITIONS) {
        System.out.println("Topic has wrong number of partitions. Exiting.");
        System.exit(-1);
      }
    } catch (ExecutionException | InterruptedException e) {
      // exit early for almost all exceptions
      if (!(e.getCause() instanceof UnknownTopicOrPartitionException)) {
        e.printStackTrace();
        throw e;
      }
      // if we are here, topic doesn't exist
      System.out.println("Topic " + TOPIC_NAME + " does not exist. Going to create it now");
      // Note that number of partitions and replicas is optional. If they are
      // not specified, the defaults configured on the Kafka brokers will be used
      CreateTopicsResult newTopic =
          admin.createTopics(List.of(new NewTopic(TOPIC_NAME, NUM_PARTITIONS, REP_FACTOR)));
      // Check that the topic was created correctly:
      if (newTopic.numPartitions(TOPIC_NAME).get() != NUM_PARTITIONS) {
        System.out.println("Topic has wrong number of partitions.");
        System.exit(-1);
      }
    }
```

Удаление топиков:

```java
    admin.deleteTopics(TOPIC_LIST).all().get();
    // Check that it is gone. Note that due to the async nature of deletes,
    // it is possible that at this point the topic still exists
    try {
      topicDescription = demoTopic.topicNameValues().get(TOPIC_NAME).get();
      System.out.println("Topic " + TOPIC_NAME + " is still around");
    } catch (ExecutionException e) {
      System.out.println("Topic " + TOPIC_NAME + " is gone");
    }
```

Удаление топика в Kafka безвозвратный процесс - нужно быть осторожным.

Операции администрирование выполняются не часто, поэтому приемлемо вызывать метод `get()` и ожидать его выполнения.
Кроме тех сервисов, который будет обрабатывать большое количество административных запросов.

Пример сервера с асинхронным ответом:

```java

    vertx
        .createHttpServer()
        .requestHandler(
            request -> {
              String topic = request.getParam("topic");
              String timeout = request.getParam("timeout");
              int timeoutMs = NumberUtils.toInt(timeout, 1000);
              DescribeTopicsResult demoTopic =
                  admin.describeTopics(
                      List.of(topic), new DescribeTopicsOptions().timeoutMs(timeoutMs));
              demoTopic
                  .topicNameValues()
                  .get(topic)
                  .whenComplete(
                      (topicDescription, throwable) -> {
                        if (throwable != null) {
                          var chunk =
                              "Error trying to describe topic %s due to %s"
                                  .formatted(topic, throwable.getMessage());
                          request.response().end(chunk);
                        } else {
                          request.response().end(topicDescription.toString());
                        }
                      });
            })
        .listen(8080);

```

## Управление конфигурацией

Управление конфигурацией осуществляется путем описания и обновления коллекций `ConfigResource`. Ресурсы: брокеры,
регистраторы брокеров и топиков. Проверка и изменение конфигурации брокеров обычно выполняется инструментами, например
`kafka-config.sh`, а топиков из приложений.

Проверка, что топик сжат и действия по исправлению его конфигурации:

```java

    ConfigResource configResource = new ConfigResource(ConfigResource.Type.TOPIC, TOPIC_NAME);
    DescribeConfigsResult configsResult = admin.describeConfigs(List.of(configResource));
    Config configs = configsResult.all().get().get(configResource);
    // print nondefault configs
    configs.entries().stream().filter(entry -> !entry.isDefault()).forEach(System.out::println);
    // Check if topic is compacted
    ConfigEntry compaction =
        new ConfigEntry(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);
    if (!configs.entries().contains(compaction)) {
      // if topic is not compacted, compact it
      Collection<AlterConfigOp> configOp = new ArrayList<>();
      configOp.add(new AlterConfigOp(compaction, AlterConfigOp.OpType.SET));
      Map<ConfigResource, Collection<AlterConfigOp>> alterConf = new HashMap<>();
      alterConf.put(configResource, configOp);
      admin.incrementalAlterConfigs(alterConf).all().get();
    } else {
      System.out.println("Topic " + TOPIC_NAME + " is compacted topic");
    }

```

## Управление группами потребителей

### Изучение групп потребителей

Список групп потребителей, возвращает только те группы, которые кластер вернул без ошибок:

`admin.listConsumerGroups().valid().get().forEach(System.out::println)`

Дополнительная информация о некоторых группах:

`admin.describeConsumerGroups(CONSUMER_GRP_LIST).describedGroups().get(CONSUMER_GROUP).get()`

Получение информации о зафиксированном смещении в группе, и насколько оно отстает от последних сообщений в журнале.

```java

    Map<TopicPartition, OffsetAndMetadata> offsets =
        admin.listConsumerGroupOffsets(CONSUMER_GROUP).partitionsToOffsetAndMetadata().get();
    Map<TopicPartition, OffsetSpec> requestLatestOffsets = new HashMap<>();
    for (TopicPartition tp : offsets.keySet()) {
      requestLatestOffsets.put(tp, OffsetSpec.latest());
    }

    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> latestOffsets =
        admin.listOffsets(requestLatestOffsets).all().get();
    for (Map.Entry<TopicPartition, OffsetAndMetadata> e : offsets.entrySet()) {
      String topic = e.getKey().topic();
      int partition = e.getKey().partition();
      long committedOffset = e.getValue().offset();
      long latestOffset = latestOffsets.get(e.getKey()).offset();
      
      System.out.println(
          "Consumer group "
              + CONSUMER_GROUP
              + " has committed offset "
              + committedOffset
              + " to topic "
              + topic
              + " partition "
              + partition
              + ". The latest offset in the partition is "
              + latestOffset
              + " so consumer group is "
              + (latestOffset - committedOffset)
              + " records behind");
    }

```

### Модификация групп потребителей

Наиболее полезная функция модификации групп потребителей - изменения смещения.

Группы потребителей не получают обновлений при изменении смещений в топике смещений. Они читают смещения только тогда,
когда потребителю назначается новый раздел, или при запуске. Чтобы предотвратить внесение изменений в смещения, о
которых потребители не будут знать, Kafka не позволит изменять смещения, пока группа потребителй активна.

Сброс смещения:

```java

    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> earliestOffsets =
        admin.listOffsets(requestEarliestOffsets).all().get();
    Map<TopicPartition, OffsetAndMetadata> resetOffsets = new HashMap<>();
    for (Map.Entry<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> e :
        earliestOffsets.entrySet()) {
      resetOffsets.put(e.getKey(), new OffsetAndMetadata(e.getValue().offset()));
    }
    try {
      admin.alterConsumerGroupOffsets(CONSUMER_GROUP, resetOffsets).all().get();
    } catch (ExecutionException e) {
      System.out.println(
          "Failed to update the offsets committed by group "
              + CONSUMER_GROUP
              + " with error "
              + e.getMessage());
      if (e.getCause() instanceof UnknownMemberIdException)
        System.out.println("Check if consumer group is still active.");
    }

```

Одно из распространенных причин сбоя `alterConsumerGroupOffsets` - до этого не остановленная группа потребителей.

## Метаданные кластера

Информации о кластере к которому подключились:

```java

    DescribeClusterResult cluster = admin.describeCluster();
    System.out.println("Connected to cluster " + cluster.clusterId().get());
    System.out.println("The brokers in the cluster are:");
    cluster.nodes().get().forEach(node -> System.out.println(" * " + node));
    System.out.println("The controller is: " + cluster.controller().get());

```

## Расширенные операции администратора

Полезны во время инцидентов.

### Добавление разделов в топик

Обычно количество партиций задается во время создания топика. И не меняется со временем работы.

Если, довольно редко, бывают случаи достижения предела пропускной способности, можно добавить партиции.

```java

    Map<String, NewPartitions> newPartitions = new HashMap<>();
    newPartitions.put(TOPIC_NAME, NewPartitions.increaseTo(NUM_PARTITIONS + 2));
    admin.createPartitions(newPartitions).all().get();

```

### Удаление записей из топика

В Kafka правила хранения для топиков не были реализованы, чтобы гарантировать соблюдение законов конфиденциальности.
Топик с хранением в течение 30 дней может хранить более старые данные.

```java

    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> olderOffsets =
        admin.listOffsets(requestOlderOffsets).all().get();
    Map<TopicPartition, RecordsToDelete> recordsToDelete = new HashMap<>();
    for (Map.Entry<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> e :
        olderOffsets.entrySet())
      recordsToDelete.put(e.getKey(), RecordsToDelete.beforeOffset(e.getValue().offset()));
    admin.deleteRecords(recordsToDelete).all().get();

```

Полная очистка диска будет происходить асинхронно.

### Выбор лидера

Типы выбора лидера:

* **Выборы предпочтительного лидера**. В каждом разделе есть реплика, которая назначается предпочтительным лидером.
  Метод `electLeader()` инициирует процесс предпочтительного лидера.
* **Выборы нечистого лидера**. Если ведущая реплика становиться недоступной, а другие реплики не имеют права становиться
  ведущими, раздел остается без ведущей реплики и, следовательно, оказывается недоступным. С помощью метода
  `electLeader()` может запустить процесс выбора лидера из реплик, которые в ином случае не имеют права стать им.

Метод асинхронный.

```java

    Set<TopicPartition> electableTopics = new HashSet<>();
    electableTopics.add(new TopicPartition(TOPIC_NAME, 0));
    try {
      admin.electLeaders(ElectionType.PREFERRED, electableTopics).all().get();
    } catch (ExecutionException e) {
      if (e.getCause() instanceof ElectionNotNeededException) {
        System.out.println("All leaders are preferred already");
      }
    }

```

### Перераспределение реплик

Метод `alterPartitionReassignments()` дает тонкий контроль над размещением каждой отдельной реплики для раздела.

Топик с несколькими разделами с одной репликой каждый. После добавления нового брокера, используем для хранения
некоторых
реплик текущих разделов.

```java

    Map<TopicPartition, Optional<NewPartitionReassignment>> reassignment = new HashMap<>();
    reassignment.put(
        new TopicPartition(TOPIC_NAME, 0),
        Optional.of(new NewPartitionReassignment(List.of(0, 1))));
    reassignment.put(
        new TopicPartition(TOPIC_NAME, 1), Optional.of(new NewPartitionReassignment(List.of(1))));
    reassignment.put(
        new TopicPartition(TOPIC_NAME, 2),
        Optional.of(new NewPartitionReassignment(List.of(1, 0))));
    reassignment.put(new TopicPartition(TOPIC_NAME, 3), Optional.empty());
    admin.alterPartitionReassignments(reassignment).all().get();
    DescribeTopicsResult demoTopic = admin.describeTopics(TOPIC_LIST);
    TopicDescription topicDescription = demoTopic.topicNameValues().get(TOPIC_NAME).get();

```

## Тестирование

Класс `MockAdminClient` можно инициализировать с любым количеством брокеров и использовать для тестирования, не запуская
реальный кластер Kafka. `MockAdminClient` не является частью Kafka API и может изменен без предупреждения.

Тестирование: обработка с помощью клиента администратора и использование его для создания топиков:

```java

  @BeforeEach
  public void setUp() {
    Node broker = new Node(0, "localhost", 9092);

    this.admin = spy(new MockAdminClient(List.of(broker), broker));
    // without this, the tests will throw
    // `java.lang.UnsupportedOperationException: Not implemented yet`
    AlterConfigsResult emptyResult = mock(AlterConfigsResult.class);
    doReturn(KafkaFuture.completedFuture(null)).when(emptyResult).all();
    doReturn(emptyResult).when(admin).incrementalAlterConfigs(any());
  }

  @Test
  public void createTestTopic() throws ExecutionException, InterruptedException {
    TopicCreator tc = new TopicCreator(admin);
    tc.maybeCreateTopic("test.is.a.test.topic");
    verify(admin, times(1)).createTopics(any());
  }

  @Test
  public void notTopic() throws ExecutionException, InterruptedException {
    TopicCreator tc = new TopicCreator(admin);
    tc.maybeCreateTopic("not.a.test");
    verify(admin, never()).createTopics(any());
  }

```

# Глава 6. Внутреннее устройство Kafka

## Членство в кластере

_Книга была напечатана до Apache Kafka 4.0, где Apache ZooKeeper не используется._

Для поддержания списка в настоящий момент в кластере брокеров Kafka использует ZooKeeper.
Каждый брокер имеет ID, указанный в файле конфигурации или генерируется автоматически.
При запуске, брокер регистрируется с ID в ZooKeeper посредством создания **временного узла**(ephemeral node).
Брокеры, контроллер и некоторые инструменты экосистемы подписываются на путь регистрации, чтобы получать уведомления.

При потере связи брокера с ZooKeeper созданный узел будет удален. Но ID брокера сохраняется в других структурах данных,
поэтому можно запустить новый брокер с тем же ID вместо старого и он получит те же разделы и топики.

## Контроллер

Контроллер - это брокер, который, помимо выполнения обычных своих функций, отвечает за выбор ведущих реплик для
партиций. Первый запущенный брокер в кластере становиться контроллером.

В случае останова брокера-контроллера или разрыва соединения - узел исчезнет. Происходит оповещение других брокеров.
Первый узел, создавший контроллер, становиться узлом-контроллером, остальные получают исключение. Новый контроллер
получает **номер эпохи контроллера**(controller epoch). Данное число будут получать остальные брокеры. Если старый
контроллер оживет(зомби) и начнет посылать сообщения. Номер эпохи в сообщения с более маленьким значением будет
игнорироваться.

Когда контроллер впервые появился, он должен прочитать последнюю карту о состоянии реплики, прежде чем сможет управлять
метаданными кластера и выполнять выборы лидера.

Если контроллер получает информацию, что брокер покинул кластер вместе с ведущей репликой в нем, назначает новую:

1. Но проходит по всем партициям, требующие новую ведущую реплику
2. Выбирает следующую из списка реплик
3. Отправляет запрос всем брокерам, содержащим или новые ведущие реплики, или существующие ведомые реплики для данных
   разделов
4. Сохраняет новое состояние в ZooKeeper
5. Отправляет запрос `LeaderAndISR` всем брокера, которые содержат реплики для этих разделов с информацией о новой
   ведущей и ведомыми репликами для этих партиций

Каждый брокер кэширует информацию о ведущих репликах и в этот момент контроллер отправляет запрос на обновление кэша.

При получении запроса на присоединении брокера к кластеру, контроллер проверяет наличие реплик на новом брокере, если
они есть то начинается процесс репликации данных.

## KRaft: новый контроллер Kafka на основе Raft

Raft - алгоритм для решения задач консенсуса в сети ненадёжных вычислений.

Apache Kafka Raft (KRaft) - это протокол согласования, который был представлен в KIP-500 для устранения зависимости
Apache Kafka от ZooKeeper для управления метаданными. Кворум контроллеров на базе Raft.

Минусы ZooKeeper при взаимодействии с Kafka:

* Обновления метаданных в ZooKeeper записываются синхронно, но отправление и получение обновления брокером асинхронно.
  Это приводит когда метаданные не согласуются между брокерами, контроллером и ZooKeeper.
* При перезапуске контроллер читает все метеоданные для всех брокеров и партиций из ZooKeeper, а затем отправляет их
  брокерам. По мере увеличения разделов и брокеров перезапуск контроллера становиться все медленнее.
* Внутренняя архитектура владения метаданными не очень хороша: некоторые операции выполнялись через контроллер, другие -
  через любой брокер, а третьи - непосредственно через ZooKeeper.
* ZooKeeper - это собственная распределенная система, она требует определенных знаний и опыта работы.

Основная идея нового дизайна контроллера: сама Kafka имеет архитектуру, основанную на журналах, где пользователи
представляют состояние в виде потока событий.

В новой архитектуре узлы контроллера представляют собой кворум Raft, который управляет журналом событий метаданных.
Журнал содержит информацию о каждом изменении метаданных кластера.

Лидер журнала метаданных называется активным контроллером. Он обрабатывает все механизмы вызова удаленных процедур RPC,
поступающие от брокеров. Контроллеры-последователи реплицируют данные, которые записываются в активный контроллер, и
служат в качестве резервных копий, если активный контроллер выйдет из строя. При отказе контроллера, не требуется
длительного перезапуска и переноса данных на новый контроллер.

Брокеры будут получать обновления от активного контроллера с помощью нового API `MetadataFetch`. Брокеры будут
отслеживать смещения последнего изменения метаданных, которое они получили, и запрашивать у контроллера только новые
обновления. Брокеры будут сохранять метаданные на диск, что позволить им быстро запускаться даже при миллионе разделов.

Брокеры регистрируются в кворуме контроллера и остаются зарегистрированным, пока их не снимет с регистрации
администратор, поэтому при отключении брокера все ещё останется зарегистрированными. Брокеры, которые находятся в режиме
онлайн, но не имеют актуальных метаданных, будут изолированны и не смогут обслуживать запросы клиентов. Это предотвратит
случаи, когда клиент отправляет события брокеру, который больше не является лидером, но слишком устарел, чтобы знать,
что он не является лидером.

## Репликация

Репликация - основа основ архитектуры Kafka. С помощью её обеспечивается доступность и сохраняемость данных при сбоях
отдельных узлов.

У партиции может быть несколько реплик, которые хранятся на брокерах, причем каждый из них обычно хранить множество
реплик относящиеся к другим партициям и топикам.

Типы реплик:

* **Ведущие**. Одна реплика из партиции. Через неё выполняются все запросы на генерацию для согласованности. Клиенты
  могут потреблять не только от ведущей.
* **Ведомые**. Все остальные реплики. По-умолчанию не обслуживают клиентские запросы. Основная задача - реплицировать
  сообщения на случай аварийных сбоев.

Чтобы читать из ведомой реплики:

* У потребителя должно быть настроено `client.rack` - идентифицирующий местоположение клиента
* У брокера `replica.selector.class` или `RackAwareReplicaSelector`, либо пользовательскую логику выбора реплики

Надежность чтения из ведомой реплики, такая же как и у чтения из ведущей. Чтобы это гарантировать, лидер включает в
данные **текущую максимальную отметку последнего зафиксированного смещения**(high-water mark). Это заносит небольшую
задержку в репликацию, ради возможности надежного чтения из ведомой реплики.

Ещё обязанность ведущей реплики - знать, какие ведомые реплики актуальны по сравнению с ней, а какие - нет. Чтобы не
отставать, реплики посылают ведущей запросы `Fetch`, такие же, что потребители отправляют для получения сообщения. В
каждом ответе с сообщениями содержится смещение сообщения, что бы поддерживать нужный порядок. Если реплика не
запрашивает сообщения более 10с или запрашивала, но отстает более чем на 10с она - **рассогласованная** (out
of sync). Она не может стать ведомой в случае сбоев. Остальные считаются **согласованными** (in-sync). Настраивается
`replica.lag.time.max.ms`.

В каждом разделе есть **предпочтительная ведущая реплика** (preferred leader) - была ведущей в момент создания топика.
При первоначальном создании партиций ведущие реплики распределяются между брокерами. Параметр
`auto.leader.rebalance.enable=true` при котором она будет проверять, является ли предпочтительная реплика ведущей или
согласована ли она, если положительно - делает реплику ведущей.

## Обработка запросов

Основная доля работы брокера заключается в обработке запросов: ведущим репликам разделов от клиентов, реплик разделов и
контроллера.

Клиенты инициируют подключения и отправляют запросы, брокер обрабатывает запросы и отвечает на них. От одного клиента
брокер обрабатывают в порядке поступления.

Каждый запросы имеет стандартный заголовок:

* тип запроса
* версию запроса
* идентификатор корреляции - необходим для отладки ошибок
* идентификатор клиента

Для каждого порта, выполняющие прослушивание, запускается **принимающий поток** (acceptor thread), создающий соединение
и передающий контроль над ним **обрабатывающему потоку** (processor thread).
Число потоков - **сетевые потоки** (network threads), можно задать в конфигурации.
Сетевые потоки отвечают за получение запросов клиентов, помещение их в **очередь запросов** (request queue), сбор
ответов из **очереди ответов** (response queue) и отправку клиентам.
Ответы могут приходить с задержкой. Отложенные ответы хранятся в **чистилище** (purgatory) до тех пор, пока они не будут
завершены.

После помещения запросов в очередь ответственность за их обработку передается потоками вывода/вывода (IO threads).
Распространение типы клиентских запросов:

* _запросы от производителей_ - отправляются производителями и содержат сообщениями
* _запросы на извлечения_ - отправляются потребителями и ведомыми репликами при чтении ими сообщений
* _запросы администратора_ - отправляются клиентами-администраторами

Запросы от производителей и на извлечения должны отправляться ведущей реплике раздела. Если брокер получает запрос от
производителя, относящийся к конкретному разделу, ведущая реплика которого находится на другом брокере, то отправляющий
запрос клиент получит сообщение об ошибке "Не является ведущей репликой раздела"

Клиент отправляют _запросы метаданных_ (metadata request), включающий список топиков, интересующие клиента. Они содержат
информацию о топиках, репликах партиций, ведущей реплике.

Клиент обычно кэшируют эту информацию и используют её для направления запросов предводителей на извлечение нужному
брокеру для каждого раздела. Периодически (metadata.max.age.ms) они обновляют эту информацию или при получении
негативного ответа о ведущей реплики.

### Запросы от производителей

Проверки запроса от производителя брокеру, на котором находится ведущая реплика:

* Если ли у отправляющего данные пользователя права на запись в этот топик?
* Допустимо ли указанное в запросе значение параметра `acks`?
* Если параметр `acks=all`, достаточно ли согласованных реплик для безопасной записи сообщения?

Затем брокер записывает сообщения на локальный диск. На Linux сообщения записываются в кэш файловой системы, и нет
никаких гарантий, что они будут записаны на диск. Kafka не ждет сохранения на диск - сохраняемость обеспечивается
репликацией.

После записи на ведущую реплику брокер проверяет значение параметра `acks`. Если `0` или `1`, брокер отвечает сразу же,
если `all`, запрос храниться в буфере-**чистилище**(purgatory) до тех пор, пока ведущая не проверит всю репликацию.

### Запросы на извлечение

Клиент посылает запрос, в котором просит брокер отправить сообщение в соответствии со списком топиков, партиций и
смещений. Также указывается ограничения на объем возвращаемых из каждого радела данных, чтобы клиентам хватило памяти на
обработку.

Запрос должен отправлен ведущим репликам указанных в запросе, для этого клиенты предварительно запрашивают метаданные,
чтобы гарантировать правильную маршрутизацию запросов на извлечение. Ведущая реплика проверяет запрос, если смещение
очень старое, то брокер вернет ошибку.

Если смещение существует, брокер читает сообщения вплоть до указанного. В Kafka используется метод `zero-copy` для
отправки сообщений клиентам - она отправляет сообщения напрямую из файлов без каких-либо промежуточных буферов.

Ограничение размера снизу - отправляет только пакет сообщения только с заданным размером или больше. Снижает
загруженность сети и процессора. При этом можно задать промежуток времени, больше которого брокеры не ждут и отправляют
сообщения.

Не все данные из ведущей реплики в партиции доступны клиентам. Большинство клиентов могут читать только сообщение,
которые записаны во все согласованные реплики. При попытках прочитать такие сообщения вернется пустой ответ.

Сообщения не реплицированные на достаточное количество реплик - небезопасны. По этому ответ на запрос может занимать
много времени, если репликация работает с задержкой. Она ограничивается параметром `replica.lag.time.max.ms` - после
исчисление этого времени, реплика будет считаться рассогласованной.

Если потребитель получает сообщения из большого количества разделов. В данном случае отправка списка всех интересующих
его разделов брокеров занимает много времени и трафика, при этом набор разделов редко меняется и метаданные редко
меняются и нужно возвращать не так много данных. Для этого в Kafka есть кэш сессии выборки. Потребитель может создать
кэшированную сессию из которой можно получить кэшированные данные. Кэш имеет ограниченный размер и Kafka отдает
приоритет ведомым репликам и потребителям с большим набором разделов.

### Другие запросы

Протокол Kafka обрабатывает 61+ типов запросов. Потребители используют 15 типов.

Этот протокол применяется для взаимодействия между самими брокерами.

Для проверки смещения используются следующие типы запросов: `OffsetCommitRequest`, `OffsetFetchRequest` и
`ListOffsetsRequest`.

Из-за разницы в версиях запросов рекомендуется сначала обновлять все брокеры, после этого обновлять клиенты.

`ApiVersionRequest` - позволяющий клиентам запрашивать у брокера поддерживаемые версии запросов и применять
соответствующую версию. Если клиенты, правильно реализуют эту возможность, у них будет возможность взаимодействовать со
старыми брокерами.

## Физическое хранилище

Основная единица хранения Kafka - реплика раздела. Разделы брокера нельзя разносить по брокерам или даже по различным
дискам одного брокера, поэтому размер раздела ограничивается доступным на отдельной точке монтирования местом.

При настройке задают список каталогов для хранения разделов с помощью параметра `log.dirs`. Обычная конфигурация
предусматривает по одному каталогу для каждой точки монтирования Kafka.

### Многоуровневое хранилище

Kafka используется для хранения больших объемов данных либо из-за высокой пропускной способности, либо из-за длительных
периодов хранения. Из-за это есть проблемы:

* Объем данных, в одном разделе, ограничен. Следовательно, срок хранения и количество разделов определяются не только
  требованию продукта, но и ограничениями размеров физического диска.
* Выбор размера диска и кластера определяется требованиями к объему памяти.
* Время, необходимое для перемещения разделов от одного брокера к другому. Большие разделы делают кластер менее
  эластичным.

При многоуровневом подходе конфигурируется с двумя уровнями хранения - локальным и удаленным. Локальный уровень -
использует локальные диски на брокерах для хранения сегментов журнала. Удаленный уровень используется специальные
системы хранения (HDFS, S3) для хранения завершенных сегментов журнала.

Поскольку стоимость хранения на локальном дороже, чем на удаленном, период хранения для локального уровня дороже.
Поэтому период хранения для локального уровня составляет несколько часов, а для удаленного - дни или даже месяцы.

Локальное хранилище имеет значительно меньшее время задержки, чем удаленное хранилище. Приложения чувствительные к
задержкам, обслуживаются с локального уровня. Приложения _backfill_, восстанавливающиеся после сбоя обслуживаются с
удаленного уровня.

Двухуровневая архитектура позволяет масштабировать хранилище независимо от памяти и процессоров в кластере. Позволяет
использовать Kafka как долгосрочное хранилище данных. Уменьшает объем данных на локальном уровне, которые необходимо
копировать во время восстановления или балансировки. Поскольку не все данные хранятся на брокерах, увеличение периода
хранение не требует добавления новых узлов. При этом общий срок хранения данных может быть больше.

Дизайн многоуровневого хранилища, имеет компонент — `RemoteLogManager` и взаимодействие с существующими
функциональными возможностями, такими как догоняющие лидера реплики и выборы лидера, подробно
документирован в https://oreil.ly/yZP6w.

Измерение производительности при использовании многоуровневого хранилища и чтения старых данных показала хороший
результат. Поскольку чтение происходит по сетевому пути. Сетевые операции чтения не конкурируют с локальными операциями
чтения за дисковый ввод-вывод или кэш страниц и оставляют кэш страниц нетронутым со свежими данными.

### Распределение разделов

Основные задачи распределения:

* Равномерно распределить реплики по брокерам
* Гарантировать - все реплики для каждого из разделов находятся на разных брокерах
* Если есть информация о размещении в стойках, то нужно по возможности разместить реплики для каждого из разделов на
  различных стойках

1. Начинаем перебор с произвольного брокера и циклически назначаем разделы каждому из брокеров для ведущих реплик
2. Для каждого из разделов будем размещать реплики в соответствии со все увеличивающимися смещениями по отношению к
   ведущей реплики

Если учитывается информация о стойках:

1. Подготовить список брокеров с учетом брокера,
2. Далее алгоритм повторяется

Выбрав нужные брокеры для всех разделов и реплик, нужно определиться с каталогом для новых разделов. Подсчитываем число
разделов в каждом каталоге и новые разделы добавляются в каталог с минимальным числом разделов.

Распределение разделов по брокерам не учитывает наличие места или имеющуюся нагрузку. Необходимо соблюдать осторожность
при распределении разделов.

### Управление файлами

Поиск сообщений для удаления, процесс затратный и грозящий ошибками - разделы разбиваются на сегменты. Текущий сегмент
называется активный. Он никогда не удаляется. Брокер держат открытыми дескрипторы файлов даже если они не активные.

### Формат файлов

Каждый сегмент храниться в отдельном файле в котором сообщения и их смещения. Формат файла на диске совпадает формату
сообщений, от производителя к брокеру и от брокера к потребителю. Одинаковый формат позволяет использовать `zero-copy`.

Kafka отправляют сообщения пакетами, если одно сообщение помещать в пакет - это дорого, если несколько - выгодно. Kafka
работает лучше при `linger.ms=10` - задержка увеличивает вероятность объединения сообщений. Пакет создается для каждого
раздела, использование меньшее количество разделов - эффективнее. Kafka включает несколько пакетов в одни запрос на
отправку, при использовании сжатия на производителе(рекомендуется!), будет лучшее сжатие как по сети, так и на дисках
брокера.

Накладные расходы на каждую записи малы, а большая часть системный информации находится на уровне пакета.

Управляющие пакеты - системные данные, например транзакционные фиксации.

`DumLogSegment` - утилита, для просмотра сегментов разделов в файловой системе. Запустить её:
`bin/kafka-run-class.sh kafka.tools.DumpLogSegments`

### Индексы

Для ускорения поиска сообщения с конкретным смещением, Kafka поддерживает индексы для всех разделов. Индекс задает
смещение файлу сегмента и месту в этом файле. Второй индекс - сопоставляет временные метки со смещениями сообщений,
улучшает поиск по времени.

### Сжатие

При необходимости хранить в Kafka только последние(актуальные) данные(сообщения) используются следующие сценарии:

* _delete_ - события, чей возраст превышает интервал хранения, удаляются
* _compact_ - сохраняется только последнее значение для каждого из ключей топика

### Как происходит сжатие

Журналы условно делятся на части:

* _чистую_ - сжатые ранее сообщения. Хранит только последнее сообщение для каждого ключа
* _грязную_ - сообщения, записанные после последнего сжатия

`log.cleaner.enabled` - активация сжатия

Сначала _грязные_ помещаются в ассоциативный массив(мапу). Затем перебираются из чистого и сравниваются из массива.
Новые сообщения не трогаются, а отсутствующие добавляются мапу.

### Удаленные события

Чтобы удалить все сообщения для конкретного ключа, приложение должно сгенерировать сообщение, содержащее этот ключ, и
пустое значение. Это сообщение называется _отметка об удалении_(tombstone). В течении некоторого времени потребители
будут получать информацию об удалении.

`AdminClient` имеет метод `deleteRecord`. Удаляет все записи до указанного смещения.

### Когда выполняется сжатие топиков

Разумный компромисс - _грязные_ записи займут 50% используемом топиком дискового пространства, после чего сжать их за
один раз.

Контролировать время сжатия:

* `min.compaction.lag.ms` - минимальная задержка после записи сообщения, прежде чем можно сжать
* `max.compaction.lag.ms` - максимальная задержка между моментом записи сообщения и моментом, когда становиться
  пригодным для сжатия

# Глава 7. Надежная доставка данных

## Гарантии надежности

Гарантии Kafka:

* Упорядоченность сообщений в разделе.
* Сообщения от производителя считаются зафиксированными, когда они записаны во все согласованные реплики раздела, но не
  обязательно уже сброшенные на диск.
* Зафиксированные сообщения не будут потеряны, если функционирует хотя бы одна реплика.
* Потребителями могут читать только зафиксированные сообщения.

## Репликация

Разделы - основные блоки данных. Они сохраняются на отдельные диски.

Реплика считается согласованной:

* отправляла в ZooKeeper контрольный сигнал в последние 6с
* извлекала сообщения из ведущей реплики в последние 10с
* извлекала наиболее свежие сообщение из ведущей реплики в последние 10с

## Настройка брокера

На надежное хранение сообщений влияют три параметра конфигурации. Можно управлять, как на уровне брокера, так на уровне
отдельных топиков.

### Коэффициент реплики

`replication.factor` - на уровне топика
`default.replication.factory` - на уровне брокера

Ключевые соображения по выбору числа реплик:

* Доступность. Чем больше реплик, тем больше доступность.
* Долговечность. При большем количестве копий, особенно на разных устройствах хранения, вероятность потери всех копий
  уменьшается.
* Пропускная способность. С каждой дополнительной репликой мы умножаем межброкерский трафик.
* Сквозная задержка. Если один брокер становится медленным по какой-либо причине, он будет замедлять работу каждого
  клиента, не зависимо от коэффициента репликации.
* Стоимость. Чем больше у нас копий данных, тем выше затраты на хранение и сеть. Обычно уменьшают. Но это снизит также
  доступность, а долговечность гарантирована устройством хранения

Рекомендуется устанавливать реплики на разных стойках, указав `broker.rack`.

### «Нечистый» выбор ведущей реплики

`unclean.leader.election.enable` - разрешает несогласованной реплики стать ведущей.

Разрешение рассогласованным репликам становиться ведущими увеличивает риск потери данных и того, что они станут
противоречивыми. Если же запретить это, уменьшится доступность из-за необходимости ждать, пока первоначальная ведущая
реплика станет доступной и можно будет восстановить работу раздела.

Администратор всегда может оценить ситуацию, принять решение о потере данных, чтобы сделать разделы доступными, и
переключить эту конфигурацию на значение `true` перед запуском кластера.

### Минимальное число согласованных реплик

`min.insync.replicas` - на уровне топика и брокера

Если число согласованных реплик будет меньше, то брокер не будет принимать сообщения, но читать из него можно.

### Поддержание синхронизации реплик

`zookeeper.session.timeout.ms` — интервал времени, в течение которого брокер может прекратить посылать контрольные
сигналы в ZooKeeper без того, чтобы ZooKeeper посчитал брокер мертвым и удалил его из кластера.

`replica.lag.time.max.ms` - максимальное время, после которого реплика становится рассинхронизированной, если реплика не
получала сообщения от лидера или не отслеживала последние сообщения от лидера.

### Долговременное хранение на диске

Наличие трех машин в отдельных стойках или зонах доступности, каждая из которых имеет копию данных, более безопасно, чем
запись сообщений на диск в лидере, поскольку одновременные сбои на двух разных стойках или в зонах маловероятны.

`flush.messages` — позволяет нам контролировать максимальное количество сообщений
`flush.ms` — частота синхронизации на диск

Рекомендуется почитать как fsync влияет на пропускную способность Kafka и как уменьшить его
недостатки https://oreil.ly/Ai1hl

## Использование производителей в надежной системе

### Отправка подтверждений

* `acks=0` - сообщение считается успешно записанным, если производитель сумел отправить его по сети. При этом работа
  имеет низкую задержку производства, но это не улучшает сквозную задержку.
* `acks=1` - ведущая реплика в момент получения сообщения и записи его в файл данных раздела(не обязательно на диск)
  отправила подтверждение или сообщение об ошибке. Задержка тоже низкая, но также не улучшает сквозную задержку.
* `acks=all` - ведущая реплика, прежде чем отправить подтверждение или сообщение об ошибке, дождется получения сообщения
  всеми согласованными репликами. Сочетается с параметром `min.insync.replicas`. Самый безопасный вариант, но и самый
  медленный.

### Настройка повтором отправки производителя

Ошибки от брокера при отправке сообщения могут быть: которые можно разрешить повторной отправкой, либо разрешить нельзя.

Если цель не терять ни одного сообщения, то лучше разрешить повторные отправки, оставить количество по умолчанию и
использовать параметр `delivery.timout.ms`.

Повторная отправка имеет риск, что оба сообщения будут сохранены. Повторная отправка гарантирует `at least once`, но не
`exactly once`.

### Дополнительная обработка ошибок

Разные ошибки нужно обрабатывать по-разному и не всегда сводить только к повторной отправке сообщения.

## Использование потребителей в надежной системе

Самое важное для надежности работы потребителя это фиксация смещения.

### Свойства конфигурации потребителей, важные для надежной обработки

* `group.id` - чтобы каждый потребитель в одной группе мог читать информацию из одного топика, но из разных партиций,
  нужно иметь свой идентификатор.
* `auto.offset.reset` - как потребитель будет читать сообщения, если никаких смещений не было зафиксировано или запросил
  смещения, которых нет в брокере
* `enable.auto.commit` - автоматическая или ручная фиксация сообщения.
* `auto.commit.interval.ms` - интервал времени автоматической фиксации.

### Фиксация смещений в потребителях явным образом

Основные моменты для надежной ручной фиксации:

* Всегда фиксируйте смещения после обработки сообщения
* Частота фиксации - компромисс между производительностью и числом дубликатов, возникающих при сбое
* Фиксируйте правильные смещения в нужное время
* Переназначение
* Потребителям может понадобиться повторить попытку. Способы
    * Столкнувшись с ошибкой, которую можно разрешить путем повтора, зафиксируйте последнюю успешно обработанную запись.
      Затем сохраните ожидающие обработки записи в буфере (чтобы следующая итерация опроса их не затерла),
      воспользуйтесь методом `pause()` потребителя для упрощения повторов, чтобы гарантировать, что дополнительные
      опросы не вернут данные, и продолжайте попытки обработки записей.
    * Столкнувшись с ошибкой, которую можно разрешить путем повтора, запишите ее в отдельный топик и продолжайте
      выполнение. Для обработки записей из этого топика для повторов можно воспользоваться отдельной группой
      потребителей. Или один и тот же потребитель может подписаться как на основной топик, так и на топик для повторов с
      приостановкой между повторами потребления данных из топика для повторов. Эта схема работы напоминает очереди
      зависших сообщений (dead-letter queue), используемые во многих системах обмена сообщениями.
* Потребителям может потребоваться сохранение состояния

## Проверка надежности системы

Три уровня проверки надежности системы.

### Проверка конфигурации

Тестирование настроек брокера и клиентов независимо от логики приложения.

Пакет `org.apache.kafka.tools` включает классы `VerifiableProducer` и `VerifiableConsumer`, для такой проверки. Это
моковые производитель и потребитель.

Примеры тестов https://oreil.ly/IjJx8

### Проверка приложения

Одни из пользовательских проверок:

* Код обработки ошибок
* Фиксация смещения
* Переназначение слушателей

Рекомендуется использовать интеграционные тесты для приложения и запускать их при сбойных обстоятельствах:

* при потере клиентами соединения с одним из брокеров;
* большой задержке между клиентом и брокером;
* заполнении диска;
* зависании диска, также называемом затмением (brown out);
* выборе ведущей реплики;
* плавающем перезапуске брокеров;
* плавающем перезапуске потребителей;
* плавающем перезапуске производителей.

`Trogdor` фреймворк для внедрения неисправностей https://oreil.ly/P3ai1.

### Мониторинг надежности при промышленной эксплуатации

Java-клиенты Kafka включают показатели JMX.

Показатели производителей: число ошибок и число поворотов в секунду.

Показатели потребителей: задержка потребления.

# Глава 8. Семантика "точно один раз"

Представляет собой сочетание двух особенностей: идемпотентный производитель и транзакция.

## Идемпотентный производитель

### Как работает идемпотентный производитель

Каждое сообщение содержит уникальный идентификатор производителя PID и порядковый номер. Они точно позволяют брокеру
идентифицировать сообщения, который с помощью них отслеживает последние 5 сообщений на наличие дублей. Если дубль
найден, он отклоняет сообщение с ошибкой. На стороне производителя ошибка регистрируется, но не вызывает никаких
исключений.

#### Перезапуск производителя

При перезапуске производителя, если включена функция идемпотентный производителя, он инициализируется и обращается к
брокеру для создания идентификатора производителя.

#### Отказ брокера

Каждая реплика хранить в своем буфере информацию об последовательности. Если лидер перезапускается, то он берет
информацию из бэкапа состояния.

### Ограничение идемпотентного производителя

Дублирование предотвращается только при повторных попытках, вызванных внутренней логикой производителя, а не ручной
попыткой `send()`. Самый просто и надежный способ, использовать внутренний механизм повторных попыток. Также, при
использовании нескольких производителей в одном приложении, идемпотентный производитель не защитить от дублей, который
отправляют разные производители.

### Как использовать идемпотентный производитель

Добавить параметр в конфигурацию производителя `enable.idempotence=true`.

## Транзакции

Транзакция - атомарная запись в один или несколько топиков и партиций Kafka.

### Сценарии использования транзакций

Транзакции полезны для обработки потоков, где важна точность и обработка потоков включает агрегацию и/или объединение.

### Как транзакции гарантируют "точно один раз"

Обработка "точно один раз" означает, что потребление, обработка и выдача данных выполняются атомарно.

**Атомарная многораздельная запись** - фиксация смещений и получение результатов включают запись сообщений в разделы.

**Транзакционный производитель** имеет параметр `transactional.id`, который сохраняется между перезапусками.

Для защиты от **зомбированных экземпляров приложения** производители имеют номер эпохи. Если регистрация производителя с
одинаковым `transactional.id`,но меньшей номером эпохи будет игнорироваться.

Также для производителя должен быть установлен параметр `isolation.level`:

* `read_committed` - `consumer.poll()` вернет сообщения либо успешно зафиксированной транзакции, либо записанные
  нетранзакционно.
* `read_uncommitted` - вернет все сообщение, даже неудачно транзакции.

### Какие проблемы не решаются транзакциями

Транзакции будут работать только в цепочках задач обработки потоков **чтение-преобразование-запись(read-copy-update)**,
в других либо не будут работать, либо будут с большими усилиями.

### Как использовать транзакции

Рекомендуемый способ включать в Kafka Streams гарантию "точно один раз", тогда применение транзакции будет под капотом.
Для включения `processing.guarantee` = `exactly_once` или `exactly_once_beta`.

Без Kafka Streams:

```java

    Properties producerProps = new Properties();
    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "DemoProducer");
    producerProps.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);
    var producer = new KafkaProducer<Integer, String>(producerProps);
    Properties consumerProps = new Properties();
    consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
    consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
    consumerProps.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
    var consumer = new KafkaConsumer<Integer, String>(consumerProps);
    producer.initTransactions();
    consumer.subscribe(List.of(inputTopic));
    while (true) {
      try {
        ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));
        if (records.count() > 0) {
          producer.beginTransaction();
          for (ConsumerRecord<Integer, String> record : records) {
            ProducerRecord<Integer, String> customizedRecord = transform(record);
            producer.send(customizedRecord);
          }
          Map<TopicPartition, OffsetAndMetadata> offsets = consumerOffsets();
          producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
          producer.commitTransaction();
        }
      } catch (ProducerFencedException | InvalidProducerEpochException e) {
        throw new KafkaException(
            String.format("The transactional.id %s is used by another process", transactionalId));
      } catch (KafkaException e) {
        producer.abortTransaction();
        resetToLastCommittedPositions(consumer);
      }
    }

```

### Идентификаторы транзакций и ограждения

Основное требование идентификатора транзакции: должен быть последовательным для одного и того же экземпляра приложения
между перезапусками и различался для разных экземпляров.

### Как работает транзакции

Kafka отслеживает процесс транзакции с помощью специального топика `transaction_state` - журналом транзакции. С которым
взаимодействует координатор транзакций.

1. `initTransaction()` - регистрирует транзакционного производителя у координатора транзакций(для каждого подмножества
   производителей он свой).
2. `beginTransaction()` - когда производитель обнаруживает, что отправляет записи в новую партицию, он дополнительно
   отправляет брокеру запрос `AddPartitionsToTxnRequest`, информируя о транзакции и о новой партиции.
3. Необходимо выполнить фиксацию смещения до фиксации транзакции - `sendOffsetsToTransaction()`.
4. Необходимо или зафиксировать `commitTransaction()` или прервать транзакцию `abortTransaction()`, отправить запрос
   `EndTransactionRequest`.
5. Если транзакция не будет завершена, то координатор транзакции по `transaction.timeout.ms` прервет её.

### Производительность транзакций

Для производителя накладные расходы не зависят от количества сообщения в них.

Для потребителя существуют накладные расходы связанные с чтением маркеров фиксации.

# Глава 9. Создание конвейеров данных

## Соображения по поводу создания конвейеров данных

### Своевременность

Kafka может работать конвейеров в режиме реального времени, так и передавать по пакетам раз в час.

### Надежность

* Исключить длительные сбои конвейера
* Возможность _как минимум однократной доставки_ сообщения. В сочетании с конечной БД и возможность _строго
  однократной_.

### Высокая/переменная нагрузка

Поскольку производитель и потребитель непосредственно не связаны, они не обязаны подстраиваться под нагрузку друг другу,
это делает Kafka.

### Форматы данных

Одна из задач конвейеров - согласование форматов. Для Kafka передаваемые форматы не важны. При это есть возможность
подключить преобразователи форматов. А также работать со схемами данных(Avro).

### Преобразования

Парадигмы создания конвейеров данных:

* ETL(Extract-Transform-Load) - конвейер данных отвечает за изменение проходящих через него данных. Плюсы: экономия
  времени и места, для потребителя. Минус: преобразования в конвейере могут лишить возможности обрабатывать данные в
  дальнейшим. При этом позволяет создать конвейер "один ко многим" - один раз преобразовать и отдать многим системам.
* ELT(Extract-Load-Transform) - целевая система собирает данные и обрабатывает. Плюсы: гибкость. Минусы: расход CPU и
  памяти.

### Безопасность

Возможности Kafka в плане безопасности:

* Возможность шифрования данных при передаче
* Поддержи аутентификацию(SASL) и авторизацию
* Журнал аудита

Не рекомендуется хранить креденшелы в конфигурационных файлах, лучше использовать внешнюю систему управления учетными
секретами.

### Обработка сбоев

Kafka может настроена на долгое хранение данных. В случае чего, можно вернуться назад и исправить ошибки.

### Связывание и гибкость

Одна из задач конвейеров: расцепление источников и приемников данных.
Причины связывания:

* Узкоспециализированные конвейеры
* Потери метаданных
* Чрезмерная обработка

## Когда использовать Kafka Connect, а когда - клиенты-производители и клиенты-потребители

Используйте клиенты, когда есть возможность модифицировать код приложения.

Kafka Connect - обычно используется для подключения к хранилищу данных, код которого невозможно изменить. Для этого
используется коннектор, который сейчас есть у многих хранилищ данных.

## Kafka Connect

Kafka Connect - фреймворк, позволяющий осуществлять копирование данных между Kafka или хранилищами данных. Предоставляет
API, среду выполнения и запуска плагинов-коннекторов.

Необходимо установить плагины-коннекторы на исполнителях, настраиваемые с помощью конфигураций. Затем взаимодействовать
с ними по REST API. Коннекторы запускают доп. задачи.

### Запуск Kafka Connect

Kafka Connect поставляется вместе с Apache Kafka. Для промышленной эксплуатации, особенно для больших данных, желательно
установить на отдельном сервере. То есть установить на все серверы Apache Kafka, но на одних запустить брокеры, на
других Kafka Connect.

Запуск исполнителя Kafka Connect, выбрав сценарий запуска передать ему файл с параметрами:

`bin/connect-distributed.sh config/connect-distributed.properties`

Основные настройки:

* `bootstrap.servers` - список брокеров, с которыми будет работать Connect. Рекомендуется указать хотя бы 3 брокера из
  кластера.
* `group.id` - идентификатор группы исполнителей.
* `plugin.path` - плагины в которых реализованы: коннекторы, конверторы, преобразования и провайдеры и др.
* `key.converter` и `value.converter` - конвектор формата для ключа и значения сообщения, сохраняемого в Kafka.
* `rest.host.name` и `rest.port` - хост и порт API для настройки и контроля.

Проверка работы кластера:

```bash
curl http://localhost:8083/{"version":"3.0.0-SNAPSHOT","commit":"fae0784ce32a448a","kafka_cluster_id":"pfkYIGZQSXm8RylvACQHdg"}%
```

### Преобразования одиночных сообщений

В экосистеме Kafka есть два преобразования:

* Преобразование одиночных сообщений, без состояния. Single Message Transform(SMT)
* Потоковая обработка, может сохранять состояние. Для более сложных случаев с объединением и агрегацией. Используется
  Kafka Streams.

### Взглянем на Kafka Connect поближе

Три основные понятия:

#### Коннекторы и задачи

Плагины коннекторов состоят:

* Коннекторы, отвечают за выполнение: определение числа задач для коннектора, разбиение работы по копированию данных
  между задачами, получение от исполнителей настроек для задач и передачу их далее.
* Задачи - отвечают за получение данных из Kafka и вставку туда дынных.

#### Исполнители

Процессы-исполнители - это “контейнерные” процессы, которые выполняют коннекторы и задачи. Они обрабатывают
HTTP-запросы с описанием соединители и их конфигурацию, хранение настроек коннекторов во внутреннем топике Kafka,
запуск коннекторов и их задач, включая передачу настроек. Также отвечают за автоматическую фиксацию смещений как для
коннекторов как источника, так и приемника и за выполнение повторов в случае ошибки.

#### Преобразование форматов и модель данных Connect

Connect API включает в себя API данных, который содержит как объекты данных, так и схему, описывающую эти данные. На
данный момент поддерживаются объекты Avro, JSON или строковые форматы.

#### Управление смещениями

Управление смещениями — коннекторам необходимо знать, какие данные они уже обработали, и они могут воспользоваться
предоставляемыми Kafka API для хранения информации о том, какие события уже обработаны.

## Альтернативы Kafka Connect

### Фреймоврки ввода и обработки данных для других хранилищ

У Hadoop и Elasticsearch есть свои собственные фреймворки, такие, как Flume или Logstash.

### ETL-утилиты на основе GUI

Informatica, Talend, Pentaho, Apache NiFi и StreamSets поддерживают Apache Kafka как в качестве источника данных, так и
в качестве получателя. Но бывают чересчур сложны.

### Фреймоврки потоковой обработки

Почти все фреймоврки потоковой обработки могут читать данные из Kafka и записывать их в некоторые другие системы.

# Глава 10. Зеркальное копирование между кластерами

_Зеркальное копирование_ - это копирование данных между кластерами Kafka, по аналогии копирование данных между узлами
кластера Kafka - _репликация_. MirrorMaker - встроенный в Kafka репликатор данных между кластерами.

## Сценарии зеркального копирования данных между кластерами

* Региональные и центральные кластеры. Когда часть приложения взаимодействует с центральным кластером, куда стекается
  нужная информация с региональных.
* Высокая доступность(HR) и аварийное восстановление (DR). Реплика кластера в случае сбоев.
* Соответствие нормативным требованиям. Когда кластеры находятся в разных странах с разным набором конфигураций, и
  нормативными и законодательными требованиями.
* Миграция в облако.
* Агрегирование данных из граничных кластеров.

## Мультикластерные архитектуры

### Реалии взаимодействия между различными ЦОД

Необходимо учитывать следующие факторы:

* Высокая длительность задержки. Зависит от расстояния и числу транзитных участков.
* Ограниченная пропускная способность сети. Пропускная способность глобальной сети хуже, чем в пределах одного ЦОД.
* Повышение по сравнению с работой в пределах одного ЦОД затраты. Связано с работой над улучшением пунктов выше.

Не рекомендуется устанавливать часть брокеров в кластере в другом ЦОД. Самая безопасная часть при репликации между
кластерами, это взаимодействие между брокером и потребителем. Но если в одном ЦОД есть приложения, которым необходимо
получать данные из Kafka другого ЦОД, то лучше развернуть рядом с приложением кластер Kafka и реплицировать данные с
кластера другого ЦОД.

Общие принципы для архитектур:

* Не менее одного кластера на ЦОД
* Каждое событие реплицируется ровно один раз между каждой парой ЦОД
* По возможности предпочитаем потребление данных из удаленного ЦОД отправке данных в него

### Архитектура с топологией типа "звезда"

Эта архитектура применяется, когда данные генерируются в нескольких ЦОД и части потребителей необходим доступ ко всем
данным. Приложения в каждом ЦОД обрабатывать только локальные по отношению к нему данные. Но нет доступа ко всем данным
из всех ЦОД.

Плюсы:

* Данные всегда генерируются для локального ЦОД, а события из каждого ЦОД реплицируются однократно - в центральной ЦОД.
* Приложения работающие с данными одно ЦОД разворачиваем там же. А с несколькими ЦОД в центральном.

Минус: приложения одного регионального ЦОД не может получать данные из другого ЦОД.

### Архитектура типа "активный-активный"

Два или более ЦОД совместно используют часть данных или их все, причем каждый из них может как генерировать, так
и потреблять события.

Плюсы:

* Возможность обслуживания пользователей ближайшим ЦОД, что обычно повышает производительность, без потерь
  функциональности из-за ограниченной доступности данных
* Избыточность и отказоустойчивость

Минусы:

* Конфликты в случае асинхронного чтения и обновления данных в нескольких местах
* Технические сложности зеркального копирования событий
* Сложности при поддержании согласованности данных между двумя ЦОД

Если удастся решить проблемы с асинхронными операциями чтения одних и тех же данных из нескольких мест и записи в них,
то данная архитектура — очень неплохой вариант. Это наиболее масштабируемая, отказоустойчивая, гибкая и
затратоэффективная архитектура из известных.

### Архитектура типа "активный-резервный"

Есть основной ЦОД и резервный в случае сбоев, если есть такие требования к системе.

Плюсы: простота настройки и возможности использовать в любом сценарии.

Минусы:

* Простаивает отличный кластер
* Переключение с одного кластера Kafka на другой на практике не простая вещь

Кластер, не выполняющий никакой полезной работы, а лишь простаивающий в ожидании аварийного сбоя, — пустая трата
ресурсов, учитывая тот факт, что аварийные сбои — событие редкое.

#### Планирование восстановления после аварийного сбоя

Два ключевых показателя восстановления _цель времени восстановления_(RTO) и _цель точки восстановления_(RPO). Чем ниже
RTO, тем ниже ручного процесса при восстановлении. Низкий RPO требует зеркального копирования в реальном времени.

#### Потери данных и несогласованность при внеплановом восстановлении после сбоя

Поскольку все программные решения для зеркального копирования Kafka асинхронны, то в DR-кластере не будет последних
сообщений из основного кластера. Следует всегда контролировать отставание DR-кластера от основного и не позволять ему
отставать слишком сильно.

#### Начальное смещение для приложений после автоматического сбоя

Одна из сложных задач при переключении кластеров - гарантировать, что приложения начнут потреблять с нужного места.

Решения:

* Автоматический сброс смещения: Если не производится зеркальное копирование смещений в рамках DR, необходимо выбрать
  один из следующих параметров: либо читать с начала имеющихся данных и обрабатывать большое количество дубликатов, либо
  перескочить в конец раздела, пропустив при этом некое число событий.
* Репликация топика для смещения: Потребители Kafka 0.9.0+ будут фиксировать смещения в специальный топик
  `__consumer_offsets`. Если зеркально копировать его в DR-кластер, то потребители смогут начать читать данные из
  DR-кластера с тех смещений, на которых они закончили чтение. Нюансы:
    1. Нет гарантий, что смещения в основном и дополнительном кластерах будут совпадать
    2. Даже если начать зеркально копировать сразу после создания топика, топики основного кластера, так и DR-топики
       будут начинаться со смещения 0, повторы попыток отправки производителем могут привести к расхождению смещений.
    3. Даже если смещения сохраняются в точности, из-за отставания DR-кластера от основного кластера и из-за того, что в
       настоящий момент решения по зеркальному копированию не поддерживают транзакции, информация о фиксации
       потребителем Kafka смещения может прибыть раньше или позже записи с данным смещением.
* Переключение времени: С версии 0.10.0+ каждое сообщение включает метку даты/времени, соответствующую времени отправки
  сообщения в Kafka. Таким образом можно сообщить потребителю с какого времени начать.
* Соответствия смещений: Можно воспользоваться внешней системой, например Cassandra, для хранения сопоставлений между
  смещениями в текущем и резервном кластере (это сложное решение и не рекомендуется).

#### После аварийного сбоя

После успешного переключения кластеров. Теперь из основного кластера можно сделать DR. Одно из лучших решений, очистить
исходный кластер, после чего начать зеркальное копирование из нового основного кластера.

#### Несколько слов об обнаружении кластера

Одно из решений, указывать клиентам имена DNS, которые указываются на основного брокера и в случае переключения, клиенты
переключаться на вспомогательный. Но потребителей все равно стоит перезапустить после сбоя, чтобы подхватить смещение.

### Эластичные кластеры

Эластичные кластеры (stretch clusters) предназначены для предотвращения отказа кластера Kafka в случае перебоев в работе
ЦОД. Это достигается за счет установки одного кластера Kafka в нескольких ЦОД.

## Утилита MirrorMarker

Утилита MirrorMaker версии 2.0 — это решение основанное на фреймворке Kafka Connect. Она использует коннектор источника
для получения данных из другого кластера. Все коннекторы распределяют работу между настраиваемым количеством задач. В
MirrorMaker каждая задача представляет собой пару из потребителя и производителя. Фреймворк Connect назначает эти
задачи различным рабочим узлам Connect по мере необходимости, то есть может быть несколько задач на одном
сервере или задачи будут распределены по нескольким серверам.

### Настройка MirrorMarket

Запуск MirrorMarket с файлом настроек:

`bin/connect-mirror-maker.sh etc/kafka/connect-mirror-maker.properties`

Некоторые параметры:

* _Поток репликации_. Пример настроек для архитектуры "активный-резервный"

```text
clusters = NYC, LON ##псевдонимы для кластеров
##Начальная загрузка для каждого кластера
NYC.bootstrap.servers = kafka.nyc.example.com:9092
LON.bootstrap.servers = kafka.lon.example.com:9092
NYC->LON.enabled = true ##включить поток репликации между парой между кластерами испльзуя source->target
NYC->LON.topics = .* ##топики, которые будут зеркально копироваться
```

* _Зеркальное копирование топиков_. При настроенной архитектуре "активный-активный", MirrorMarket проверяет в исходном
  кластере новые топики и создает в целевом, используя префикс, например `LON.orders`.
* _Миграция смещения потребителей_. У MirrorMaker `RemoteClusterUtils`, позволяющий потребителям обращаться к последнему
  смещению в DR-кластере. В 2.7.0+ есть топик смещений `__consumer_offsets`, который реплицируется в целевом кластере.
  Также можно настроить группу потребителей, для которых активировать или деактивировать смещения.
* _Миграция конфигурации топика и механизма управления доступом_. Есть определенный список конфигурации топиков, которые
  реплицируется в целевой кластер, можно настроить. Механизмы управления переносятся только буквальные, специфичные или
  зависящие от наименований - нет.
* _Задачи коннектора_. Параметр tasks.max ограничивается кол-во задач, которые может использовать коннектор.
  Рекомендуется устанавливать минимум 2.
* _Префиксы конфигурации_. Конфигурация MirrorMaker может включать конфигурацию для нескольких кластеров, префиксы можно
  использовать для указания конфигураций конкретного кластера или конкретного потока репликации

### Топология мультикластерной репликации

Настройка для топологии типа "активный-активный":

```text
clusters = NYC, LON
NYC.bootstrap.servers = kafka.nyc.example.com:9092
LON.bootstrap.servers = kafka.lon.example.com:9092
NYC->LON.enabled = true
NYC->LON.topics = .*
LON->NYC.enabled = true
LON->NYC.topics = .*
```

Рекомендуется использовать один файл конфигурации, для разных процессов MirrorMaker.

### Обеспечение безопасности MirrorMaker

Для настройки учетных данных для MirrorMaker подходит конфигурация:

```text
## Протокол безопасности должен соответствовать протоколу безопасности приемника брокера,
## соответствующего серверам начальной загрузки, указанным для кластера
NYC.security.protocol=SASL_SSL
NYC.sasl.mechanism=PLAIN
## Учетные данные для MirrorMaker задаются с помощью конфигурации JAAS,
## поскольку используется SASL.
## Для SSL следует указать хранилища ключей, если включена взаимная аутентификация клиентов.
NYC.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="MirrorMaker" password="MirrorMaker-password";
```

Администратору доступа, связанному с MirrorMaker, должны быть предоставлены соответствующие разрешения в исходном и
целевом кластерах, если на последних включена авторизация.

### Развертывание MirrorMaker для промышленной эксплуатации

Если запущены несколько MirrorMaker копирующий один и тот же кластер, автоматически распределять нагрузку между собой.
Если запускать в прод среде, рекомендуется с помощью команды `nohup` и перенаправлять логи в отдельный журнал.
MirrorMaker не сохраняет состояние и эй не нужно дискового хранилища, все данные и состояния хранятся только в Kafka.

Для пром контура, рекомендуется запускать MirrorMaker в распределенном режиме либо в выделенном кластере MirrorMaker,
либо в общем распределенном кластере Connect.

Если возможно, запускайте MirrorMaker в целевом ЦОД. Сеть внутри ЦОД надежней внешней. При разрыве, потребитель целевого
кластера просто не будет получать данные, но они будут в целости в источнике.

Если трафик между ЦОД требует шифрования, а внутри ЦОД не требует, лучше разместить MirrorMaker в исходном ЦОД,
чтобы он потреблял незашифрованные данные локально, после чего отправлять посредством генерации их в удаленный ЦОД
через зашифрованное SSL-соединение. SSL требует копирования данных для шифрования: потребители больше не пользуются
преимуществами производительности обычной оптимизации с нулевым копированием.

При пром эксплуатации также необходимо учесть:

* Мониторинг Kafka Connect
* Мониторинг метрик MirrorMaker
* Мониторинг отставания
    * Проверка последнего зафиксированного MirrorMaker смещения в исходном кластере Kafka.
    * Проверка последнего прочитанного MirrorMaker смещения, даже если оно не зафиксировано.
    * Использование Confluen Control Center
* Мониторинг показателей производителей и потребителей
    * показатели потребителя — `fetch-size-avg`, `fetch-size-max`, `fetch-rate`, `fetch-throttle-time-avg` и
      `fetch-throttle-time-max`
    * показатели производителя — `batch-size-avg`, `batch-size-max`, `requestsin-flight` и `record-retry-rate`;
    * показатели обоих — `io-ratio` и `io-wait-ratio`
* "Канарейка" - процесс, который ежеминутно отправляет событие в специальный топик в исходном кластере, после чего
  пытается прочитать это событие из целевого кластера

### Тонкая настройка MirrorMaker

Если отставание допустимо, можно выбрать такие параметры, при которых MirrorMaker 95–99 % времени использовался бы
на 75–80 %.

Тонкие настройки производителя:

* `linger.ms` и `batch.size` - кода отправляются пакеты можно увеличить пропускную способность путем создания
  небольшой искусственной задержки или/и настроить размер пакета
* `max.in.flight.requests.per.connection` - Если неважен порядок сообщений, то, используя значение по умолчанию 5, можно
  значительно увеличить пропускную способность

Тонкие настройки потребителя:

* `fetch.max.bytes` - Если есть резервы памяти, можно попробовать увеличить значение параметра, позволив потребителю
  читать больший объем данных при каждом запросе
* `fetch.min.bytes` и `fetch.max.wait.ms` - увеличить значения параметров, чтобы потребитель получал в каждом запросе
  больше данных, а брокер перед отправкой ему данных ждал, пока не появится достаточное их количество

## Другие программные решения для зеркального копирования между кластерами

Альтернативы MirrorMaker созданы, для решения проблем старых версий. Одни из них: Uber uReplicator, LinkedIn Brooklin,
Confluent Replicator.

# Глава 11. Обеспечение безопасности Kafka

Безопасность системы сильна настолько, насколько сильно её самое слабое звено.

## Блокировка Kafka

Процедуры безопасности используемой Kafka:

* Аутентификация
* Авторизация
* Шифрование
* Аудит отслеживает, что вы сделали или пытались сделать
* Квоты контролируют, сколько ресурсов вы можете использовать

Безопасное развертывание должно гарантировать:

* Подлинность клиента
* Подлинность сервера
* Конфиденциальность данных
* Целостность данных
* Контроль доступа
* Возможность аудита
* Доступность

## Протоколы безопасности

Kafka использует две технологии безопасности: TLS и SASL.
И поддерживает протоколы безопасности:

* `PLAINTEXT` - нет аутентификации и шифрования, использовать в частных сетях
* `SSL` - поддерживается аутентификация клиента и сервера, и шифрования, использовать в незащищенных сетях
* `SASL_PLAINTEXT` - поддерживает аутентификацию, но не шифрование, использовать в частных сетях
* `SASL_SSL` - поддерживается аутентификация клиента и сервера, и шифрования, использовать в незащищенных сетях

## Аутентификация

Экземпляр объекта `KafkaPrincipal` представляет личность клиента, после проверки подлинности и создается для каждого
соединения. Настраиваться с помощью параметр `principal.builder.class`.

### SSL

Используется протокол TLS в качестве безопасного транспортного уровня. TLS-рукопожатие выполняет аутентификацию,
согласовывает криптографические параметры и генерирует общие ключи для шифрования.

#### Настройки TLS

Брокеры должны настроены с хранилищем ключей, содержащий закрытый ключ и сертификат брокера, а клиенты - с хранилищем
доверия, содержащий сертификат брокера или сертификат центра сертификации.

Хранилища ключей и доверия должны периодически обновляться до истечения срока действия сертификатов, чтобы избежать сбоя
TLS-рукопожатий.

#### Соображения безопасности

Kafka не поддерживает старые версии протока TLS. Так как хранилища ключей хранятся в файловой системе, важно ограничить
доступ с помощью разрешений файловой системы. От атак типа "отказ в обслуживании" можно использовать параметр
`connection.failed.authentication.delay.ms` для задержки ответа при сбоях и снизить частоту повторных попыток.

### SASL

В качестве шифрования используется TLS.

Возможны следующие механизмы аутентификации:

* GSSAPI. Аутентификация Kerberos
* PLAIN. Аутентификация по имени пользователя/паролю, которая обычно применяется с настраиваемым обратным вызовом на
  стороне сервера для проверки паролей из внешнего хранилища паролей.
* SCRAM-SHA-256 и SCRAM-SHA-512. Аутентификация по имени пользователя/паролю, доступна в Kafka по умолчанию и не требует
  дополнительных хранилищ паролей.
* OAUTHBEARER. Аутентификация с помощью токенов на предъявителя OAuth, которая обычно используется с настраиваемыми
  обратными вызовами для получения и проверки токенов, предоставляемых стандартными серверами OAuth.

### Повторная аутентификация

Kafka задействует фоновый поток входа в систему для получения новых учетных данных до истечения срока действия старых,
но новые учетные данные по умолчанию применяются только для аутентификации новых соединений.

Брокеры Kafka поддерживают повторную аутентификацию для соединений, аутентифицированных с помощью механизма SASL,
применяя параметр конфигурации `connections.max.reauth.ms`.

Возможно сценарии повторной аутентификации для усиления безопасности:

* Для механизмов SASL: GSSAPI и OAUTHBEARER, которые используют учетные данные с ограниченным сроком
  действия, повторная аутентификация гарантирует, что все активные соединения связаны с действительными учетными
  данными.
* Механизмы SASL на основе паролей, такие как PLAIN и SCRAM, могут поддерживать смену паролей путем добавления
  периодического входа в систему.
* Параметр `connections.max.reauth.ms` инициирует повторную аутентификацию во всех механизмах SASL, в том числе с
  учетными данными с неистекшим сроком действия.
* Соединения от клиентов, не поддерживающих повторную аутентификацию SASL, прерываются по истечении срока действия
  сессии, заставляя клиентов заново подключаться и проходить аутентификацию, что обеспечивает те же гарантии
  безопасности для просроченных или отозванных учетных данных.

### Обновления системы безопасности без простоя

Многие задачи обслуживания выполняются с помощью текущих обновлений (rolling update), когда брокеры один за другим
отключаются и перезапускаются с обновленной конфигурацией. Некоторые задачи, такие как обновление хранилищ ключей
SSL и хранилищ доверия, могут быть выполнены с помощью динамических обновлений конфигурации без перезапуска брокеров.

При добавлении нового протокола безопасности в существующее развертывание можно добавить новый приемник в брокеры с
новым протоколом, сохранив при этом старый приемник со старым протоколом, чтобы клиентские приложения могли продолжать
работать со старым приемником во время обновления.

## Шифрование

Приемники Kafka, использующие SSL и SASL_SSL, применяют протокол TLS в качестве транспортного уровня, обеспечивая
безопасные зашифрованные каналы, которые защищают данные, передаваемые по незащищенной сети. Дополнительные меры должны
быть приняты для защиты данных в состоянии покоя, чтобы гарантировать, что конфиденциальные данные не могут быть
извлечены даже пользователями(администраторами), имеющими физический доступ к диску, на котором хранятся журналы Kafka.

### Сквозное шифрование

Сериализаторы и десериализаторы могут быть интегрированы с библиотекой шифрования для выполнения шифрования сообщения в
ходе сериализации и дешифрования во время десериализации. Шифрование обычно выполняется с помощью алгоритмов
симметричного шифрования (напр. AES). Общий ключ шифрования, хранящийся в системе управления ключами (KMS), позволяет
производителям шифровать сообщение, а потребителям — расшифровывать его.

## Авторизация

_!!! В Kafka 4 `AclAuthorizer` заменен на `org.apache.kafka.metadata.authorizer.StandardAuthorizer`, требуется отдельно
изучить. https://kafka.apache.org/documentation/#security_authz_

Экземпляр класса `AclAuthorizer` представляет уровень доступа клиента к ресурсам. Настраивается
`authorizer.class.name=kafka.security.authorizer.AclAuthorizer`.

### AclAuthorizer

Авторизатор `AclAuthorizer` поддерживает управление доступом к ресурсам Kafka с помощью списков управления
доступом (ACL). Списки управления доступом хранятся в ZooKeeper(KRaft) и кэшируются в памяти каждого брокера
для обеспечения высокопроизводительного поиска при авторизации запросов.

Привязка списков управления доступом состоит из следующих элементов:

* тип ресурса: Cluster|Topic|Group|TransactionalId|DelegationToken (Кластер|Топик|Группа|Идентификатор транзакции|Токен
  делегирования);
* тип шаблона: Literal|Prefixed (Буквальный|Префиксный);
* имя ресурса: имя ресурса, или префикс, или подстановочный знак *;
* операция: Describe|Create|Delete|Alter|Read|Write|DescribeConfigs|AlterConfigs (
  Описать|Создать|Удалить|Изменить|Читать|Записать|Описать настройки|Изменить настройки);
* тип разрешения: Allow|Deny (Разрешить|Запретить). Deny (Запретить) имеет более высокий приоритет;
* принципал: принципал Kafka, представленный как <principalType>:<principalName>, например, ser:Bob или Group:Sales.
  Списки управления доступом могут использовать User:* для предоставления доступа всем пользователям;
* хост: исходный IP-адрес клиентского соединения или *, если разрешены все хосты.

Например: `User:Alice has Allow permission for Write to Prefixed Topic:customer from 192.168.0.1`

## Аудит

Уровень ведения журнала, а также аппендеры (appender) и параметры их конфигурации можно указать в файле
`log4j.properties`. Экземпляры журналов `kafka.authorizer.logger`, применяемый для регистрации авторизации, и
`kafka.request.logger`, используемый для регистрации запросов, могут быть заданы независимо друг от друга, чтобы
настроить уровень журнала и срок хранения журналов для ведения журнала аудита.

## Обеспечение безопасности платформы

При проектировании системы безопасности для производственной системы следует использовать модель угроз, которая
рассматривает угрозы безопасности не только для отдельных компонентов(брокеров), но и для системы в целом.

# Глава 12. Администрирование Kafka

Kafka предоставляет пользователям несколько утилит командной строки(CLI), удобных для администрирования кластеров,
которые реализованы в виде классов Java.

## Операция с топиками

Утилита `kafka-topics.sh` позволяет выполнить большинство операций с топиками, но рекомендуется использовать более новый
инструмент `kafka-config.sh`.

### Создания нового топика

`kafka-topics.sh --bootstrap-server <bootstrap server> --create --topic <string> --replication-factor <integer> --partitions <integer>`

### Вывод списка всех топиков в кластере

`kafka-topics.sh --bootstrap-server <bootstrap server> --list`

### Подробное описание топиков

`kafka-topics.sh --bootstrap-server <bootstrap server> --describe`

### Добавление разделов

`kafka-topics.sh --bootstrap-server <bootstrap server> --alter --topic <string> --partitions <integer>`

### Уменьшение количества разделов

Уменьшить количество разделов топика невозможно. Если необходимо уменьшить количество, лучше создать новую версию топика
и переместить весь трафик производства в новый.

### Удаление топика

Чтобы это было возможно, параметр конфигурации брокеров кластера `delete.topic.enable` должен быть равен true.

`kafka-topics.sh --bootstrap-server <bootstrap server> --delete --topic <string>`

Удаление топика — асинхронная операция. Настоятельно рекомендуется, чтобы операторы не удаляли более одного или двух
топиков одновременно и давали им достаточно времени для завершения перед удалением других топиков из-за ограничений в
способе выполнения этих операций контроллером.

`kafka-topics.sh --bootstrap-server <bootstrap server> --delete --topic <string>`

## Группы потребителей

Для работы с группами потребителей есть инструмент `kafka-consumer-groups.sh`.

### Вывод списка и описание группы

`kafka-consumer-groups.sh --bootstrap-server <bootstrap server> --list`

### Удаление группы

Нельзя удалить не пустую группу.

`kafka-consumer-groups.sh --bootstrap-server <bootstrap server> --delete –group <string>`

### Управление смещениями

#### Экспорт смещений

Экспорт в файл CSV(<название топика>, <номер раздела>, <смещение>):

`kafka-consumer-groups.sh --bootstrap-server <bootstrap server> --export --group <string> --topic <string>--reset-offsets --to-current --dry-run > <string>`

#### Импорт смещений

`kafka-consumer-groups.sh --bootstrap-server <bootstrap server> --reset-offsets --group <string> --from-file <string> --execute`

## Динамические изменения конфигурации

Существует конфигурации, которые можно динамически обновлять во время работы без необходимости отключения или повторного
развертывания кластера, для этого используется `kafka-configs.sh`.

### Переопределение значений настроек топиков по умолчанию

`kafka-configs.sh --bootstrap-server <bootstrap server> --alter --entity-type topics --entity-name <topic name> --add-config <key1>=<value1>[,<key2>...]`

### Переопределение настроек клиентов и пользователей по умолчанию

`kafka-configs.sh --bootstrap-server <bootstrap server> --alter --entity-type clients --entity-name <client ID> --add-config <key1>=<value1>[,<key2>...]`

### Переопределение настроек конфигурации брокера по умолчанию

Подробнее в документации https://oreil.ly/R8hhb

Важные настройки:

* `min.insync.replicas` - минимальное количество реплик, которые должны подтвердить запись, чтобы запрос на производство
  был успешным, если производители установили параметру `acks` значение `all` (или `–1`);

* `unclean.leader.election.enable` - позволяет репликам быть избранными в качестве лидера, даже если это приводит к
  потере данных. Это полезно, когда допустимо иметь некоторые потери данных или включать на короткое время, чтобы
  отключить кластер Kafka, если невозможно избежать безвозвратной потери данных;

* `max.connections` - максимальное количество подключений к брокеру, разрешенных в любой момент времени. Мы также можем
  использовать параметры `max.connections.per.ip` и `max.conn`

### Описание переопределений настроек

`kafka-configs.sh --bootstrap-server <bootstrap server> --describe --entity-type topics --entity-name <string>`

### Удаление переопределений настроек

`kafka-configs.sh --bootstrap-server <bootstrap server> --alter --entity-type topics --entity-name <string> --delete-config retention.ms`

## Производство и потребление

### Консольный производитель

Отправка сообщения в топик через консоль:

```console
kafka-console-producer.sh --bootstrap-server <bootstrap server> --topic <string>
>Message 1
>Test Message 2
>Test Message 3
>Message 4
>^D
```

Для добавления параметров к консольной команде либо можно использовать конфигурационный файл
`--producer.config <файл_конфигурации>`, либо указать в консоли в виде `--producer-property <ключ>=<значение>`.

### Консольный потребитель

Получение сообщения из топика через консоль:

```console
kafka-console-consumer.sh --bootstrap-server <bootstrap server> --whitelist <string> --from-beginning
Message 1
Test Message 2
Test Message 3
Message 
```

Для добавления параметров к консольной команде либо можно использовать конфигурационный файл
`--consumer.config <файл_конфигурации>`, либо указать в консоли в виде `--consumer-property <ключ>=<значение>`.

По умолчанию используется форматтер `DefaultFormatter` - выводят в формате байтов, также доступны:

* `kafka.tools.LoggingMessageFormatter` - использует логгер на уровне INFO
* `kafka.tools.ChecksumMessageFormatter` - выводит контрольные суммы
* `kafka.tools.NoOpMessageFormatter` - потребляет сообщения, но не выводит их

## Управление разделами

### Выбор предпочтительной ведущей реплики

Запустить выбор предпочтительного лидера для всех топиков кластера:
`kafka-leader-election.sh --bootstrap-server <bootstrap server> --election-type <string> --all-topic-partitions`

Можно передать список разделов и топиков в формате JSON:

`kafka-leader-election.sh --bootstrap-server <bootstrap server> --election-type <string> --topics-to-move-json-file <string>`

### Изменение реплик раздела

Одни из причин вручную изменить назначения реплик для раздела:

* Существует неравномерная нагрузка на брокеров, которую автоматическое распределение лидеров обрабатывает неправильно.
* Один из брокеров отключился, и раздел недостаточно реплицирован.
* Был добавлен новый брокер, и мы хотим быстрее сбалансировать новые разделы в нем.
* Вы хотите настроить коэффициент репликации топика.

Этапы:

1. На основе списка топиков(JSON) генерируем предложение для набора перемещения
   `kafka-reassign-partitions.sh --bootstrap-server <bootstrap server> --topics-to-move-json-file <string> --broker-list <broker1,broker2...> --generate`.
   Команда возвращает JSON-файлы с текущим и целевым распределением.
2. Перемещение, используется целевой файл с предыдущего шага
   `kafka-reassign-partitions.sh --bootstrap-server <bootstrap server> --reassignment-json-file <string> --execute`
3. Проверка хода перемещения
   `kafka-reassign-partitions.sh --bootstrap-server <bootstrap server> --reassignment-json-file <string> --verify`

#### Изменение коэффициента репликации

Для изменения коэффициента репликации для патриции можно использовать предыдущую команду. В целевом JSON файле указать
ещё дополнительный брокер для каждой партиции.

#### Отмена переназначения реплик

Команда `kafka-reassign-partitions.sh` имеет параметр `--cancel` - отменяет активные переназначения в кластере.

### Сброс на диск сегментов журнала

`kafka-dump-log.sh --files <string>`

### Проверка реплик

Проверка согласованность реплик:

`kafka-replica-verification.sh --broker-list <comma separated list of brokers> --topic-white-list '<regex expression>'`

## Другие утилиты

* Списки управления доступом клиентов (`kafka-acls.sh`)
* Облегченный скрипт MirrorMaker (`kafka-mirror-maker.sh`)
* Инструменты тестирования (`kafka-broker-api-versions.sh`)

## Небезопасные операции

Операции которые технически выполнить возможно, но не рекомендуется или очень осторожно:

* Перенос контроллера кластера
* Отмена удаления топиков
* Удаление топиков вручную

# Глава 13. Мониторинг Kafka



