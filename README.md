# Краткое содержание "Apache Kafka. Потоковая обработка и анализ данных", 2-е издание, Гвен Шапира, Тодд Палино, Раджини Сиварам, Крит Петти

## Описание

Это мой конспект
книги ["Краткое содержание "Apache Kafka. Потоковая обработка и анализ данных", 2-е издание, Гвен Шапира, Тодд Палино, Раджини Сиварам, Крит Петти](https://www.piter.com/collection/bestsellery-oreilly/product/apache-kafka-potokovaya-obrabotka-i-analiz-dannyh-2-e-izdanie) (
сайт российского издательства).

Если Вы автор и считаете, что данный конспект нарушает авторские права - прошу сообщить, я сделаю этот репозиторий
приватным.

Нашли опечатку/неточность? Пишите - разберемся.

[Связаться со мной](https://t.me/szachesov)

# Знакомьтесь: Kafka

## Обмен сообщениями по типу "публикация/подписка"

Обмен сообщениям по типу "публикация/подписка"(publish/subscribe(pub/sub) messaging) - паттерн проектирования, в котором
отправитель(издатель) элемента данных(сообщений) не направляет его конкретному потребителю. Вместо этого он
классифицирует сообщения, а потребитель(подписчик) подписывается на определенные их классы.

## Открываем для себя систему Kafka

**Apache Kafka** - система обмена сообщения по принципу "публикация/подписка". Её также называют **распределенной
платформой
потоковой обработки**.

Данные в Kafka хранятся долго, упорядочено. Их можно читать когда угодно, они могут распределяться по системе либо ради
защиты от сбоев, либо ради производительности.

### Сообщения и пакеты

**Сообщения (message)** - единица данных в Kafka. Аналогия в БД - строки(row) или запись(record). Для Kafka это просто
массив байтов.

**Ключ (key)** - фрагмент метаданных, который может быть в сообщении. Может использоваться для лучшего управления
сообщениями в разделе. Простейшая схема - подобна распределению `value` при записи в `java.util.HasMap`.

**Пакет (batch)** - набор сообщений, относящиеся к одному топику или разделу. Используется для производительности. При
этом следует соблюдать баланс времени задержки-пропускной способности.

### Схемы

**Схема** - дополнительная структура на содержимое сообщения. Варианты задания схемы: JSON, XML, Apache Avro и др.

Для Kafka важен единообразный формат данных.

### Топики и разделы

**Топик (topic)** - категории, служащие для распределения сообщений. Аналогия БД - таблица.

**Разделы, партиции (partitions)** - единицы топиков. Сообщения записываются в него добавлением в конец, а читаются по
порядку от
начала.

Топик, состоящий из нескольких разделов не гарантирует упорядоченность сообщений в пределах всего топика - только в
разделе.

Разделы можно разместить на разные сервера и/или реплицировать.

**Поток данных (stream)** - данные, перемещающиеся от производителя к потребителю в рамках всего топика.

### Производители и потребители

Типы пользователей Kafka: производитель, потребитель. Для клиентских API(например, Kafka Streams) производители и
потребители выступают в качестве строительных блоков и предоставляют более высокий уровень взаимодействия.

**Производитель (producer)** - генерирует новые сообщения для конкретного топика. По умолчанию будет поставлять
сообщения по всем партициям, а может в конкретный

**Потребитель (consumers)** - читают сообщения. Подписывается на один топик или более и читает в порядке их возрастания.

**Смещение, офсет (offset)** - непрерывно возрастающее целочисленное значение, ещё один элемент метаданных. Обычно
значение сохраняется в хранилище Kafka, что бы потребитель приостанавливал и возобновлял чтение с нужного места.

**Группы потребителей (consumer groups)** - один или несколько потребителей, объединившихся для обработки топика. В
группе гарантируется, что несколько потребителей не могут читать одну партицию.

**Владение (ownership)** - соответствие потребителя партиции.

### Брокеры и кластеры

**Брокер (broker)** - отдельный сервер Kafka. Они получают сообщения от производителей, присваивают смещение и
записывают в дисковое хранилище. И обслуживает потребителей.

**Кластер (cluster)** - группа брокеров.

**Контроллер (cluster controller)** - один из брокеров в рамках кластера. Выбирает автоматически. Отвечает за
администрирование операций, распределение разделов по брокерам и мониторинг отказов последних.

**Ведущий (leader)** - брокер, которому принадлежит один раздел.

**Последователи (followers)** - помещается реплицированный раздел.

Производители соединяются с ведущим, потребители получают сообщения либо от ведущего, либо из последователя.

**Сохранение информации (retention)** - ключевая возможность Kafka, в течение длительного времени.

**Сжатые журналы (log compacted)** - механизм хранения данных в топиках, например хранения последнего сообщения с
конкретным ключом.

### Несколько кластеров

Преимущества больших систем с несколькими кластерами:

* Разделение типов данных
* Изоляция по требованиям безопасности
* Несколько центров обработки данных

Репликация осуществляется в рамках одного кластера, но не нескольких кластеров.

Для возможности репликации данных между кластерами, можно использовать утилиту **MirrorMarker**.

# Установка Kafka

## Настройка среды

### Установка ZooKeeper

ZooKeeper - это централизованный сервис для хранения информации о конфигурации, присвоения имен, обеспечения
распределения синхронизации и предоставления группового обслуживания.

#### Ансамбль ZooKeeper

ZooKeeper предназначен для работы в качестве кластера - **ансамбль(ensemble)**.

Рекомендуется делать нечетное число серверов, из-за особенности алгоритма балансировки, и он сможет отвечать на запросы,
когда функционируют большинство членов ансамбля(кворум).

Чтобы внести изменения в настройки ансамбля, необходимо перезагрузить узлы по одному разу за раз.

Не рекомендуется запускать одновременно более семи узлов - будет снижаться производительность, причина - природа
протокола консенсуса. Лучше рассмотреть возможность добавление дополнительных узлов-наблюдателей.

## Настройка брокера

### Основные параметры брокера

#### broker.id

Целочисленный идентификатор

* Обязательный
* Значение по-умолчанию 0
* Должно быть уникальным для брокера в пределах кластера
* Рекомендуется, чтобы число было как-то связано с хостом, чтобы более прозрачным было соответствие и удобно было при
  сопровождении брокеров. Например, имена хостов: `host1.example.com`, `host2.example.com`, а `broker.id` 1, 2
  соответственно.

#### Listeners

Настройка слушателей: список URL, которые мы прослушиваем, с именем слушателя. Если имя слушателя не является общим
протоколом безопасности, то необходимо настроить ещё одну конфигурацию **listener.security.protocol.map**. Формат:
`protocol://hostname:port`

#### zookeeper.connect

Путь, который использует ZooKeeper для хранения метаданных брокеров. Формат: `hostname:port/path`. `/path` -
опциональный, путь используемый в качестве нового корневого пути кластера. Считается хорошей практикой
использовать `/path`, чтобы использовать ZooKeeper с другими приложениями.

#### log.dirs

Список путей в локальной системе, куда будут сохраняться файлы, которые содержится в партициях. Если задано несколько,
брокер будет сохранять в них файлы равномерно, с учетом количества внутренних разделов, а не дискового пространства.

#### num.recovery.threads.per.data.dir

Число потоков пула, которые обрабатывают сегменты журналов(директорий с файлами). Пул применяется:

* при обычном запуске
* запуске после сбоя
* останове

При восстановлении после сбоя брокера с большим количеством разделов, выгода от применения может достигать нескольких
часов.

Определяется из расчета на один каталог журналов и числа `log.dirs`, то
есть `num.recovery.threads.per.data.dir * log.dirs = число потоков`.

#### auto.create.topics.enable

Автоматически топик создается в следующих случаях:

* производитель начинает писать в топик сообщение
* потребитель начинает читать из топика сообщение
* любой клиент запрашивает метаданные топика

Если управление топика явно, вручную или системой инициализации, то можно установить значение `false`.

#### auto.leader.rebalance.enable

Активирует балансировку ведущий реплик брокеров. Если не активно, ведущие реплики всех разделов сосредоточены на одном
брокере и кластер будет несбалансированным. Активирует фоновый поток, который проверяет распределение ведущих реплик.

#### delete.topic.enable

Запретить удаление топиков. Полезно для безопасности.

### Настройки топиков по умолчанию

#### num.partitions

С каким количеством партиций создается топик автоматически. Количество может увеличиваться, но не уменьшаться. То есть
требуется его создать вручную.

```text
КАК ВЫБРАТЬ КОЛИЧЕСТВО РАЗДЕЛОВ
Вот несколько факторов, которые следует учитывать при выборе количества разделов.
• Какой пропускной способности планируется достичь для топика? Например, планируете вы записывать 100 Кбайт/с или 1 Гбайт/с?
• Какая максимальная пропускная способность ожидается при потреблении сообщений из 
отдельного раздела? Раздел всегда будет полностью потребляться одним потребителем (даже если 
не используются группы потребителей, потребитель должен прочитать все сообщения в разделе). 
Если знать, что потребитель записывает данные в базу, не способную обрабатывать более 50 Мбайт/с 
по каждому записывающему в нее потоку, становится очевидным ограничение в 50 Мбайт/с при 
потреблении данных из раздела.
• Аналогичным образом можно оценить максимальную пропускную способность из расчета на 
производитель для одного раздела, но, поскольку быстродействие производителей обычно выше, 
чем быстродействие потребителей, этот шаг чаще всего можно пропустить.
• При отправке сообщений разделам по ключам добавление новых разделов может оказаться очень 
непростой задачей, так что желательно рассчитывать пропускную способность, исходя не из текущего 
объема использования, а из планируемого в будущем.
• Обдумайте число разделов, размещаемых на каждом из брокеров, а также доступные каждому 
брокеру объем дискового пространства и полосу пропускания сети.
• Старайтесь избегать завышенных оценок, ведь любой раздел расходует оперативную память и другие 
ресурсы на брокере и увеличивает время обновления метаданных и передачи руководства.
• Будете ли вы выполнять зеркальное копирование данных? Возможно, вам также потребуется учесть 
пропускную способность своей конфигурации зеркального копирования. Большие разделы могут 
стать серьезным недостатком во многих конфигурациях зеркального копирования.
• Если вы используете облачные сервисы, есть ли ограничение количества IOPS (операции ввода/вывода 
в секунду) на ваших виртуальных машинах или дисках? В зависимости от облачного сервиса и конфигурации 
виртуальных машин могут существовать жесткие ограничения на количество разрешенных IOPS, которые 
приведут к нарушению квот. Наличие слишком большого количества разделов может иметь побочный 
эффект увеличения количества IOPS из-за задействованного параллелизма.
```

Если есть оценка целевой пропускной способности топика и ожидаемой пропускной способности,
то `целевая пропускная способность / ожидаемую пропускную способность потребителей = число партиций`.

Если подробной информации нет, то по опыту ограничение разделов на диске до 6 Гбайт сохраняемой информации в день часто
дает удовлетворительные результаты.

#### default.replication.factor

При активном `auto.create.topics.enable`, задает коэффициент репликации для новых топиков.

Рекомендации, позволяющие избежать перебоев, от внутренних факторов(оборудования):

* Рекомендуется устанавливать минимум на 1 больше значения `min.insync.replicas`
* Для больших кластеров и большого оборудования на 2 больше значения `min.insync.replicas`. В идеале дать разрешить одно
  запланированное и одно не запланированное отключение, те у каждого минимум 3 точных копии каждой партиции.

#### log.retention.ms

Наряду с `log.retention.hours`(по-умолчанию 168) и `log.retention.minutes`, задается время хранения сообщения. Если
заданы несколько параметров, то этот в приоритете.

Параметр анализирует времени последнего изменения (mtime) на диске.

#### log.retention.bytes

Ограничение хранения на основе размера партиции. То
есть `количество партиций * log.retention.bytes = максимальный размер топика`.

Если использовать `log.retention.ms` и `log.retention.bytes`, то будут учитываться оба параметра. Для простоты, лучше
установить один, но для продвинутых можно оба.

#### log.segment.bytes

Настройки выше относятся к сегментам журналов, а не отдельным сообщениям. Разер сегмента журнала на диске, когда он
закрывается и открывается новый. Удаление сообщений по таймеру `log.retention.ms` и `log.retention.bytes`, возможно
только для закрытых сегментов.

#### log.roll.ms

Закрытие сегмента журнала по времени. Надо учитывать, если много партиций, то будет происходить закрытие многих журналов
на диске и возможно скажется на производительность диска.

#### min.insync.replicas

Устанавливает количество реплик, которые будут гарантировано подхвачены и синхронизированы с производителем. Она хорошо
сочетается с настройкой производителя на проверку всех запросов. Снижает производительность и если кластер с высокой
пропускной способностью, которые допускают иногда потерю сообщения, то этот параметр лучше не использовать.

#### message.max.bytes

Максимальный размер сообщений, если размер превышен при отправке сообщения - производителю возвращается ошибка. Данный
размер указан для уже сжатых сообщений.

Данный параметр нужно согласовывать с параметром `fetch.message.max.bytes` у клиента и `replica.fetch.max.bytes` на
брокерах при конфигурации кластера.

## Выбор аппаратного обеспечения

На производительность влияют следующие факторы:

* Емкость дисков
* Пропускная способность дисков
* Оперативная память
* Сеть
* CPU

### Пропускная способность дисков

SSD являются лучшим вариантом при наличии очень большого количества клиентских подключений.

### Емкость диска

При подборе емкости, нужно понимать, сколько планируется получать информацию и учитывать 10% для других файлов.

### Память

Сообщения сохраняются в страничном кэше системы, поэтому операция чтения достаточно быстрое.

Для Kafka не требуется выделение для JVM большого объема оперативной памяти в куче. Например, брокер обрабатывает 150к
сообщений с секунду при скорости передачи данных 200 Мбит/с, может работать с кучей 5 Гбайт. Остальная часть оперативной
памяти будет использована для страничного кэша. Поэтому не следует располагать Kafka в системе с другими важными
приложениями.

### Передача данных по сети

Максимальный объем трафика у Kafka, определяется пропускной способностью сети. Рекомендуется применять сетевые адаптеры
NICs емкостью не менее 10 Гбайт.

### CPU

Вычислительная мощность важна при очень большом масштабировании. Также мощность влияет на разархивирование и
архивирование сообщений.

## Настройка кластеров Kafka

Преимущества кластера:

* Возможность масштабировать
* Возможность использовать репликации для надежности

### Сколько должно быть брокеров

Размер кластера зависит от:

* емкости диска
* емкости реплик на одного брокера
* мощности процессора
* пропускной способности

В настоящее время рекомендуется иметь не более 14000 реплик партиций на брокер и 1 млн. реплик на кластер.

### Конфигурация брокеров

Требования к конфигурации брокеров:

* У всех брокеров должно быть одинаковое значение `zookeeper.connect`
* У каждого брокера в кластере должен быть уникальный `broker.id`

### Тонкая настройка операционной системы

На Linux эти параметры обычно настраиваются в файле `/etc/sysctl.conf`

#### Виртуальная память

Для высокой пропускной способности, следует избегать подкачки страниц памяти на диск, установив `vm.swappines=1`.

При использовании быстрых дисков(SSD) имеет смысл уменьшить количество "грязных" станиц, установив
`vm.dirty_background_ratio=5`.

`vm.dirty_ratio` - процент от всего объема памяти при достижении которой блокируется запись "грязных" станиц и
инициализируется сброс их на диск, разумно устанавливать 60-80, но быть осторожным с этим параметром.

Рекомендованное кол-во файловые дескрипторов для сегментов журнала и открытых
соединений `= (количество_разделов) Х (размер_раздела/размер_сегмента)`. На основе этого устанавливается
параметр `vm.max_map_count`, значение 400к-600к в зависимости от среды дает положительны результат. Также рекомендуется
установить `vm.overcommit_memory=0` иначе ОП будет захватывать слишком много памяти и не давая работать Kafka
оптимально.

#### Диск

В качестве локальной файловой системы чаще задействуется:

* `Ext4` - работает хорошо, но требует потенциально небезопасный параметров
* `XFS` - использует алгоритм отложенного выделения, но более безопасный, чем в `Ext4`

`atime` - время последнего обращения к файлу. Данный параметр неиспользуемая Kafka, поэтому его можно деактивировать,
чтобы лишний раз не нагружать процессор, указав `noatime`.

При больших объемах рекомендуется использовать `largeio`.

#### Передача данных по сети

Вначале нужно изменить объемы памяти, выделяемой для буферов отправки и получения для каждого сокета,
`net.core.wmem_default` и `net.core.rmem_default`, рекомендуется 2097152(2Мб).

Рекомендуется задать буферы отправки и получения для сокетов TCP с помощью `net.ipv4.tcp_wmem` и `net.ipv4.tcp_rmem`.

`net.ipv4.tcp_window_scaling=1` - оконное масштабирования TCP, чтобы клиенты эффективно передавали данные и
буферизировать их на стороне брокера.

`net.ipv4.tcp_max_syn_backlog` - больше чем по-умолчанию 1024, чтобы увеличить одновременное подключение.

`net.ipv4.netdev_max_backlog` - больше чем по-умолчанию 1000, чтобы помочь при всплесках сетевого трафика.

## Промышленная эксплуатация

### Параметры сборки мусора

Рекомендуется использовать сборщик мусора `G1`. Для корректировки производительности используются
параметры: `MaxGCPauseMillis`, `InitiatingHeapOccupancyPercent`. Для Kafka можно задавать более маленькие значения этих
параметров.

Например, для сервера 64 Гбайт оперативной памяти и Kafka работала с кучей 5 Гбайт. Можно
установить `MaxGCPauseMillis=20`, `InitiatingHeapOccupancyPercent=35`.

### Планировка ЦОД

Важно применять репликации и размещать физически брокеров на стойках в ЦОД в среде со зоной отказа.

Рекомендуется размещать реплики партиций на разных полках и в целом разные брокеры на разных полках.

### Размещение приложений на ZooKeeper

Со временем зависимость от ZooKeeper уменьшается и сойдет на нет.

Использования целого ансамбля ZooKeeper для одного кластера необоснованно, достаточно одного ZooKeeper.

Рекомендуется, чтобы потребители использовали для смещения Kafka, а не ZooKeeper.

Не рекомендуется использование одного ансамбля ZooKeeper, для других приложений отличный, не считая разные кластеры
брокеров Kafka.

# Производители Kafka: запись сообщений в Kafka

## Обзор производителя

Общий алгоритм производителя:

1. Для генерации сообщений для Kafka нужно создать объект `ProducerRecord`, включающий топик и значение. Опционально
   можно задать ключ, партицию, временную метку и/или набор заголовков.
2. После отправки `ProducerRecord` он сериализует объекты ключа и значения в байтовый массив для передачи по сети.
3. Если не указали партицию явно, данные попадают в `Partitioner`. Он выбирает партицию, обычно в соответствии с ключом.
   Если партиция указана, то выбор будет очевиден.
4. Далее запись помещается в пакет записей, предназначенных для отправки в соответствующие топики и разделы.
5. После получения брокером, он отправляет ответ. В случае успеха - возвращает объект `RecordMetadata`, содержащий инфу
   о топике, партиции и офсете.
6. Если брокеру не удалось записать запись, вернется сообщение об ошибке.
7. При получении об ошибке, производитель может попробовать отправить несколько раз сообщение.

## Создание производителя Kafka

Обязательные свойства:

* `bootstrap.servers` - список пар `host:port` брокеров для ПЕРВОНОЧАЛЬНОГО соединения с кластером. Не обязательно
  включать все брокеры, но рекомендуется указывать как минимум два, на случай сбоев.
* `key.serializer` - имя класса для сериализации ключей записей. Производителю нужно знать, как преобразовать объекты в
  байтовые массивы. Должен указан класс, реализующего от `org.apache.kafka.serialization.Serializer`. Необходимо задать,
  даже если отправляется только значение, можно применять `VoidSerializer`.
* `value.serializer` - имя класса для сериализации значения записей.

Пример создания производителя:

```java
class Producer {
    void method() {
        Properties kafkaProps = new Properties();
        kafkaProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092, broker2:9092");
        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProps);
    }
}
```

Методы отправки сообщения:

* _Сделать и забыть_ - отправляем на сервер сообщение и не волнуемся.
* _Синхронная отправка_ - Технически производитель всегда асинхронный и при вызове метода `send()` возвращается объект
  `Future`. Однако, можно воспользоваться методом `get()` и ожидать ответа.
* _Асинхронная отправка_ - вызывается метод `send()` и передаем функцию обратного вызова.

## Отправка сообщения в Kafka

Просто вариант:

```java
class Producer {
    void method() {
        ProducerRecord<String, String> record =
                new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        try {
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

```

### Синхронная отправка сообщения

```java
class Producer {
    void method() {
        ProducerRecord<String, String> record =
                new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        try {
            producer.send(record).get();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

```

В классе `KafkaProducer` два типа ошибок.

* Ошибки которые можно исправить, отправив сообщение повторно(retriable). Можно настроить `KafkaProducer`, чтобы при
  таких ошибках отправка была автоматическая.
* Ошибки невозможно исправить повторной отправкой.

### Асинхронная отправка сообщения

```java
class Producer {
    void method() {
        producer.send(record, new DemoProducerCallback());
    }
}

private static class DemoProducerCallback implements Callback {

    @Override
    public void onCompletion(RecordMetadata recordMetadata, Exception e) {
        if (e != null) {
            e.printStackTrace();
        }
    }
}

```

Обратные вызовы выполняются в главном потоке производителя, последовательно. Обратный вызов должен при этом выполняться
достаточно быстро, чтобы не мешать отправке. Не рекомендуются выполнять блокирующие операции внутри обратного вызова, а
следует использовать другой поток.

## Настройка производителей

### client.id

Идентификатор клиента, может быть только строкой.

### acks

Определяет, сколько реплик партиций должны получить запись, прежде чем производитель сможет считать запись успешной.

* `acks=0` - производитель не будет ждать ответа от брокера, чтобы счесть отправку сообщения успешной.
* `acks=1` - производитель получает от брокера ответ об успешном получении сразу же, как только ведущая реплика получит
  сообщение.
* `acks=all` - ответ от брокера об успешном получении сообщения приходит производителю после того, как оно дойдет до
  всех синхронизируемых реплик

При менее надежной конфигурации `acks` будет выше только производительность производителя. Но не сквозной задержкой
между моментами создания сообщения и момента, когда сообщение будет доступно потребителями. В данном случае время
одинаково.

### Время доставки сообщения

Время, за которое Kafka не ответит успешно или пока мы не будем готовы сдаться при повторных отправках.

Временные интервалы затраченные на отправку:

* Время до возвращения асинхронного вызова `send()`. В этот момент вызывающий поток заблокирован.
* Время с момента успешного возвращаемого `send()` до запуска обратного вызова(успешного или с ошибкой).

#### max.block.ms

Время ожидания `send()` и явного запроса метаданных `partitionsFor()`. Иначе ошибка.

#### delivery.timeout.ms

Время с момента, когда готова запись к отправке(функция `send()` возвращается успешно) до момента, пока брокер не
ответил или клиент не откажется. `delivery.timeout.ms >= linger.ms + retry.backo.ms + request.timeout.ms`.

#### request.timeout.ms

Время ожидания производителя ответа от сервера при отправке.

#### Повторные попытки и retry.backoff.ms

`retries` - количество повторных попыток

Ожидание между повторными попытками отправить. Не рекомендуется в текущий версии, лучше подсчитать сколько времени
потребуется для восстановления после сбоя брокера и установить параметр `delivery.timeout.ms` общее время, потраченное
на повторные попытки было больше, времени восстановления кластера.

### linger.ms

Время ожидания дополнительных сообщений перед отправкой текущего пакета. Рекомендуется выставлять значение больше 0.

### buffer.memory

Объем памяти, используемой для буферизации сообщений, ожидающих отправки.

### compression.type

Тип сжатия сообщения. По умолчанию сообщения не сжимаются.

* `snappy` - хорошая степень сжатия при высокой производительности
* `gzip` - лучшая степень сжатия

### batch.size

Объем памяти в байтах для каждого пакета, который собирается перед отправкой в одну партицию.

### max.in.flight.requests.per.connection

Количество пакетов, которые производитель может отправить серверу, не получая ответов.

При активных параметрах `retries > 0` и `max.in.flight.requests.per.connection > 1` может нарушаться последовательность
отправки сообщений. Для сохранения упорядоченности??? и отсутствия дубликатов, необходимо использовать
`enable.idempotence=true`.

### max.request.size

Максимальный размер запроса производителя. Ограничивает максимальный размер сообщения, так и число сообщений, отсылаемых
в одном запросе. Рекомендуется устанавливать равное значению параметру брокера `message.max.bytes`.

### receive.buffer.bytes и send.buffer.bytes

Размеры TCP-буферов отправки и получения, используемых сокетами при записи и чтении данных. `-1` - значения ОП.

### enable.idempotence

Когда активно, производитель будет прикреплять порядковый номер к каждой отправляемой записи, чтобы у брокера не было
дубликатов.

При активации, важно установить значения для следующих параметров:

* `max.in.flight.requests.per.connection <= 5`
* `retries > 0`
* `acks = all`

## Сериализаторы

### Пользовательские сериализаторы

Класс значения:

```java
class Customer {
    private int customerID;
    private String customerName;

    public Customer(int ID, String name) {
        this.customerID = ID;
        this.customerName = name;
    }

    public int getID() {
        return customerID;
    }

    public String getName() {
        return customerName;
    }
}

```

Пользовательский сериализатор:

```java
import java.nio.ByteBuffer;
import java.util.Map;

import org.apache.kafka.common.errors.SerializationException;

public class CustomerSerializer implements Serializer<Customer> {

    @Override
    public void configure(Map configs, boolean isKey) {
        // нечего настраивать
    }

    @Override
    /**
     * Мы сериализуем объект Customer как: 4-байтное целое число, соответствующее
     * customerId 4-байтное целое число, соответствующее длине customerName в байтах в кодировке UTF-8
     * (0, если имя не заполнено) N байт, соответствующих customerName в кодировке UTF-8
     */
    public byte[] serialize(String topic, Customer data) {
        try {
            byte[] serializedName;
            int stringSize;
            if (data == null) return null;
            else {
                if (data.getName() != null) {
                    serializedName = data.getName().getBytes("UTF-8");
                    stringSize = serializedName.length;
                } else {
                    serializedName = new byte[0];
                    stringSize = 0;
                }
            }
            ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + stringSize);
            buffer.putInt(data.getID());
            buffer.putInt(stringSize);
            buffer.put(serializedName);
            return buffer.array();
        } catch (Exception e) {
            throw new SerializationException("Error when serializing Customer to byte[] " + e);
        }
    }

    @Override
    public void close() {
        // нечего закрывать
    }
}

```

Данный подход не рекомендуется, так как код при этом не надежный при изменении класса сообщения.

### Сериализация с помощью Apache Avro

Apache Avro - формат сериализации данных.

Пример схемы Avro:

```json

{
  "namespace": "customerManagement.avro",
  "type": "record",
  "name": "Customer",
  "fields": [
    {
      "name": "id",
      "type": "int"
    },
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "email",
      "type": [
        "null",
        "string"
      ],
      "default": "null"
    }
  ]
}

```

Важные моменты использования данных схем:

* Схема для записи и схема для чтения должны быть совместимы
* У десериализатора должен быть доступ к схеме, задействованной при записи данных, даже если она отличается от схемы,
  которую ожидает обращающееся к данным приложение.

### Использование записей Apache Avro

Можно хранить схемы в записи, но это имеет накладные расходы. Лучше применить паттерн Реестр схем(Schema Registry).
Данную схему будут использовать как производитель, так и потребитель. При в отправляемой в Kafka записи, храним только
идентификатор схемы в реестре.

```java
class Producer {
    void method() {

        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
        props.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
        props.put("schema.registry.url", schemaUrl);

        String topic = "customerContacts";
        Producer<String, Customer> producer = new KafkaProducer<>(props);
        // Генерация новых событий продолжается вплоть до нажатия Ctrl+C
        while (true) {
            Customer customer = CustomerGenerator.getNext();
            System.out.println("Generated customer " + customer.toString());
            ProducerRecord<String, Customer> record =
                    new ProducerRecord<>(topic, customer.getName(), customer);
            producer.send(record);
        }
    }
}

```

## Партиции

Ключи сообщения служат двум целям:

1. Дополнительная информация, сохраняемая вместе с сообщением
2. Для определения, в какую партицию записывать. Все сообщения с одинаковым ключом попадут в одну партицию.

При создании объекта `ProducerRecord` с неопределенным ключом, используется метод секционирования по умолчанию.
`ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "France");`

Разделитель по умолчанию и ключ `null` - запись отправиться в один из доступных партиций случайным образом с
использованием циклического алгоритма.

Если ключ есть и разделитель по умолчанию - Kafka вычисляет хеш-значение ключа собственным алгоритмом и отправляет
сообщение в конкретный раздел. Для вычисления нужны все партиции раздела, поэтому если один не доступен - будет ошибка.

Существуют также разделители:

* `RoundRobinPartitioner` - случайное назначение разделов
* `UniformStickyPartitioner` - "липкое" случайное назначение разделов(для балансировки нагрузки)

Когда применяется разделитель по умолчанию, соответствие ключей партициям остается согласованным до тех пор, пока число
партиций не меняется. Если ключи важны для распределения, простейшим решением будет создавать топики с достаточным
числом партиций(https://oreil.ly/ortRk).

### Реализация пользовательской стратегии секционирования

Например, есть B2B-поставщик(producer) и покупатель(consumer). На одного покупателя, приходится большой процент всех
покупок. Поэтому рекомендуется выделить в отдельную партицию инфу о его покупках:

```java
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.record.InvalidRecordException;
import org.apache.kafka.common.utils.Utils;

public class BananaPartitioner implements Partitioner {
    public void configure(Map<String, ?> configs) {
    }

    public int partition(
            String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        if ((keyBytes == null) || (!(key instanceof String))) {
            throw new InvalidRecordException("We expect all messages " + "to have customer name as key");
        }
        if (key.equals("Banana")) return numPartitions - 1; // Banana всегда попадает в последний раздел
        // Другие записи распределяются по разделам путем хеширования
        return Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1);
    }

    public void close() {
    }
}

```

## Заголовки

Заголовки позволяют добавлять метаданные о записи, не добавляя никакой информации к самой записи.

Варианты использования для указания: источника данных, маршрутизации или отслеживания сообщения.

```java
class Producer {
    void method() {

        ProducerRecord<String, String> record =
                new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        record.headers().add("privacy-level", "YOLO".getBytes(StandardCharsets.UTF_8));
    }
}

```

## Перехватчики

```java
public class CountingProducerInterceptor implements ProducerInterceptor<Object, Object> {

    ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();
    static AtomicLong numSent = new AtomicLong(0);
    static AtomicLong numAcked = new AtomicLong(0);

    public void configure(Map<String, ?> map) {
        long windowSize = Long.parseLong((String) map.get("counting.interceptor.window.size.ms"));
        executorService.scheduleAtFixedRate(
                CountingProducerInterceptor::run, windowSize, windowSize, TimeUnit.MILLISECONDS);
    }

    public ProducerRecord<Object, Object> onSend(ProducerRecord<Object, Object> producerRecord) {
        numSent.incrementAndGet();
        return producerRecord;
    }

    public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {
        numAcked.incrementAndGet();
    }

    public void close() {
        executorService.shutdownNow();
    }

    public static void run() {
        System.out.println(numSent.getAndSet(0));
        System.out.println(numAcked.getAndSet(0));
    }
}

```

## Квоты и регулирование запросов

Механизм квотирования - ограничивать скорость создания и потребления сообщений.

Типы:

1. На производство
2. На потребление
3. На запрос

п.1 и п.2 - ограничивают скорость, с которой клиенты могут отправлять и получать данные, измеряются байт/сек. п.3 -
ограничивают процент времени, обработки клиентских запросов.

Можно применять для всех клиентов, для конкретных идентификаторам клиентов, конкретным пользователям.

Квоты можно установить в конфигурационном файле, но это настройка статично и не рекомендуется.

Для динамичной установки квот рекомендуется `kafka-config.sh` или `API AdminClient`.

Примеры:

1. Ограничение клиента C на выдачу только 1024 байт/с:

`bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024' --entity-name clientC --entity-type clients`

2. Ограничение пользователя 1 на выдачу только 1024 байт/с и потребление только 2048 байт/с:

`bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048' --entity-name user1 --entitytype users`

3. Ограничение на потребление только 2048 байт/с всех пользователей, за исключением имеющих более конкретные настройки:

`bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config 'consumer_byte_rate=2048' --entity-type users`

# Потребители Kafka: чтение данных из Kafka

## Принципы работы потребителей Kafka

### Потребители и группы потребителй

Если несколько потребителей подписаны на один топик и относятся к одной группе, они будут получать сообщения из разных
подмножеств партиций.

* Если есть 1 топик, 4 партиции и 1 группа потребителей и 1 потребитель. Потребитель будет читать сообщения из всех
  разделов.
* Если есть 1 топик, 4 партиции и 1 группа потребителей и 2 потребителя. Каждый потребитель будет читать из 2х партиций.
* Если есть 1 топик, 4 партиции и 1 группа потребителей и 4 потребителей. Каждый потребитель будет читать из 1ой
  партиции.
* Если есть 1 топик, 4 партиции и 1 группа потребителей и 5 потребителей. 4 потребителя будут читать из 1ой партиции, а
  1 потребитель будет простаивать.

Основной способ масштабирования получения данных - добавление потребителей в группу. Потребители выполняют операции с
большим значением задержки, чем производители.

Можно, но не рекомендуется масштабировать чтение, за счет создания новых групп и чтения в рамках одного приложения, т.е.
разных сообщений/партиций.

### Группы потребителей и переблалансировка разделов

Передача партиции от одного потребителя другому называется перебалансировка(rebalance).

Типы перебалансировки:

* _Безотлагательная перебалансировка_. Потребители прекращают потребление, отказываются от своих прав владения
  разделами, снова присоединится к группе потребителей и получают совершенное новое назначение разделов.
* _Совместная перебалансировка_. Инкрементальная перебалансировка, включает в себя перебаланисровку небольшого
  подмножество разделов от одного потребителя к другому. Более долгая, но не прерывает работу.

Потребители поддерживают членство в группе(активность) и принадлежность разделов за счет отправки координатору группы
брокеров периодических контрольных сигналов.

Если потребитель длительное время прекращает отправку контрольных сигналов, время его сеанса истекает, координатор
группы признает его неработающим и инициирует перебалансировку.

### Статические участники группы

Идентификация потребителя в группе - временная, если потребитель покинет и зайдет в группу идентификация поменяется.

Можно установить потребителя с уникальным идентификатором `group.instance.id`. Если такие потребители выключаться, он не
покидает группу, а останется пока его сессия не завершиться. После переподключения к группе ему назначаться теже
партиции.

Статическое членство полезно, когда приложение поддерживает локальное состояние или кэш, который заполняется разделами,
назначенными каждому потребителю. Однако партиции не будут переназначаться другим потребителям.

Статические участники не покидают группу проактивно при выключении, это зависит от `session.timeout.ms`. Данный параметр
должен быть достаточно высоким, чтобы не вызывать перебалансировку при простом перезапуске приложения, но достаточно
низким, обеспечивая автоматическое переназначение разделов потребителей при более длительном времени простоя.

## Создание потребителя Kafka

Потребитель представляет собой экземпляр класса `KafkaConsumer`. Для его создания требуются экземпляр `Properties`.
Обязательные свойства:

* `bootstrap.servers` - строка подключения к кластеру
* `key.deserializer` - десериализатор ключа
* `value.deserializer` - десериализатор значения

`group.id` - задает группу потребителей. Не является обязательным, но очень широко используется.

```java
public class ConsumerMain {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092,broker2:9092");
        props.put(CommonClientConfigs.GROUP_ID_CONFIG, "CountryCounter");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
    }
}

```

## Подписание на топик

Чтобы подписаться на топик, нужно передать список топиков методу `consumer.subscribe(List.of("customerCountries"))`.

Также можно передать название топиков через регулярное выражение `consumer.subscribe(Pattern.compile("test.*"))`.
Фильтрация топиков для подписки осуществляется на стороне клиента. То есть при использовании регулярного выражения,
наличии множества топиков и партиций, и множества клиентов. Будет большой трафик на предоставление метаданных, может
даже больше, чем для отправки данных.

## Цикл опроса

В самой основе API потребителя лежит простой цикл.

```java
class ConsumerMain {
    void method() {
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf(
                        "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());

                custCountryMap.merge(record.value(), 1, (oldValue, newValue) -> oldValue + 1);

                JSONObject json = new JSONObject(custCountryMap);
                System.out.println(json);
            }
        }
    }
}

```

Получение данных производится методом `poll()`. Передаваемый параметр - длительность ожидания ответа. При первом вызове
метода `poll()` для нового потребителя он отвечает за поиск координатора группы, присоединение потребителя к группе и
назначение ему разделов.

Если `poll()` не вызывается дольше, чем значение `max.poll.interval.ms`, потребитель станет считаться мертвым и будет
вычеркнут из группы потребителей, поэтому стоит избегать длительных дополнительных действий при обработке сообщения.

### Потокобезопасность

Железное правило: один потребитель на один поток. Полезно обернуть логику потребителя в его собственный объект, а затем
использовать`ExecutorService` для запуска нескольких потоков, каждый с собственным потребителем(https://oreil.ly/8YOVe).

Другой подход: один потребитель заполнял очередь событий, а несколько потоков выполняли работу из
нее(https://oreil.ly/uMzj1).

## Настройка потребителей

### fetch.min.bytes

Минимальный объем данных, получаемых от брокера при извлечении записей. По умолчанию 1 байт. Если размер сообщений
меньше, то брокер будет ждать ещё сообщения для отправки потребителю. Рекомендуется повышать значение, но с учетом
повышения задержки с низкой пропускной способностью.

### fetch.max.wait.ms

Максимальное время ожидания, пока не наберется размер сообщение больше `fetch.min.bytes`. То есть либо передача
сообщений пройдет по `fetch.min.bytes`, или `fetch.max.wait.ms`.

### fetch.max.bytes

Максимальное количество байтов, которые Kafka будет возвращать каждый раз, когда потребитель опрашивает брокер. Если
первый пакет записи будет превышать этого размера при отправке, то в данной отправке параметр игнорируется.

### max.poll.records

Максимальное количество записей, возвращаемое по вызову `poll()`.

### max.partition.fetch.bytes

Максимальное число байтов, возвращаемых сервером из расчета на один раздел. Конфигурация сложная, рекомендуется
использовать `max.poll.records`.

### session.timeout.ms и heartbeat.interval.ms

`session.timeout.ms` - время ожидания ответа контрольного сигнала от потребителя, после которого он будет считаться
отключенным. `heartbeat.interval.ms` - частота отправки потребителем контрольных сигналов. Данные параметры желательно
менять одновременно. При низком значении, возможно ранее выявить не работающий потребитель, но может привести к частой
перебалансировке.

### max.poll.interval.ms

Время, в течение которого потребитель может обходиться без опроса(`poll()`), прежде чем будет признан мертвым.

### default.api.timeout.ms

Тайм-аут, который будет применяться ко всем вызовам API, не указанный явно.

### request.timeout.ms

Максимальное время, в течение которого потребитель будет ожидать ответа от брокера. При превышении, закроет соединение и
попытается подключиться снова. Не рекомендуется уменьшать дефолтное значение.

### auto.offset.reset

Поведение потребителя при начале чтения партиции, для которого у него зафиксированное смещение отсутствует или стало
некорректным.

* `latest` - в отсутствие корректного смещения потребитель начинает читать самые свежие
* `earliest` - начинает читать все данные из раздела с начала
* `none` - возникает исключение при попытках пользования с недопустимым смещением

### enable.auto.commit

Будет ли потребитель фиксировать смещение автоматически.

### partition.assignment.strategy

Определяет, какие разделы к какому потребителю будут относиться

* **Range (Диапазонная)**. Каждому потребителю присваиваются последовательные подмножества разделов из топиков, на
  которые он подписан.
* **RoundRobin (Циклическая)**. Все разделы от всех партиций подписанных топиков распределяются по потребителям
  последовательно, один за другим.
* **Sticky (Липкая)**. Первая цель: получить максимально сбалансированное назначение. Вторая: оставить как можно больше
  назначений на месте.
* **Cooperative Sticky (Совместная липкая)**. Такая же, как `Липкая`, но поддерживает совместные перебалансировки, при
  которых потребители могут продолжать потреблять из разделов, которые не были переназначены.

### client.id

Используют брокеры для идентификации отправленных клиентом запросов. Применяются для логирования и для показателей, при
задании квот.

### client.rack

По-умолчанию потребители получают сообщения от ведущей реплики каждой партиции. Реплики могут находится физически в
разных местах. Данная настройка позволяет включить выборку из физически ближайшей реплики относительно потребителя.

### group.instance.id

Предоставление потребителю статического членства в группе.

### receive.buffer.bytes и send.buffer.bytes

Размер TCP-буферов отправки и получения, применяемых сокетами при записи и чтения. Рекомендуется повышать, когда клиенты
взаимодействуют с брокерами из другого ЦОД.

### offsets.retention.minutes

Настройка брокера, влияет на потребителей. Последнее смещение храниться в Kafka, до тех пор пока есть активные члены
группы. Если группа неактивна, то через время указанное в настройке смещение сбрасывается.

## Фиксация и смещение

Метод `poll()` возвращает ещё не прочитанные сообщения. Потребитель фиксирует последнее сообщение, которое успешно
обработано из партиции. Это называется фиксацией смещения(offset commit).

Потребитель отправляют сообщение в брокер, которое обновляет специальный топик `__consumer_offsets`, содержащий смещение
для каждого раздела. После перебалансировки, новый потребитель будет знать с какого смещения было последнее чтение.

### Автоматическая фиксация

При активном `enable.auto.commit` потребитель каждые 5с (настраивается `auto.commit.interval.ms`) проверяет
необходимость фиксации. Если есть такая необходимость, то при следующем вызове poll() будет фиксировать последнее
смещение, возвращенное предыдущим вызовом.

Автоматическая фиксация удобна, но она не позволяет управлять так, чтобы избежать дублирования в случае сбоев.

### Фиксация текущего смещения

При отключенном `enable.auto.commit` фиксация будет происходить при явном указании фиксации в коде.

```java
public class ConsumerMain {

    public static void main(String[] args) {
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>();

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf(
                        "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
            }

            // Фиксация смещения после обработки всего пакета сообщений
            try {
                consumer.commitSync();
            } catch (CommitFailedException e) {
                log.error("commit failed", e);
            }
        }
    }
}

```

Важно не вызывать метод `commitSync()` до окончания обработки последней записи в пакете сообщения.

### Асинхронная фиксация

Минусы синхронной фиксации - приложение заблокировано, пока не придет ответ фиксации. При асинхронной такого минуса нет.

Метод асинхронной фиксации: `commitAsync()`.

Минус данного подхода, что `commitAsync()` не повторяет попытку фиксации. Причина: на момент получения `commitAsync()`
ответа от сервера уже может быть успешно выполнена более поздняя фиксация.

Также можно передать функцию обратного вызова применяемую при ответе брокера. Их часто используют для каналирования
ошибок фиксации или их подсчета в виде показателей:

```java
public class ConsumerMain {

    public static void main(String[] args) {

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf(
                        "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
            }

            // Асинхронная фиксация с обратным вызовом
            consumer.commitAsync(
                    (offsets, e) -> {
                        if (e != null) log.error("Commit failed for offsets {}", offsets, e);
                    });
        }
    }
}
```

Простой способ обеспечить правильный порядок при асинхронных повторах:

1. Увеличивайте порядковый номер при каждой фиксации и вставьте его во время фиксации в обратный вызов `commitAsync()`.
2. Когда будете готовы отправить повторный запрос, проверьте, равен ли порядковый номер фиксации в обратном вызове
   переменной экземпляра.
3. Если да, то более поздняя фиксация не выполнялась и можно спокойно пробовать отправить запрос
   еще раз.
4. Если же значение переменной экземпляра больше, не нужно пробовать повторно отправлять запрос, потому что уже был
   сделан более поздний запрос на фиксацию.

Пример из https://stackoverflow.com/a/53244658/17965846

```java
class ConsumerCommitAsync {

    public static void main(String[] args) {
        Properties props = new Properties();
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        try {
            AtomicInteger atomicInteger = new AtomicInteger(0);
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(5);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %d, offset = %d, " + "customer = %s, country = %s\n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }

                consumer.commitAsync(
                        new OffsetCommitCallback() {
                            private final int marker = atomicInteger.incrementAndGet();

                            @Override
                            public void onComplete(
                                    Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
                                if (exception != null) {
                                    if (marker == atomicInteger.get()) consumer.commitAsync(this);
                                } else {
                                    // Can't try anymore
                                }
                            }
                        });
            }
        } catch (WakeupException e) {
            // ignore for shutdown
        } finally {
            consumer.commitSync(); // Block
            consumer.close();
            System.out.println("Closed consumer and we are done");
        }
    }
}

```

### Сочетание асинхронной и синхронной фиксации

Случайные сбои при фиксации - незначительная помеха, но если речь идет о последней фиксации перед закрытием потребителя
или перебалансировкой, то лучше позаботится, чтобы он точно оказалась успешной. Для этого сочетают асинхронную и
синхронную фиксацию:

```java
public class ConsumerCommitAsyncAndSync {
    public static volatile boolean closing = false;

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "broker1:9092,broker2:9092");
        props.put(CommonClientConfigs.GROUP_ID_CONFIG, "CountryCounter");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            Duration timeout = Duration.ofMillis(100);
            while (!closing) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %s, offset = %d, customer = %s, country = %s",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }
                consumer.commitAsync();
            }
            consumer.commitSync();
        } catch (Exception e) {
            log.error("Unexpected error", e);
        }
    }
}

```

### Фиксация заданного смещения

Если требуется выполнять фиксацию смещения раньше, чем после полного обработки пакетов. Возможно вызвать `commitAsync()`
или `commitSync()` с конкретным оффсетом:

```java
public class ConsumerCommitCurrent {

    public static void main(String[] args) {

        Properties props = new Properties();
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {

            Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
            int count = 0;

            Duration timeout = Duration.ofMillis(100);
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    currentOffsets.put(
                            new TopicPartition(record.topic(), record.partition()),
                            new OffsetAndMetadata(record.offset() + 1, "no metadata"));
                    if (count % 1000 == 0) consumer.commitAsync(currentOffsets, null);
                    count++;
                }
            }
        }
    }
}

```

## Прослушивание на предмет перебалансировки

Во время смены принадлежности патриций потребителю, возможно задать необходимое поведение с помощью объекта
`ConsumerRebalanceListener` передав его в метода `subscribe()`.

У `ConsumerRebalanceListener` методы для реализации:

1. `void onPartitionsRevoked(Collection<TopicPartition> partitions)` - вызывается после переназначения разделов
   потребителю, но до того как он начнет получать сообщения.
2. `void onPartitionsAssigned(Collection<TopicPartition> partitions)` - При безотлагательной перебалансировки вызывается
   до начала преебалансировки и после того, как потребитель стал получать сообщения. При совместной перебалансировки -
   вызывается в конце перебалансировке, только с тем подмножеством партиций, от которых потребитель должен отказаться.
3. `void onPartitionsLost(Collection<TopicPartition> partitions)` - дефолтно делегирует методу `onPartitionsAssigned()`.
   Вызывается только при использовании совместного алгоритма перебалансировки и только в исключительных случаях, когда
   разделы были назначены другим потребителям без предварительного отзыва алгоритма перебалансировки.

Если используется алгоритм совместной перебалансировки, есть следующие момент:

* `onPartitionsAssigned()` будет вызываться при каждой перебалансироке, как способ уведомления. Если нет новых разделов,
  вызывиться с пустой коллекцией.
* `onPartitionsRevoked()` будет вызываться только в том случае, если потребитель отказался от прав владения разделом.
* `onPartitionsLost()` будет вызываться в исключительных условиях перебалансироки, к моменту вызова в коллекции у
  партиций будут новые владельцы.

```java
public class ConsumerMainRebalanceListener {

    private static final KafkaConsumer<String, String> consumer = getConsumer();
    private static final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
    private static final Duration timeout = Duration.ofMillis(100);

    public static void main(String[] args) {

        try {
            consumer.subscribe(List.of("topic"), new HandleRebalance());
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf(
                            "topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    currentOffsets.put(
                            new TopicPartition(record.topic(), record.partition()),
                            new OffsetAndMetadata(record.offset() + 1, null));
                }
                consumer.commitAsync(currentOffsets, null);
            }
        } catch (WakeupException e) {
            // Игнорируем, поскольку закрываемся
        } catch (Exception e) {
            log.error("Unexpected error", e);
        } finally {
            try {
                consumer.commitSync(currentOffsets);
            } finally {
                consumer.close();
                System.out.println("Closed consumer and we are done");
            }
        }
    }

    private static class HandleRebalance implements ConsumerRebalanceListener {
        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
        }

        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
            System.out.println(
                    "Lost partitions in rebalance. " + "Committing current offsets:" + currentOffsets);
            consumer.commitSync(currentOffsets);
        }
    }
}
```

## Получение записей с заданными смещениями

Если необходимо получать конкретные смещения:

* `seekToBeginning()` - начать с начала для топиков
* `seekToEnd()` - начать с конца для топиков

Установить текущее смещение для всех разделов:

```java
        long oneHourEarlier =
                Instant.now().atZone(ZoneId.systemDefault()).minusHours(1).toEpochSecond();
        Map<TopicPartition, Long> partitionTimestampMap =
                consumer.assignment().stream().collect(Collectors.toMap(tp -> tp, tp -> oneHourEarlier));
        Map<TopicPartition, OffsetAndTimestamp> offsetMap =
                consumer.offsetsForTimes(partitionTimestampMap);
        for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetMap.entrySet()) {
            consumer.seek(entry.getKey(), entry.getValue().offset());
        }

```

## Выход из цикла

Если требуется выключить потребителя и выйти немедленно из цикла опроса, может быть проблема длительного ожидания
`poll()`. Для этого можно воспользоваться ещё одним потоком для вызова метода `wakeup()`, если цикл опроса в главном
потоке можно воспользоваться `ShutdownHook`. Если `wakeup()` был вызван в момент, когда поток не ожидает опроса, то
исключение `WakeupException` будет вызвано в следующем вызове `poll()`.

```java
    // Registering a shutdown hook so we can exit cleanly
    Runtime.getRuntime()
        .addShutdownHook(
            new Thread(
                () -> {
                  System.out.println("Starting exit...");
                  // Note that shutdownhook runs in a separate thread, so the only thing we can
                  // safely do to a consumer is wake it up
                  movingAvg.consumer.wakeup();
                  try {
                    mainThread.join();
                  } catch (InterruptedException e) {
                    e.printStackTrace();
                  }
                }));

```

Подробнее http://bit.ly/2u47e9A.

## Десериализаторы

Сериализатор применяемый при отправке сообщения, должен соответствовать десериализатору при получении. В этом плане
реестр схем `AvroSerializer` гарантирует, все записываемые данные совместимы со схемой топика, а значит, их можно
десериализовать с помощью соответствующего десериализатора и схемы.

### Пользовательские сериализаторы

Реализовывать пользовательские десериализаторы и сериализаторы не рекомендуется, оно приводит к сильному сцеплению
производителей и потребителей. Лучше использовать формат `JSON`, `Trift`, `Protobuf` или `Avro`.

### Использование десериализаци Avro в потребителе Kafka

```java
    Duration timeout = Duration.ofMillis(100);
    Properties props = new Properties();
    props.put("bootstrap.servers", "broker1:9092,broker2:9092");
    props.put("group.id", "CountryCounter");
    props.put("key.serializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroDeserializer");
    props.put("specific.avro.reader", "true");
    props.put("schema.registry.url", schemaUrl);
    String topic = "customerContacts";
    KafkaConsumer<String, Customer> consumer = new KafkaConsumer<>(props);
    consumer.subscribe(Collections.singletonList(topic));
    System.out.println("Reading topic:" + topic);
    while (true) {
      ConsumerRecords<String, Customer> records = consumer.poll(timeout);
      for (ConsumerRecord<String, Customer> record : records) {
        System.out.println("Current customer name is: " + record.value().getName());
      }
      consumer.commitSync();
    }

```

## Автономный потребитель: зачем и как использовать потребитель без группы

Можно назначить конкретные разделы для чтения. Пользователям может или подписываться на топик и состоять в группе, или
назначать себе разделы, но не то и другое одновременно.

```java
    Duration timeout = Duration.ofMillis(100);
    List<PartitionInfo> partitionInfos = consumer.partitionsFor("topic");
    List<TopicPartition> partitions = new ArrayList<>();
    if (partitionInfos != null) {
      for (PartitionInfo partition : partitionInfos)
        partitions.add(new TopicPartition(partition.topic(), partition.partition()));
      consumer.assign(partitions);
      while (true) {
        ConsumerRecords<String, String> records = consumer.poll(timeout);
        for (ConsumerRecord<String, String> record : records) {
          System.out.printf(
              "topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
              record.topic(), record.partition(), record.offset(), record.key(), record.value());
        }
        consumer.commitSync();
      }

```

В данном случае, если добавляются новые разделы в топик, то необходимо об этом позаботится самостоятельно: либо
периодически обращаться к `consumer.partitionsFor()` или просто перезапуская приложение при добавлении разделов.